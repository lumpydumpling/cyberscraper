[
{"title": null, "article": ""},
{"title": "Evaluating the Effectiveness of AI Legislation in Cybersecurity with Bruce Schneier", "article": "The rapid evolution of artificial intelligence in cybersecurity presents both significant opportunities and daunting challenges. On this episode, Iâ€™m joined by Bruce Schneier, who is renowned globally for his expertise in cybersecurity and is dubbed a â€œsecurity guruâ€ by the Economist. Bruce, a best-selling author and lecturer at Harvard Kennedy School, discusses the fast-paced world of AI and cybersecurity, exploring how these technologies intersect with national security and what that means for future regulations. (00:00) I discuss with Bruce the challenges of regulating AI in the US. (02:28) Bruce explains the role and future potential of AI in cybersecurity. (05:05) The benefits of AI in defense, enhancing capabilities at computer speeds. (07:22) The need for robust regulations akin to those in the EU. (12:56) Bruce draws analogies between AI regulation and pharmaceutical controls. (19:56) The critical role of knowledgeable staff in supporting legislators. (22:24) The challenges of effectively regulating AI. (26:15) The potential of AI to transform enforcement across various sectors. (30:58) Reflections on the future of AI governance and ethical considerations."},
{"title": "History of RSA Conference. Bruce Schneier. The First â€˜Exhibitorâ€™ in 1994.", "article": "Bruce Schneier was at the first ever RSA Conference in 1991, and he was the first â€˜exhibitorâ€™ in 1994 when he asked Jim Bidzos, Creator of the RSA Conference, if he could sell copies of his book â€œApplied Cryptography.â€ Bidzos set Schneier up in the hotel lobby where the conference was being heldâ€”and the rest is history. Listen to some great RSA Conference memories on this episode of the History of RSA Conference."},
{"title": "The Hacking of Organizational Systems", "article": "â€œThere are only two types of organizations. Those that have been hacked and those that donâ€™t know it yet.â€â€”John Chambers Comcast said nearly 36 million U.S. Xfinity accounts were compromised after hackers accessed its systems through a vulnerability in third-party cloud-computing software. The breach occurred between October 16 and October 19, 2023. On Sunday, February 18, 2024, at the Munich Security Conference, FBI Director Christopher Wray said Chinaâ€™s cyberattacks on U.S. infrastructure are â€œunprecedented.â€ AT&T announced that the cause of its 12-hour nationwide outage on February 22, 2024, was the â€œexecution of an incorrect process,â€ not a cyberattack. In simpler terms, the company admitted to human error. Whatâ€™s the difference between cyberattacks and hacking? Cybercriminals hack and infiltrate computer systems with malicious intent, while hackers supposedly seek new and innovative ways to use a system, good or bad.Â (Micro Trend) According to Security Magazine, there areÂ over 2,200Â attacks daily, which breaks down to nearly 1 cyberattack every 39 seconds. On average, 1.4 billion social media accounts are hacked every month. All systems are vulnerable In (Norton), a book worth your time, Bruce Schneier defines hacking as â€œan activity allowed by the system that subverts the goal or interest of the system.â€ Anything from medical records to the U.S. tax code can be hacked. â€œHacking is how the rich and powerful subvert the rules to increase their wealth and power. Itâ€™s not that the wealthy and powerful are better at their hacks; theyâ€™re less likely to be punished for doing so,â€ adds Schneier, a fellow at the Berkman Center for Internet and Society at Harvard. As F. Scott Fitzgerald observed, â€œThe rich are different from you and me.â€ Schneier says that hacking is not the same as cheating. â€œHacking targets a system and turns it against itself without breaking it.Â Itâ€™s gaming the system and occupies a middle ground between cheating and innovation. AÂ hack follows the letter of the systemâ€™s rules but violates their spirit and intent,â€ he concludes. Systems evolve through hacking, especially when less critical and on a smaller scale. They might actually benefit from hacking as a way to improve their functionality and security. A breach shows where to patch, as itâ€™s impossible to think of every susceptibility when designing a system. Here are three stories illustrating different types of hacking Who has access? The focus in moves from IT toÂ organizational systems.Â For hacking to occur, a system of rules, such as corporate policies, must be hacked. And policies are plentiful. The author explains that itâ€™s â€œone short step from hacking computers to hacking economics, politics, and social systems,â€ as they are just as vulnerable to hacking as technology. Protecting the integrity of any system is rooted in the character and values of those in charge. Hiring decisions, which are extremely important but imperfect, are often a door ajar.Â To quote one observer, â€œPeople are honest most of the time but become dishonest in some situations when they perceive there is an advantage to be gained from it.â€ The bookâ€™s critical point is that not all systems are equally hackable. Complex systems with many rules are the most vulnerable because there are more possibilities for unanticipated and unintended consequences. Schneier makes clear: â€œComplexity is the worst enemy of security.â€ Questions to askâ€” If you are responsible for an enterprise, know it can and will be hacked (rules bent or ignored, boundaries stretched, goals subverted). Therefore, keep policies and procedures simple to reduce security risks. Cognitive hacking is powerful Schneier wants everyone to know that any time something can alter information, choice, and agency, it represents a danger to the human mind. â€œIf you can hack a mind, you can hack any system governed by human action,â€ he writes. Can AI machines think? AI, or artificial intelligence, is defined in as (a) computers that can generally sense, think, or act and (b) as an umbrella term encompassing a broad array of decision-making technologies that stimulate human thinking. An example of that last point is how specialized AI is designed for a specific task, like controlling a self-driving car. Tech writer Andy Kessler says, â€œComputers win in realms with defined rules, but humans have free will and make choices.â€ The AI insight: A corporate plan Steve Durbin, Chief Executive of the Information Security Forum, recommends that AI be viewed from the lens of corporate strategy and risk. â€œBefore you can chart an AI strategy, develop a thorough understanding of its potential, its current usage across the organization, and the security challenges and threats that lie ahead,â€ he emphasizes. At the corporate level, there is a need to integrate ethical considerations into policy and procedures. â€œFairness, transparency, accountability, and privacy are the most ethical considerations surrounding AI,â€ Durbin concludes. In the AI gold rush (Nvidia and OpenAI), programming and security are the next frontiers. Bringing untold financial gain, higher-than-average risk, and opportunities for hacking systems previously unconceived."},
{"title": "Harvard Technologist Encourages Use of AI to Protect Democracy", "article": "Exploring ways in which generative artificial intelligence will affect democracy, prominent Harvard lecturer and public-interest technologist Bruce Schneier said itâ€™s important for people to look both ways and to be unafraid of using the technology when it can help. Schneier said he foresees an â€œarms raceâ€ where those who fail to engage with the technology will quickly lose ground to those who do. He offered examples of how AI can be used throughout the democratic process, including to augment polling, fundraising and campaign strategies in electoral politics, and to more routinely submit comments to regulatory agencies, craft legislation, and improve law enforcement. â€œIt lowers the barrier to lobbying, I think thatâ€™s a good thing,â€ Schneier said during facilitated by the Harvard Kennedy School. Schneier is at work on a new book about generative AI and Democracy. His remarks come as members of Congress discuss , and as some democracy and consumer the technologyâ€™s search and summarization abilities will further decimate journalism and the broader marketplace for information if it is controlled only by a limited number of corporate giants. Schneier balanced his recommendations encouraging use of the technology by imploring policymakers to ensure equity is centered and reiterating his . But as the technology becomes more accessible, he sees greater use of AI potentially shining light on the corners of government that have gone dark due to the drop off in human journalists, for example. â€œA lot of what happens in government happens in secret now because weâ€™ve lost journalism,â€ he said. â€œWell this can be used to get it back.â€ More broadly, Schneier sees â€œenormous potentialâ€ for AI to be used as a moderator and consensus builder. Itâ€™s â€œa perfectly reasonable human job where there arenâ€™t enough humans,â€ he said noting experiments done on such abilities at MIT. â€œYou can imagine a conversation Â… where the AI serves as the moderator. It ensures all voices are heard, can block hateful or off topic comments, highlight areas of agreement and disagreement and help groups reach consensus.â€ In another example, Schneier noted ways AI can be used to enhance law enforcement, including the possibility of the IRS using it to better catch tax cheats. â€œWe have a lot of rules and regulations, but not a lot of enforcement, AI can scale that,â€ he said. But, as with most of his examples, there were cautionary notes such as the danger of false positives and proprietary software. Asked about the risk of bias associated with AI, Schneier highlighted the importance of transparency and auditing. â€œWe talk about AI being racist, but human policemen are racist, so maybe we can do better. Or at least we can see how well weâ€™re doing in a way we canâ€™t with a human,â€ he said. Describing the human brain as the ultimate â€œblack box,â€ Schneier said, â€œThereâ€™s a lot of bias embedded in us, structural bias is a thing, and itâ€™s hard to test. I donâ€™t know why someoneâ€™s racist, I just see the outputs. At worst, AIs are no worse than humans.â€ â€œIâ€™m optimistic, because at least [with] AIs I can open them up and look at the details in a way that I canâ€™t with a human brain,â€ Schneier said noting scientists are getting better at examining those details. For now, â€œthe AIâ€™s are trained on human output and human output is kind of gross,â€ but â€œby cleaning the human output so itâ€™s less gross, and looking at the AI system, we can find the grossness and manually remove it,â€ Schneier said. Noting the way a human brainâ€”the neural networks of which AIs mimicâ€”can be trained, it turns out, to â€œforget addiction,â€ he said maybe, eventually, we can get AI to â€œforget racism.â€"},
{"title": "Bruce Schneier Predicts a Future of AI-Powered Mass Spying", "article": "If the internet helped create the era of mass surveillance, then artificial intelligence will bring about an era of mass spying. Thatâ€™s the latest prediction from noted cryptographer and computer security professional Bruce Schneier, who, in December, where artificial intelligenceâ€”AIâ€”will be able to comb through reams of surveillance data to answer the types of questions that, previously, only humans could. â€œSpying is limited by the need for human labor,â€ Schneier wrote. â€œAI is about to change that.â€ As theorized by Schneier, if fed enough conversations, AI tools could spot who first started a rumor online, identify who is planning to attend a political protest (or unionize a workforce), and even who is plotting a crime. But â€œthereâ€™s so much more,â€ Schneier said. â€œTo uncover an organizational structure, look for someone who gives similar instructions to a group of people, then all the people they have relayed those instructions to. To find peopleâ€™s confidants, look at whom they tell secrets to. You can track friendships and alliances as they form and break, in minute detail. In short, you can know everything about what everybody is talking about.â€ Today, on the Lock and Code podcast with host David Ruiz, we speak with Bruce Schneier about artificial intelligence, Soviet era government surveillance, personal spyware, and why companies will likely leap at the opportunity to use AI on their customers."},
{"title": "Why Is Regulation Slower Than Technology?", "article": ""},
{"title": "23andMe DNA Data Hack", "article": "Watch cybersecurity expert Bruce Schneier and genetics expert John Greally, M.D. discuss the 23andMe DNA data hack consequences, how and why it happened, who did it, and how to protect yourself."},
{"title": "The Best Information Security Books of 2023", "article": "Itâ€™s been a year since I wrote , two years since , which was preceded by and With that, as the year is coming to a close, hereâ€™s my list of the Best Information Security Books of 2023. When it comes to information security rock stars, Bruce Schneier is on everyoneâ€™s list. Heâ€™s written numerous books over the decades, the most important of which may be his classic . The underlying theme Schneier makes in his excellent book is that hacking is, in fact, a universal trait. While those in the information security field think of hacking in terms of zero days and Windows vulnerabilities, finding gaps in things is a normal human response. Schneier writes that all systems will have ambiguities, inconsistencies, and oversights, and they will always be exploitable. Systems of rules, in particular, have to tread the fine line between being complete and being comprehensive within the many limits of human language and understanding. Combine that with the natural human need to push against constraints and test limits, and with the inevitability of vulnerabilities, and you get everything being hacked all the time. This is a delightful and readable book where he discusses how hacking is pervasive across all systems. From hacking financial and legal systems, to political systems, cognitive systems, and more. Not only that, creating an unbreakable system, based on GÃ¶delâ€™s incompleteness theorems, is fundamentally unattainable. A fascinating and engaging read, is my choice for the best information security book of 2023."},
{"title": "Due to AI, â€œWe Are About to Enter the Era of Mass Spying,â€ Says Bruce Schneier", "article": "Schneier: AI will enable a shift from observing actions to interpreting intentions, en masse. In an editorial for Slate Monday, renowned security researcher warned that AI models may enable a new era of mass spying, allowing companies and governments to automate the process of analyzing and summarizing large volumes of conversation data, fundamentally lowering barriers to spying activities that currently require human labor. In the piece, Schneier notes that the existing landscape of electronic surveillance has already transformed the modern era, becoming the , where our digital footprints are constantly tracked and analyzed for commercial reasons. Spying, by contrast, can take that kind of economically inspired monitoring to a completely new level: â€œSpying and surveillance are different but related things,â€ Schneier writes. â€œIf I hired a private detective to spy on you, that detective could hide a bug in your home or car, tap your phone, and listen to what you said. At the end, I would get a report of all the conversations you had and the contents of those conversations. If I hired that same private detective to put you under surveillance, I would get a different report: where you went, whom you talked to, what you purchased, what you did.â€ Schneier says that current spying methods, like phone tapping or physical surveillance, are labor-intensive, but the advent of AI significantly reduces this constraint. Generative AI systems are increasingly adept at summarizing lengthy conversations and sifting through massive datasets to organize and extract relevant information. This capability, he argues, will not only make spying more accessible but also more comprehensive. â€œThis spying is not limited to conversations on our phones or computers,â€ Schneier writes. â€œJust as cameras everywhere fueled mass surveillance, microphones everywhere will fuel mass spying. Siri and Alexa and â€˜Hey, Googleâ€™ are already always listening; the conversations just arenâ€™t being saved yet.â€ From action to intent Weâ€™ve recently seen a movement from companies like and to feed what users create through AI models for the purposes of assistance and analysis. Microsoft is also building into Windows, which require remote cloud processing to work. That means private user data goes to a remote server where it is analyzed outside of user control. Even if run locally, sufficiently advanced AI models will likely the contents of your device, including image content. Microsoft recently , â€œSoon there will be a Copilot for everyone and for everything you do.â€ Despite assurances of privacy from these companies, itâ€™s not hard to imagine a future where AI agents probing our sensitive files in the name of assistance start phoning home to help customize the advertising experience. Eventually, government and law enforcement pressure in some regions could compromise user privacy on a massive scale. Journalists and human rights workers could become initial targets of this new form of automated surveillance. â€œGovernments around the world already use mass surveillance; they will engage in mass spying as well,â€ writes Schneier. Along the way, AI tools can be replicated on a large scale and are continuously improving, so deficiencies in the technology now may soon be overcome. Whatâ€™s especially pernicious about AI-powered spying is that deep-learning systems introduce the ability to analyze the intent and context of interactions through techniques like . It signifies a shift from observing actions with traditional digital surveillance to interpreting thoughts and discussions, potentially impacting everything from personal privacy to corporate and governmental strategies in information gathering and social control. In his editorial, Schneier raises concerns about the chilling effect that mass spying could have on society, cautioning that the knowledge of being under constant surveillance may lead individuals to alter their behavior, engage in self-censorship, and conform to perceived norms, ultimately stifling free expression and personal privacy. So what can people do about it? Anyone seeking protection from this type of mass spying will likely need to look toward government regulation to keep it in check since commercial pressures technological safety and ethics. President Bidenâ€™s mentions AI-powered surveillance as a concern. The European Unionâ€™s also may this issue to some extent, although apparently not directly, to our understanding. Neither is currently in legal effect. Schneier isnâ€™t optimistic on that front, however, closing with the line, â€œWe could prohibit mass spying. We could pass strong data-privacy rules. But we havenâ€™t done anything to limit mass surveillance. Why would spying be any different?â€ Itâ€™s a thought-provoking piece, and you can read the on Slate."},
{"title": "Leading Public-Interest Technologist Sees National Research Resource as a Potential Foundation for an â€œAI Public Optionâ€", "article": "As a chorus of transatlantic public interest groups calls for governments to build their own bedrock artificial intelligence systems, the Harvard Kennedy Schoolâ€™s Bruce Schneier says the National Artificial Intelligence Research Resource backed by key U.S. policymakers could lay the necessary groundwork. \"Itâ€™s a start, and [could] serve as a foundation for an AI Public Option,\" Schneier told referring to the NAIRR, a pilot for which is included in the Oct. 30 executive order on artificial intelligence. The NAIRR has also been highlighted in a series of closed-door AI \"insight forums\" hosted by Senate Majority Leader Charles Schumer (D-NY) who has said there was agreement with top Republicans to spend establishing it over the next five years. The idea that the public sector should provide AI systems that can be used to generate all manner of applications to benefit societyâ€”not just those that would enrich the handful of powerful companies currently in control of the technologyâ€”is being explored by individuals from certain startup companies as well as groups like Public Knowledge in the U.S. and Chatham House in the United Kingdom. Schneier is also a contributor to the new teasing out the associated policy details. \"A Public Option means building AI systems such as foundational large language models that would serve as an alternative to corporate-controlled AI,\" he said. \"Like public roads and a postal system, a public AI option can guarantee universal access to the technology that is fast becoming fundamental for participation in the economy.\" The idea has also taken hold within the UKâ€™s Labor Party, with , proposing the government spend billions to build a \"Great British Cloud,\" and \"BritGPT.\" In the U.S., while some have said the NAIRR, as described in the executive order, represents to that kind of industrial policy, others are concerned that a plan the National Science Foundation produced for implementing the NAIRR would continue to rely on the major commercial cloud service providers who are primarily building foundational AI models for data storage and computing power. Schneier has establishing Public AI neednâ€™t rely on governments \"owning and operating the entire AI supply chain,\" putting resources toward building data centers and special, super expensive, computer chips. He sees value in building Public AI to set an example for how to responsibly deploy the technology in a variety of policy areas. It \"could set an implicit standard that services offered by private entities must surpass,\" he said. \"Widely available public models and compute infrastructure would yield numerous benefits to the U.S. and to broader society. It would provide a mechanism for public input and oversight on the critical ethical questions facing AI development, such as whether and how to incorporate copyrighted works in model training, how to distribute access to private users whose insatiable appetites for AI integrations may outstrip cloud computing capacity, and how to license access for sensitive applications ranging from policing to medical use.\" \"It would serve as an open platform for innovation, on top of which researchers and small businesses Â­as well as mega-corporations Â­could experiment with novel training approaches and new user-facing applications,\" he said. \"An AI Public Option, administered by a competent and accountable public agency, would offer greater guarantees about the availability, equitability, and sustainability of AI technology for all of society than would exclusively private AI development.\""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": "The Hackerâ€™s Mind with Bruce Schneier", "article": ""},
{"title": null, "article": ""},
{"title": "Obligatorisk LÃ¦sning", "article": "Jeg har lige lagt Bruce Schneiers â€œ â€ fra mig og det bliver ikke nemt at gÃ¸re den retfÃ¦rdighed i en boganmeldelse. De fleste af jer har aldrig hÃ¸rt om fÃ¸r, men blandt IT folk er han et , hvis om kryptografi er obligatoriske klassikere i branchen. Denne gang har han skrevet en bog om sikkerhed der ikke handler om computere og faktisk kun halvvejs handler om sikkerhed. Bogen er i bund og grund en analyse af hvordan mennesker omgÃ¥s hinanden, hverken mere eller mindre, men det er ikke nogen sÃ¦rlig hjÃ¦lpsom opsummering, for det dÃ¦kker alt fra affaldshÃ¥ndtering over skattelovgivning til computersikkerhed. Det er et meget gammelt emne, men hvor langt de fleste bÃ¸ger, fra Sun Tzu til Clausewitz behandler â€œos imod demâ€ problematikken, handler denne bog om â€œos imod osâ€: Konflikten imellem gruppen dens deltagere. Forfatteren opstiller en simpel model for hvorledes man kan analysere spÃ¦ndingsfeltet imellem fÃ¦lleskabets og deltagernes interesser og bruger derefter modellen til at gennemgÃ¥ en lang rÃ¦kke eksempler, fra overfiskning til produktsikkerhed, for at vise hvorledes fÃ¦lleskabets forskellige virkemidler over et stort felt kan tunes til at fremme fÃ¦lles interesser, men aldrig helt undgÃ¥ at nogen gÃ¥r over stregen. PÃ¥ sin vis er det â€œbareâ€ en forlÃ¦ngelse af Von Neumans og Morgensterns spilleteori, men frem for analysen af spillet, der oftest ender med uafgjort eller uafgÃ¸rligt, handler det her om designet af spillet: Hvorledes spilleregler indrettes og hvorledes kan de pÃ¥virke spillernes opfÃ¸rsel, sÃ¥ledes at resultatet bliver til fÃ¦llesskabets fordel. Og reglerne skal der skrues pÃ¥, for et fÃ¦llesskab er altid under â€œangrebâ€ fra fraktioner der modarbejder fÃ¦llesskabets bedste, hvad enten de gÃ¸r det af uvidenhed (invasive arter i haven), snÃ¦vertsyn (bÃ¸rnevaccinationer) eller ond hensigt (kriminalitet). Man kommer vidt omkring, ikke mindst hvis man giver sig tid til at tÃ¦nke de opstillede eksempler igennem selv, inden man lÃ¦ser Scheneiers bud pÃ¥ svarene, der bestemt ikke alle kan siges at vÃ¦re indlysende eller intuitive. Et af eksemplerne er en kemisk fabrik der er sÃ¥ uheldigt placeret, at et uheld vil have meget stÃ¸rre omkostninger for samfundet, end den vÃ¦rdi virksomhedens ejere kan risikere at miste derved. Det rammer prÃ¦cis situationen med Sojakagefabrikken. Virksomhedsledelsen havde ingen rationel grund til at gÃ¸re noget ved den uheldige placering: Det vÃ¦rste der kunne ske for dem var at firmaet gik fallit og den risiko kunne aldrig gÃ¸re en flytning af firmaet til en fornuftig beslutning. KÃ¸benhavn havde meget mere pÃ¥ spil end nogle fÃ¥ millioner kroner og mÃ¥tte derfor skride til forholdsvis drastiske midler, for at varetage gruppens interesse. Terminologien og analysen er hele vejen igennem vÃ¦rdineutral, hvilket tillader bogen at vÃ¦re prÃ¦cis, uanset emnets moralske placering og dermed ogsÃ¥ relevant i de tilfÃ¦lde hvor det er â€œoutliersâ€ der vinder, f.eks vedr. slaveriets ophÃ¸r. Det er faktisk lidt af et kendemÃ¦rke for Schneiers bÃ¸ger, at de nÃ¸gternt blotlÃ¦gger mekanismerne uden at tage parti for den ene eller den anden side, bogen er derfor en glimrende ballast hvad enten man skal holde et fÃ¦llesskab sammen (f.eks en afdeling i et firma), splitte det ad (f.eks en rockergruppe) eller bare prÃ¸ver at forstÃ¥ hvad der foregÃ¥r i de mange grupper vi indgÃ¥r i. Et klart pletskud som bÃ¸r lÃ¦ses af alle, bÃ¥de minoriteret og majoriteter. Bruce Schneier: Liars and Outliers Enabling the Trust that Society Needs to Thrive John Wiley & Sons ISBN: 978-1118143308 PS: Jeg har modtaget bogen som gratis anmeldereksemplar men det har pÃ¥ ingen mÃ¥de forandret eller pÃ¥virket min holdning til den."},
{"title": "å‚³å¥‡å¯†ç¢¼å­¸å¤§å¸«å°ˆè¨ªï¼šåˆ¥è¼•ä¿¡ç‰©è¯ç¶²", "article": "åœ¨è¨ªå•è¢«CNNè­½ç‚ºã€Œå…¨çƒæœ€é ‚å°–çš„å¯†ç¢¼å­¸å®¶ã€å¸ƒé­¯æ–¯ï¼æ–½å¥ˆçˆ¾ï¼ˆBruce Schneierï¼‰ä¹‹å‰ï¼Œæˆ‘å€‘å¾ˆå®¹æ˜“é–‹å§‹è¯æƒ³å„ç¨®ç¥ç§˜å½¢è±¡ã€‚ç•¢ç«Ÿï¼Œå¯†ç¢¼é€™å€‹é—œéµå­—ï¼Œç¸½å‡ºç¾åœ¨æƒ…å ±ã€ç‰¹å‹™èˆ‡é§­å®¢é›»å½±ä¸­ï¼Œé€£é›»å½±ã€Šä¸å¯èƒ½çš„ä»»å‹™ï¼šé¬¼å½±è¡Œå‹•ã€‹æ‰¯åˆ°æ ¸å½ˆäº¤æ˜“æ¡ˆæ™‚ï¼Œä¹Ÿæœƒæœ‰å¯†ç¢¼å­¸å®¶çš„æˆ²åˆ†ã€‚ å°æª”æ¡ˆ_æ–½å¥ˆçˆ¾ å‡ºç”Ÿï¼š1963å¹´ å­¸æ­·ï¼šè‹±åœ‹è¥¿æ•å¤§å­¸æ¦®è­½åšå£«ã€ç¾åœ‹ç¾åˆ©å …å¤§å­¸é›»è…¦ç§‘å­¸ç¢©å£« ç¶“æ­·ï¼šCounterpaneç¶²è·¯å®‰å…¨å…¬å¸å‰µè¾¦äºº ç¾è·ï¼šIBM Resilient æŠ€è¡“é•·æš¨è³‡å®‰äº‹æ¥­éƒ¨ç‰¹åˆ¥é¡§å•ã€å“ˆä½›å¤§å­¸ä¼¯å…‹æ›¼ç¶²è·¯èˆ‡ç¤¾æœƒç ”ç©¶ä¸­å¿ƒç ”ç©¶å“¡ è‘—ä½œï¼šã€Šæ‡‰ç”¨å¯†ç¢¼å­¸ã€‹ã€ã€Šéš±å½¢å¸åœ‹ã€‹ æ–½å¥ˆçˆ¾çš„ä¸€ç”Ÿå¾ˆæœ‰è¶£ï¼Œä»–æœ€æš¢éŠ·çš„ã€Šæ‡‰ç”¨å¯†ç¢¼å­¸ã€‹ï¼ˆApplied Cryptographyï¼‰ä¸€æ›¸ï¼Œè¢«ã€Šé€£ç·šã€‹é›œèªŒï¼ˆWiredï¼‰è©•ç‚ºï¼šç¾åœ‹åœ‹å®¶å®‰å…¨å±€æœ€ä¸å¸Œæœ›è¢«å‡ºç‰ˆçš„æ›¸ã€‚ ã€Šé”æ–‡è¥¿å¯†ç¢¼ã€‹å°èªªè£¡æåˆ°ï¼Œå¦‚ä½•ä»¥é›»è…¦åŠ å¯†ä¿è­·è³‡æ–™æ™‚ï¼Œå¿…æçš„å¯†ç¢¼å­¸å®¶æ˜¯ä»–ã€‚ ç•¶åˆæ­éœ²ç¾åœ‹åœ‹å®‰å±€ç§˜å¯†æ–‡ä»¶çš„æ„›å¾·è¯ï¼å²è«¾ç™»ï¼ˆEdward Snowdenï¼‰ï¼Œè¢«ç¾åœ‹é€šç·å¾Œï¼Œæ¥µå°‘æ•¸é¡˜æ„éœ²é¢çš„å ´åˆä¹‹ä¸€ï¼Œå°±æ˜¯èˆ‡æ–½å¥ˆçˆ¾åœ¨å“ˆä½›å¤§å­¸çš„è¬›å ‚è¦–è¨Šå°è«‡ï¼Œè¨è«–ä¸»é¡Œé‚„æ˜¯ï¼šæ”¿åºœç›£æ§èˆ‡éš±ç§ã€‚ çµæœï¼Œç•¶æˆ‘å€‘èˆ‡é€™ä½å‚³å¥‡äººç‰©å°è«‡æ™‚ï¼Œä»–å»ä¸€é»éƒ½ä¸ç¥ç§˜ï¼Œè€Œä¸”æœ‰å¤ æ˜å¿«ï¼Œç«‹å ´åˆ†æ˜ã€‚ æ¯”å¦‚ï¼Œä»–è«‡æœ€ç†±é–€çš„ç‰©è¯ç¶²è­°é¡Œã€‚ä»–æ“ºæ˜ï¼šã€Œå±æ©Ÿæ¯”å¥½è™•å¤§æ›´å¤šï¼ã€ä»–èªç‚ºï¼Œå¾å†°ç®±åˆ°çƒ¤ç®±ï¼Œç•¶æ‰€æœ‰æ±è¥¿éƒ½è®Šæˆé›»è…¦é€£ç·šï¼Œå°‡æœƒç•°å¸¸è„†å¼±ï¼Œå®¹æ˜“è¢«é§­å…¥ï¼Œä¸å¤ å®‰å…¨ã€‚ æ¯”å¦‚ï¼Œä»–çœ‹å¤§å®¶èªç‚ºæœ€å®‰å…¨çš„å€å¡Šéˆï¼Œä»–èªªï¼šã€Œæ²’æœ‰ä»»ä½•å¥½çš„ç†ç”±è¦å»ä¿¡ä»»ï¼ˆå€å¡Šéˆï¼‰ã€‚ã€ æ¯”å¦‚ï¼Œæˆ‘å€‘å•ï¼Œä»–å¦‚ä½•ä¿è­·è‡ªå·±çš„éš±ç§æ™‚ï¼Œä»–ç›´èªªï¼Œä¸èƒ½ç­”ï¼Œã€Œå› ç‚ºæˆ‘çš„éš±ç§ä¸€éƒ¨åˆ†å°±æ˜¯ï¼Œä¸å›ç­”æˆ‘å¦‚ä½•ä¿è­·æˆ‘å€‹äººéš±ç§çš„é€™äº›å•é¡Œã€‚ã€ é€™æ¨£ç›´ä¾†ç›´å¾€çš„äººï¼Œç‚ºä»€éº¼æœƒè¢«è³‡å®‰æ¥­ç•Œæ¨å´‡ï¼Ÿé€£ã€Šç¶“æ¿Ÿå­¸äººã€‹éƒ½èªªï¼Œä»–æ˜¯ã€Œå®‰å…¨å¤§å¸«ã€ã€‚ ä»–æ˜¯å¦‚ä½•é¤Šæˆçš„ï¼Ÿ æ–½å¥ˆçˆ¾å‡ºç”Ÿåœ¨ç´ç´„ï¼Œä»–çš„çˆ¶è¦ªåœ¨å¸ƒé­¯å…‹æ—å€æ“”ä»»æ³•å®˜ï¼Œçˆ¶è¦ªå½¢å®¹ï¼Œã€Œä»–ç¸½æ˜¯è‘—è¿·æ–¼æ•¸å­—ä¹‹é–“ã€‚ã€å°æ™‚å€™ï¼Œä»–æœƒä¸€ç›´å˜—è©¦æ•¸æ•¸çš„æ¥µé™ï¼ŒæŠŠæ•¸å­—å¯«åœ¨ç´™ä¸Šï¼Œè©¦è‘—èƒ½ä¸èƒ½æ•¸åˆ°100è¬ã€‚ æ–½å¥ˆçˆ¾æ›¾èªªï¼Œä»–å’Œçˆ¶è¦ªåœ¨éš±ç§ã€å…¬æ°‘æ¬Šåˆ©é ˜åŸŸï¼Œæœ‰è‘—å…±åŒçš„ä¿¡ä»°ã€‚ä»–ä¸æƒ³å’Œä»–çˆ¶è¦ªä¸€æ¨£æˆç‚ºæ³•å®˜ï¼Œã€Œä½†æˆ‘è¢«æ³•å¾‹çš„é›„è¾¯ã€å¯«ä½œã€é‚è¼¯ã€è«–è­‰æ‰€å•Ÿç™¼ã€‚ã€ ä»–å¦è¨€ï¼Œå¹´è¼•æ™‚å€™ï¼Œä»–ç†±æ„›æ•¸å­¸æ›´å‹èˆ‡äººäº¤æµã€‚å¾æ•¸å­¸å‡ºç™¼ï¼Œæ•¸å­¸æœƒå†å»¶ä¼¸åˆ°å¯†ç¢¼å­¸ï¼Œå¯†ç¢¼å­¸åŸç†çš„ä¸€ç’°ï¼Œå…¶å¯¦å°±æ˜¯å°‡è¨Šæ¯åŸæœ¬çš„æ¨£å­ï¼Œç¶“éåŠ å¯†é‹ç®—ä¹‹å¾Œï¼Œè½‰ç‚ºè®“äººç„¡æ³•ç›´æ¥è¾¨èªå‡ºä¾†çš„è¨Šæ¯ï¼Œä¹Ÿå°±æ˜¯å¯†æ–‡ï¼Œã€Œç†è§£å¯†ç¢¼å­¸å°±çœŸçš„æ˜¯ç†è§£æ•¸å­¸ã€‚ã€ ã€Œæœ¬ä¾†ï¼Œæ²’äººæŠŠé€™äº›å¯†ç¢¼å­¸çš„çŸ¥è­˜é€™æ¨£æ•´ç†èµ·ä¾†ï¼Œå¯èƒ½å°±æ˜¯åœ¨ä¸€äº›åœ‹é˜²å–®ä½ã€åœ‹å®‰å–®ä½ï¼Œå¤§å®¶é—œèµ·é–€ä¾†ç ”ç©¶ã€‚ã€ç²¾é€šå¯†ç¢¼å­¸çš„å°å¤§æ•¸å­¸ç³»å…¼ä»»åŠ©ç†æ•™æˆé™³å›æ˜èªªã€‚ä½†æ–½å¥ˆçˆ¾å»æƒ³æŠŠä»–ç†è§£çš„å¯†ç¢¼é‚è¼¯æ™®åŠåŒ–ï¼Œ30æ­²çš„é‚£å¹´ï¼Œä»–å‡ºç‰ˆã€Šæ‡‰ç”¨å¯†ç¢¼å­¸ã€‹ï¼Œåœ¨é€™ä¹‹å‰ï¼Œæ¥­ç•Œæ²’æœ‰ä¸€æœ¬å®Œæ•´ã€æ˜“æ‡‚çš„å¯†ç¢¼å­¸æ•™ç§‘æ›¸ã€‚ ã€Œæˆ‘å› ç‚ºé€™æœ¬æ›¸è€Œèªè­˜äº†å¯†ç¢¼å­¸ï¼Œã€é™³å›æ˜èªªã€‚ åªæ˜¯ï¼Œæ–½å¥ˆçˆ¾åˆæ˜¯å¦‚ä½•å¾å¯†ç¢¼å­¸ï¼Œè®Šæˆè³‡è¨Šå®‰å…¨å¤§å¸«ï¼Ÿ é™³å›æ˜è§£é‡‹å¯†ç¢¼èˆ‡è³‡è¨Šå®‰å…¨çš„é—œä¿‚ã€‚ä»–èˆ‰ä¾‹ï¼Œåœ¨æˆ‘å€‘ç”Ÿæ´»ä¹‹ä¸­ï¼Œé€£åˆ°Googleé¦–é æ™‚ï¼Œç¶²å€æ˜¯httpsï¼Œè€Œä¸æ˜¯httpï¼Œå­—å°¾çš„ã€Œsã€ï¼Œå°±æ˜¯åŠ å¯†ï¼Œé€²å…¥ç¶²é ä¹‹å¾Œï¼Œæ‰€æœ‰çš„é€šä¿¡éƒ½æœƒæœ‰å¯†ç¢¼ç³»çµ±ä¿éšœå®‰å…¨ï¼Œå°‡è³‡æ–™è®Šæˆå¯†æ–‡å‚³é€ï¼Œä½†ï¼Œå¦‚æœä½ çš„é›»è…¦æ²’æœ‰å¯†ç¢¼ç³»çµ±ä¿è­·ï¼Œåœ¨GoogleæŸ¥è©¢æˆ–å‚³é€çš„è³‡æ–™å¯èƒ½æœƒè¢«å…¶ä»–äººçœ‹åˆ°ï¼ æ–½å¥ˆçˆ¾èªªï¼Œã€Œæˆ‘çš„è·æ¶¯å¯ä»¥èªªæ˜¯ä¸€ç³»åˆ—å°‡äº‹ç‰©ã€æ™®éåŒ–ã€çš„éç¨‹ï¼Œå¾å¯†ç¢¼å­¸çš„æ•¸å­—å®‰å…¨é–‹å§‹ï¼Œæˆ‘çš„ç¬¬ä¸€æœ¬æ›¸æ˜¯ã€Šæ‡‰ç”¨å¯†ç¢¼å­¸ã€‹ï¼Œé—œæ–¼å¯†ç¢¼å­¸ã€‚ç„¶å¾Œæˆ‘è‘—æ‰‹å¯«é›»è…¦å®‰å…¨ï¼Œå†ä¾†æ˜¯ä¸€èˆ¬çš„å®‰å…¨ç§‘æŠ€ã€å®‰å…¨ç¶“æ¿Ÿå­¸ã€å®‰å…¨å¿ƒç†å­¸ã€å®‰å…¨ç¤¾æœƒå­¸ã€å®‰å…¨å…¬å…±æ”¿ç­–ã€‚ã€ ä»–ä¸æ–·æ¢ç´¢ï¼Œå°å®‰å…¨æœ‰ç•°æ–¼å¸¸äººçš„åŸ·è‘—ã€‚ ä»–ç”¨ç¶“æ¿Ÿå­¸è§’åº¦è«‡å®‰å…¨ã€‚ã€Œç•¶æˆ‘å€‘åœ¨è£è¨­é˜²ç›œç³»çµ±ï¼Œæˆ‘å€‘ä»˜å‡ºçš„æ˜¯æ™‚é–“ã€é‡‘éŒ¢ã€æ–¹ä¾¿æ€§æŠ‘æˆ–æ˜¯è‡ªç”±çš„ä»£åƒ¹ã€‚æˆ‘å€‘è¦æƒ³çš„ï¼Œä¸æ˜¯å®ƒèƒ½ä¸èƒ½è®“æˆ‘å€‘å®‰å…¨ï¼Œè€Œæ˜¯å€¼ä¸å€¼å¾—ã€‚ã€ æ–½å¥ˆçˆ¾é‚„ç”¨å¿ƒç†å­¸çœ‹å®‰å…¨ã€‚ã€Œå®‰å…¨åˆ†ç‚ºå…©ç¨®ï¼Œå¯¦éš›ä¸Šçš„å®‰å…¨è·Ÿå¿ƒç†ä¸Šçš„å®‰å…¨ã€‚ã€æ–½å¥ˆçˆ¾æå‡ºã€Œå®‰å…¨åŠ‡é™¢ã€ï¼ˆSecurity Theaterï¼‰çš„æ¦‚å¿µï¼Œå› ç‚ºå¾ˆå¤šè®“äººå€‘æ„Ÿåˆ°å®‰å…¨çš„æªæ–½ï¼Œä¸¦éæ˜¯çœŸçš„å®‰å…¨ã€‚ æ¯”å¦‚ï¼Œä½ åˆ°æ©Ÿå ´å®‰æª¢ï¼Œå…ˆçœ‹ä½ æœ‰ç„¡æ”œå¸¶ç‚¸è—¥ã€æ§æä¹‹é¡çš„ç‰©å“ï¼Œä»–èªç‚ºï¼Œå…¶å¯¦ä¸€èˆ¬äººçš„æ™‚é–“éƒ½è¢«æµªè²»åœ¨æ²’æ”¶å¦‚å‰ªåˆ€ã€æ‰“ç«æ©Ÿä¹‹é¡çš„ç‰©å“ï¼Œä½†æ²’æœ‰äººèƒ½è­‰æ˜ï¼Œé€™æ¨£æ˜¯çœŸçš„å®‰å…¨ï¼Œåªæ˜¯ã€Œæ„Ÿå—ã€èµ·ä¾†æ¯”è¼ƒå®‰å…¨ï¼Œé€™è®“å®‰å…¨åŠ‡é™¢çš„å¯¦è¸æˆæœ¬é€šå¸¸é å¤§æ–¼å¯¦éš›åˆ©ç›Šã€‚ ä»–åˆèªªï¼Œå®‰å…¨æ„Ÿæ˜¯æœ‰éŒ¯è¦ºçš„ï¼Œæˆ‘å€‘æ™‚å¸¸æ”¾å¤§ä¸å¸¸è¦‹çš„å±éšªï¼Œä½†å»å¿½ç•¥å¸¸è¦‹çš„å±æ©Ÿã€‚ä¾‹å¦‚ï¼Œå¤§å®¶å¸¸å¸¸æ“”å¿ƒè¢«é™Œç”Ÿäººç¶æ¶ï¼Œä½†æ•¸æ“šé¡¯ç¤ºï¼Œç†Ÿäººç¶æ¶çš„å¯èƒ½æ€§æ›´é«˜ã€‚æˆ‘å€‘ä¹Ÿå°æ–¼åœ°çƒæš–åŒ–æˆ–æ˜¯å€‹è³‡è¢«æŒæ¡çš„å±æ©Ÿï¼Œå¸¸å¸¸è¼•å¿½ã€‚ ç•¶æ•¸å­—çµ„æˆå¯†ç¢¼ï¼Œå¯†ç¢¼å½¢æˆå®‰å…¨ï¼Œä»–ç™¼æ®çˆ¶è¦ªæ•™çµ¦ä»–çš„æ…‹åº¦ï¼šè¿½æ ¹ç©¶æŸ¢ï¼Œåè¦†è¾¯è­‰ã€‚ é™³å›æ˜å›æ†¶ï¼Œä»–æ›¾åˆ°ç¾åœ‹åƒåŠ çŸ¥åçš„Defconé§­å®¢å¤§æœƒï¼Œç•¶æ™‚è§€çœ¾å°æ–½å¥ˆçˆ¾çš„ä¸€å€‹æå•æ˜¯ï¼šã€Œæ¨™æº–å°ç¨±å¼çš„åŠ è§£å¯†ï¼Œå®‰ä¸å®‰å…¨ï¼Ÿã€æå•è€…éš¨å¾Œè£œä¸Šä¸€å¥ï¼šã€Œç¾åœ‹æ”¿åºœè¬›çš„æˆ‘ä¸ä¿¡ï¼Œä½†ä½ è¬›çš„æˆ‘ä¿¡ï¼ã€ åœ¨æ–½å¥ˆçˆ¾çš„éƒ¨è½æ ¼ä¸Šï¼Œä»–æœ‰ä¸€ç¯‡æ–‡ç« æ˜¯é€™æ¨£å¯«çš„ï¼šæœªä¾†ï¼Œå¤§å‹ç§‘æŠ€å…¬å¸å°±åƒæ˜¯åœ°ä¸»ä¸€æ¨£ï¼Œäºé¦¬éœæƒ³è¦è®“Alexaæ™ºæ…§éŸ³ç®±æˆç‚ºæ™ºæ…§å®¶å±…çš„ä¸­å¿ƒï¼Œè˜‹æœåŠGoogleè¦å®ƒå€‘çš„æ‰‹æ©Ÿæˆç‚ºå”¯ä¸€èƒ½æ§åˆ¶ä½ æ‰€æœ‰ç‰©è¯ç¶²è£ç½®çš„è¨­å‚™ã€‚é€™äº›å¤§å‹ç§‘æŠ€å…¬å¸ä¿è­·æˆ‘å€‘å…æ–¼å¤–åœ¨æ”»æ“Šï¼Œä½†æ˜¯å»æŒæ¡è‘—æˆ‘å€‘èƒ½çœ‹è¦‹çš„ã€æˆ–è€…èƒ½åšçš„äº‹ã€‚ ä»–è­¦å‘Šï¼Œè‹¥çµ¦äºˆç§‘æŠ€æ€ªç¸å¤ªå¤šæ¬ŠåŠ›ï¼Œå¾Œè€…ä¹Ÿæœƒçµ¦äºˆæˆ‘å€‘æ›´å¤šé™åˆ¶ï¼Œã€Œï¼ˆä»Šæ—¥ï¼‰HPå°è¡¨æ©Ÿä¸å†è®“ä½ ä½¿ç”¨éå®˜æ–¹å¢¨æ°´åŒ£ï¼Œæ˜å¤©ï¼Œä»–å€‘å¯èƒ½è¦ä½ ç”¨å®˜æ–¹çš„ç´™å¼µã€‚ã€ åœ¨ä»–çœ¼è£¡ï¼Œæ²’æœ‰ç‚ºäº†å®‰å…¨å°±è©²çŠ§ç‰²éš±ç§ï¼Œä½¿æ•¸æ“šéƒ½è¢«å¤§å» æŒæ¡çš„é“ç†ï¼Œã€Œæˆ‘å€‘éœ€è¦éš±ç§ï¼Œéš±ç§å°±æ˜¯å®‰å…¨çš„ä¸€éƒ¨åˆ†ã€‚ã€ä»–èªªï¼Œä»–ä¿è­‰è”¡è‹±æ–‡ç¸½çµ±ç”¨çš„çµ•å°æ˜¯iPhoneï¼Œå¥¹çš„éš±ç§ï¼Œä¹Ÿå°±ç­‰åŒæ–¼å°ç£åœ‹å®¶å®‰å…¨çš„ä¸€éƒ¨åˆ†ï¼Œæ²’æœ‰å­°é‡å­°è¼•ï¼›æ ¸é›»å» ç‡Ÿé‹å•†ä¹Ÿä¸€æ¨£ï¼Œé€™äº›äººçš„éš±ç§éƒ½æ˜¯åœ‹å®¶å®‰å…¨çš„ä¸€éƒ¨åˆ†ã€‚ é€™ä¸€ç”Ÿï¼Œä»–ä¸€è·¯å¾æ•¸å­¸å»¶ä¼¸ï¼Œè«‡å®‰å…¨ï¼Œç¾åœ¨é‚„å»¶ä¼¸åˆ°äººèˆ‡äººå½¼æ­¤çš„ä¿¡ä»»æ©Ÿåˆ¶ã€‚ä»–è§£é‡‹ï¼Œå®‰å…¨çš„å­˜åœ¨æ˜¯ç‚ºäº†ä¿ƒé€²ä¿¡ä»»ï¼Œä¿¡ä»»æ˜¯ç›®æ¨™ï¼Œåœ¨ç¾ä»Šç¤¾æœƒï¼Œæˆ‘å€‘ä¿¡ä»»å„å¼çš„äººã€æ©Ÿæ§‹å’Œç³»çµ±å¾…æˆ‘å€‘æ˜¯èª å¯¦çš„ï¼Œã€Œï¼ˆä¿¡ä»»ï¼‰æ˜¯è®“ä¸€å€‹ç¤¾æœƒç”Ÿå­˜çš„å¿…éœ€ï¼Œæ²’æœ‰ä¿¡ä»»ï¼Œç¤¾æœƒæœƒå´©è§£ã€‚ã€è‡³ä»Šï¼Œä»–ä¸€é€£å¯«äº†åå¹¾æœ¬æ›¸ã€‚ é€™ç¨®è¶…ä¹ä¸€èˆ¬æ•¸å­¸å®¶åŸ‹é ­è‹¦ç®—çš„æ…‹åº¦ï¼Œè®“æ–½å¥ˆçˆ¾ä¸åªæ˜¯ä¸€å€‹å®‰å…¨æŠ€è¡“å®¶ï¼Œæ›´æ˜¯ä¸€å€‹å®‰å…¨å“²å­¸å®¶ã€‚ä»–çš„ä¸€ç”Ÿä¸æ˜¯å¿™è‘—ç·¨å¯†ç¢¼ï¼Œè€Œæ˜¯å¿™è‘—è§£å¯†ï¼Œæ‰¾åˆ°ã€Œå®‰å…¨ï¼Œå¦‚ä½•èˆ‡äººé¡äº’åˆ©ã€çš„ä½¿å‘½æ„Ÿï¼Œé€™æˆ–è¨±ï¼Œæ­£æ˜¯ä»–è¢«ä¿¡ä»»çš„åŸå› ã€‚"},
{"title": "Liars & Outliers, o cÃ³mo se articula la confianza", "article": "es el nuevo libro de Bruce Schneier que por cortesÃ­a de Wiley. Aunque el libro sale a la venta en los prÃ³ximos dÃ­as, ya se y ciertamente si ya han leÃ­do otros libros de , seguramente no se arrepentirÃ¡n de buscar este . Pero antes de que nadie se aburra, vamos con algo de chicha sobre el libro ğŸ™‚ Schneier, como es habitual, construye un ensayo sÃ³lido al que dota de un relato y un hilo conductor que te va desglosando en capÃ­tulos-pÃ­ldora de unas 20 pÃ¡ginas. Por hacer un sÃ­mil rÃ¡pido, y salvando las (enormes) distancias, es a la seguridad lo que es a la economÃ­a. La comparaciÃ³n es un tanto injusta, porque Freakonomics es una simple colecciÃ³n de casos curiosos donde hace un enorme y valioso trabajo de conceptualizaciÃ³n, pero ambos ilustran con gran certeza la enormÃ­sima dificultad de construir un sistema que equilibre los incentivos en la direcciÃ³n deseada. Schneier lo hace, ademÃ¡s, cumpliendo gratamente con las expectativas: continÃºa la lÃ­nea de sus Ãºltimos libros, alejÃ¡ndose cada vez mÃ¡s de los aspectos tÃ©cnicos de la seguridad e indagando en los sistemas que sirven para controlar nuestra reacciÃ³n ante los riesgos que, en Ãºltimo tÃ©rmino, nos llevan a evaluar cÃ³mo se gestionan y organizan los sistemas para respaldar las normas sociales. En ese Ãºltimo punto es donde transcurre este ensayo, en el que Schneier consigue explicar de forma sencilla lo que, en realidad, es bastante complicado. Siguiendo esquemas habituales en teorÃ­a de juegos (que enfrentan la norma de grupo frente a los intereses que entran en competencia con los mismos), Schneier va construyendo el marco teÃ³rico en el que se apoya para detallar algunas cosas interesantes. Hay muchas ideas en el libro, hoy quiero mencionar sÃ³lo algunas: Hay muchas otras ideas y prometo mÃ¡s adelante dedicar algÃºn post a comentarlas. Pero ya me estoy alargando bastante asÃ­ que por ahora esto es todo, sÃ³lo reiterar el hecho de que el libro me ha parecido una lectura de lo mÃ¡s recomendable. De alguna forma creo que no leÃ­a nada tan interesante desde que hace ya varios aÃ±os me leÃ­ . : En un nuevo post he comentado ."},
{"title": "Interview with Bruce Schneier", "article": "Bruce Schneier is an expert for cryptography and computer security, developer of popular crypto algorithms, author of many books and co-founder of Counterpane Internet Security. Bruce Schneier: Most of my travel involves some speaking these days. Iâ€™ve just come back from participating in a seminar called â€œThe Politics of Fearâ€ at Tufts University. Next week I am speaking to staffers in Congress about data mining, and giving a lecture at â€œThe Politics of Fearâ€. Later in the month, I head to Europe for a series of conferences. Quite a bit of my working life is like that these days. Cryptography was always a hobby, and I can remember the various childrenâ€™s cryptography books I used to own. I did some work in cryptography for the U.S. government, but I didnâ€™t become seriously immersed in the field until the early 1990s, when I was writing Applied Cryptography. Like many engineers, Iâ€™m proud of the algorithms I developed that are being used. For that reason I would point to Twofish and Blowfish. But cryptography has an interesting twist on that: the true measure of a cryptographer is his breaks. When I look back at my work, it is my cryptanalysis papersâ€”the papers that broke other peopleâ€™s algorithms in new and interesting waysâ€”that I am most proud of. Most of what I am doing these days is about how security works in context. Itâ€™s not enough to have a good technical security solution, because so much of security has nothing to do with security. Itâ€™s important to understand the economics of security, the psychology of security decision making, and the legal framework in which security works. Iâ€™ve written a conceptual sequel; itâ€™s called Practical Cryptography (John Wiley & Sons, 2003). Itâ€™s not the same book, though. Applied Cryptography was broad; it tried to survey the whole field of cryptography. Practical Cryptography is much more focused. It takes the basic problem of cryptographyâ€”setting up a secure channel between two peopleâ€”and examines every aspect of it. I think itâ€™s a better book for someone who wants to understand cryptography, and a much better book for an engineer who is trying to learn how to code a cryptographic system. Certainly cryptanalysis was a huge factor in World War II. Modern historians think that it shortened the war by two years or so. The reason is unique in history: the encryption machines were adding machine-era electromechanical devices, and the code-breaking machines were the worldâ€™s first digital computers. That isnâ€™t true any more. While computer and network security techniques, both offensive and defensive, will play an important role in any hypothetical future â€œworld war,â€ I think cryptanalysis will play a minor role. As to the â€œwar on terror,â€ thatâ€™s a rhetorical war and not a real war. It makes no sense to declare war on an abstract noun. It makes no sense to declare war on a tactic. Further, it makes no sense to declare war on a tactic that has been with us for thousands of years, and will be with us for as long as we are a human civilization. Wars are declared by countries against countries, and end when one country is defeated or the two countries mutually declare peace. We already know what to call a tactic that has been with us since the beginning of civilization and will be with us until the end of civilization: crime. Terrorism is a particularly heinous and awful crime, but it is still a crime. Calling it a war only clouds our ability to deal with terrorism. All technologies have good and bad uses. In this way, cryptography is no different from anything else. We all use cars to drive around, and the bad guys use them to flee from robberies. We all use telephones to communicate, and the bad guys use them to plan crimes. This is okay because society is overwhelmingly made up of good and honest people, and the positive uses of these technologies far outweigh the negative uses. Restricting cryptography is, as you said, a tool of a dictatorship. Itâ€™s a tool of a police state. Any rhetoric about it being a way to fight terrorism is a lie; itâ€™s a way to control the rest of the population. Of course. Security testing of closed-source systems can be very effective in improving products. Itâ€™s time-consuming and expensive, but itâ€™s very effective. And itâ€™s exactly the same with open-source testing. The difference is the economic model. In the closed-source model, the company developing the software pays experts to test and evaluate the security. In the open-source model, the developers throw the software out there and hope experts evaluate it out of the goodness of their hearts. Both models can result in well-tested software, and both models can result in lousy software. Itâ€™s not whether the code is open or closed; itâ€™s who has evaluated it. PGP is not a flop. PGP Corporation is a viable company, and PGP is selling better than ever. Itâ€™s taken this long for PGP to be successful for a number of reasons, primarily the fact that it was too hard to use. PGP Corporation has spent a lot of effort making its user interface better, and in many cases invisible. Itâ€™s also true that e-mail encryption doesnâ€™t solve the most pressing security problems. Your data isnâ€™t likely to be eavesdropped on when it is in transit. Itâ€™s much more likely to be eavesdropped on when it is sitting on your computer, by some bad guy who hacks into your network. PGP doesnâ€™t solve this problem. PGP has another product, PGP Disk, that encrypts files and directories and drivesâ€”but thatâ€™s something different entirely. Security is a process, not a product. The industry shows no sign of disappearing, but the solutions are looking more like services and less like products. This makes sense; the solutions need to react quickly as the threats evolve, and service-based solutions are better suited for that. I see this trend continuing. Another, parallel, trend is outsourcing. Organizations simply donâ€™t have the expertise or resources to deal with security issues directly, and it makes far more sense to outsource it. These trends have combined in Managed Security Services, which will take over more and more of security. Every tool has its purpose, and no tool is useful in every situation. There are problems that lend themselves to math, and there are problems that donâ€™t. Security is one of those problems that doesnâ€™t, by the way. We can use the mathematics of cryptography to solve a very specific class of security problems, but the really hard ones are people problems. Iâ€™m not a good chess player, either, although I couldnâ€™t tell you if it is from lack of ability or lack of practice. And Iâ€™ve never been good at memorization, which is what separates the serious Scrabble players from the hobbyists."},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": "Is This A Hack? Generating Income From Your Home. Bruce Schneier, Author of â€œA Hackerâ€™s Mindâ€", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of â€œA Hackerâ€™s Mind,â€ which answers the question. In this episode, we cover the phenomenon known as â€œhouse hacking,â€ which isâ€”according to an article from Rocket Mortgageâ€”â€œa modern lifestyle choice that borrows heavily from old-school ways and has been reimagined with the help of modern home-sharing platforms.â€"},
{"title": "Is This A Hack? Beating The Customer Service Phone Line. Bruce Schneier, Author of â€œA Hackerâ€™s Mindâ€", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of â€œA , which answers the question. In this episode, we cover an â€œingenious hack,â€ according to The Daily Mail, that â€œhelps callers bypass the endless automated questions now used by most major firmsâ€™ helplines and get straight through to a human being.â€"},
{"title": "Is This A Hack? Password Sharing On Netflix. Bruce Schneier, Author of â€œA Hackerâ€™s Mindâ€", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of â€œA Hackerâ€™s Mind,â€ which answers the question. In this episode, we cover a common practice among Netflix users: password sharing, which gained popularity for allowing friends and family members to access a wider variety of content without having to pay for additional accounts."},
{"title": "Is This A Hack? Increased AirBnB Bookings. Bruce Schneier, Author of â€œA Hackerâ€™s Mindâ€", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of â€œA Hackerâ€™s Mind,â€ which answers the question. In this episode, we talk about AirBnB listings and how some property owners are increasing their propertyâ€™s occupancy."},
{"title": "Is This A Hack? Lower Hotel Costs. Bruce Schneier, Author of â€œA Hackerâ€™s Mindâ€", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of â€œA Hackerâ€™s Mind,â€ which answers the question. In this episode, we talk about hotel costs and how some travelers are getting the most out of their stay."},
{"title": "Bruce Schneier on Trust", "article": "Modern society depends on trust more than we realise, and the basis for that trust is security. The trick, says the security guru, is preserving the forces that allow us to trust one another, while also knowing who not to trust Security exists to facilitate trust. Trust is the goal, and security is how we enable it. Think of it this way: As members of modern society, we need to trust all sorts of people, institutions and systems. We have to trust that theyâ€™ll treat us honestly, wonâ€™t take advantage of us and so on â€“ in short, that theyâ€™ll behave in a trustworthy manner. Security is how we induce trustworthiness, and by extension enable trust. An example might make this clearer. For commerce to work smoothly, merchants and customers need to trust each other. Customers need to trust that merchants wonâ€™t misrepresent the goods theyâ€™re selling. Merchants need to trust that customers wonâ€™t steal stuff without paying. Each needs to trust that the other wonâ€™t cheat somehow. Security is how we make that work, billions of times a day. We do that through obvious measures like alarm systems that prevent theft and anti-counterfeiting measures in currency that prevent fraud, but I mean a lot of other things as well. Consumer protection laws prevent merchants from cheating. Other laws prevent burglaries. Less formal measures like reputational considerations help keep merchants, and customers in less anonymous communities, from cheating. And our inherent moral compass keeps most of us honest most of the time. In my new book , I call these societal pressures. None of them are perfect, but all of them â€“ working together â€“ are what keeps society functioning. Of course there is, and always will be, the occasional merchant or customer who cheats. But as long as theyâ€™re rare enough, society thrives. These notions of trust and trustworthiness are as old as our species. Many of the specific societal pressures that induce trust are as old as civilisation. Morals and reputational considerations are certainly that old, as are laws. Technical security measures have changed with technology, as well as details around reputational and legal systems, but by and large theyâ€™re basically the same. What has changed in modern society is scale. Today we need to trust more people than ever before, further away â€“ whether politically, ethnically or socially â€“ than ever before. We need to trust larger corporations, more diverse institutions and more complicated systems. We need to trust via computer networks. This all makes trust, and inducing trust, harder. At the same time, the scaling of technology means that the bad guys can do more damage than ever before. That also makes trust harder. Navigating all of this is one of the most fundamental challenges of our society in this new century. It might be the first rule of security, but itâ€™s the worst rule of society. I donâ€™t think I could even total up all the people, institutions and systems I trusted today. I trusted that the gas company would continue to provide the fuel I needed to heat my house, and that the water coming out of my tap was safe to drink. I trusted that the fresh and packaged food in my refrigerator was safe to eat â€“ and that certainly involved trusting people in several countries. I trusted a variety of websites on the Internet. I trusted my automobile manufacturer, as well as all the other drivers on the road. I am flying to Boston right now, so that requires trusting several major corporations, hundreds of strangers â€“ either working for those corporations, sitting on my plane or just standing around in the airport â€“ and a variety of government agencies. I even had to trust the TSA [US Transportation Security Administration], even though I know itâ€™s doing a lousy job â€“ and so on. And itâ€™s not even 9:30am yet! The number of people each of us trusts every day is astounding. And we trust them so completely that we often donâ€™t even think about it. We donâ€™t walk into a restaurant and think: â€œThe food vendors might have sold the restaurant tainted food, the cook might poison it, the waiter might clone my credit card, other diners might steal my wallet, the building constructor might have weakened the roof, and terrorists might bomb the place.â€ We just sit down and eat. And the restaurant trusts that we wonâ€™t steal anyone elseâ€™s wallet or leave a bomb under our chair, and will pay when weâ€™re done. Without trust, society collapses. And without societal pressures, thereâ€™s no trust. The devil is in the details, of course, and thatâ€™s what my book is about. My primary concerns are threats from the powerful. Iâ€™m not worried about criminals, even organised crime. Or terrorists, even organised terrorists. Those groups have always existed, always will, and theyâ€™ll always operate on the fringes of society. Societal pressures have done a good job of keeping them that way. Itâ€™s much more dangerous when those in power use that power to subvert trust. Specifically, I am thinking of governments and corporations. Let me give you a few examples. The global financial crisis was not a result of criminals, it was perpetrated by legitimate financial institutions pursuing their own self-interest. The major threats against our privacy are not from criminals, theyâ€™re from corporations trying to more accurately target advertising. The most significant threat to the freedom of the Internet is from large entertainment companies, in their misguided attempt to stop piracy. And the cyberwar rhetoric is likely to cause more damage to the Internet than criminals could ever dream of. What scares me the most is that today, in our hyper-connected, hyper-computed, high-tech world, we will get societal pressures wrong to catastrophic effect. This could be considered a companion book to my own. I write from the perspective of security â€“ how society induces cooperation. Benkler takes the opposite perspective â€“ how does this cooperation work and what is its value? More specifically, what is its value in the 21st century information-age economy? He challenges the pervasive economic view that people are inherently selfish creatures, and shows that actually we are naturally cooperative. More importantly, he discusses the enormous value of cooperation in society, and the new ways it can be harnessed over the Internet. I think this view is important. Our culture is pervaded with the idea that individualism is paramount â€“ Thomas Hobbesâ€™s notion that we are all autonomous individuals who willingly give up some of our freedom to the government in exchange for safety. Itâ€™s complete nonsense. Humans have never lived as individuals. We have always lived in communities, and we have always succeeded or failed as cooperative groups. The fact that people who separate themselves and live alone â€“ think of Henry David Thoreau in â€“ is so remarkable indicates how rare it is. Benkler understands this, and wants us to accept the cooperative nature of ourselves and our societies. He also gives the same advice for the future that I do â€“ that we need to build social mechanisms that encourage cooperation over control. That is, we need to facilitate trust in society. , by the biologist Robert Trivers. Trivers has studied self-deception in humans, and asks how it evolved to be so pervasive. Humans are masters at self-deception. We regularly deceive ourselves in a variety of different circumstances. But why? How is it possible for self-deception â€“ perceiving reality to be different than it really is â€“ to have survival value? Why is it that genetic tendencies for self-deception are likely to propagate to the next generation? Triversâ€™s book-long answer is fascinating. Basically, deception can have enormous evolutionary benefits. In many circumstances, especially those involving social situations, individuals who are good at deception are better able to survive and reproduce. And self-deception makes us better at deception. For example, there is value in my being able to deceive you into thinking I am stronger than I really am. Youâ€™re less likely to pick a fight with me, Iâ€™m more likely to win a dominance struggle without fighting, and so on. I am better able to bluff you if I actually believe I am stronger than I really am. So we deceive ourselves in order to be better able to deceive others. The psychology of deception is fundamental to my own writing on trust. Itâ€™s much easier for me to cheat you if you donâ€™t believe I am cheating you. There have been a number of books about the violent nature of humans, particularly men. I chose both because it is well-written and because it is relatively new, published in 2005. David M Buss is a psychologist, and he writes well about the natural murderousness of our species. Thereâ€™s a lot of data to support natural human murderousness, and not just murder rates in modern societies. Anthropological evidence indicates that between 15% and 25% of prehistoric males died in warfare. This murderousness resulted in an evolutionary pressure to be clever. Hereâ€™s Buss writing about it: â€œAs the motivations to murder evolved in our minds, a set of counterinclinations also developed. Killing is a risky business. It can be dangerous and inflict horrible costs on the victim. Because itâ€™s so bad to be dead, evolution has fashioned ruthless defences to prevent being killed, including killing the killer. Potential victims are therefore quite dangerous themselves. In the evolutionary arms race, homicide victims have played a critical and unappreciated role â€“ they pave the way for the evolution of anti-homicide defences.â€ Those defences involved trust and societal pressures to induce trust. is Steven Pinkerâ€™s explanation as to why, despite the selection pressures for murderousness in our evolutionary past, violence has declined in so many cultures around the world. Itâ€™s a fantastic book, and I recommend that everyone read it. From my perspective, I could sum up his argument very simply: Societal pressures have worked. Of course itâ€™s more complicated than that, and Pinker does an excellent job of leading the reader through his analysis and conclusions. First, he spends six chapters documenting the fact that violence has in fact declined. In the next two chapters, he does his best to figure out exactly what has caused the â€œbetter angels of our natureâ€ to prevail over our more natural demons. His answers are complicated, and expand greatly on the interplay among the various societal pressures which I talk about myself. Itâ€™s not things like bigger jails and more secure locks that are making society safer. Itâ€™s things like the invention of printing and the resultant rise of literacy, the empowerment of women and the rise of universal moral and ethical principles. , by the neuroscientist Patricia Churchland. This book is about the neuroscience of morality. Itâ€™s brand new â€“ published in 2011 â€“ which is good because this is a brand new field of science, and new discoveries are happening all the time. Morality is the most basic of societal pressures, and Churchland explains how it works. This book tries to understand the neuroscience behind trust and trustworthiness. In her own words: â€œThe hypothesis on offer is that what we humans call ethics or morality is a four dimensional scheme for social behavior that is shaped by interlocking brain processes: (1) caring (rooted in attachment to kin and kith and care for their well-being), (2) recognition of otherâ€™s psychological states (rooted in the benefits of predicting the behavior of others) (3) problem-solving in a social context (e.g., how we should distribute scarce goods, settle land disputes; how we should punish the miscreants) and (4) learning social practices (by positive and negative reinforcement, by imitation, by trial and error, by various kinds of conditioning, and by analogy).â€ Those are our innate human societal pressures. They are the security systems that keep us mostly trustworthy most of the time â€“ enough for most of us to be trusting enough for society to survive. Of course not. There are two parts to the question. One: Are we doing the right thing? That is, does it make sense for America to focus its anti-terrorism security efforts on airports and airplanes? And two: Are we doing things right? In other words, are the anti-terrorism measures at airports doing the job and preventing terrorism? I say the answer to both of those questions is no. Focusing on airports, and specific terrorist tactics like shoes and liquids, is a poor use of our money because itâ€™s easy for terrorists to switch targets and tactics. And the current TSA security measures donâ€™t keep us safe because itâ€™s too easy to bypass them. There are two basic kinds of terrorists â€“ random idiots and professionals. Pretty much any airport security, even the pre-9/11 measures, will protect us against random idiots. They will get caught. And pretty much nothing will protect us against professionals. Theyâ€™ve researched our security and know the weaknesses. By the time the plot gets to the airport, itâ€™s too late. Much more effective is for the US to spend its money on intelligence, investigation and emergency response. But this is a shorter answer than your readers deserve, and I suggest they read more of my . Like everything else, cloud computing is all about trust. Trust isnâ€™t new in computing. I have to trust my computerâ€™s manufacturer. I have to trust my operating system and software. I have to trust my Internet connection and everything associated with that. I have to trust all sorts of data I receive from other sources. So on the one hand, cloud computing just adds another level of trust. But itâ€™s an important level of trust. For most of us, it reduces our risk. If I have my email on Google, my photos on Flickr, my friends on Facebook and my professional contacts on LinkedIn, then I donâ€™t have to worry much about losing my data. If my computer crashes Iâ€™ll still have all my email, photos and contacts. This is the way the iPhone works with iCloud â€“ if I lose my phone, I can get a new one and all my data magically reappears. On the other hand, I have to trust my cloud providers. I have to trust that Facebook wonâ€™t misuse the personal information it knows about me. I have to trust that my data wonâ€™t get shipped off to a server in a foreign country with lax privacy laws, and that the companies who have my data will not hand it over to the police without a court order. Iâ€™m not able to implement my own security around my data; I have to take what the cloud provider offers. And I must trust thatâ€™s good enough, often without knowing anything about it. Seven."},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": "Liars and Outliers", "article": "I got signed, for cheap, direct from on the condition that I write a review of it. He sold 100 of them this way as a pretty clever way to stir up some publicity. It also worked as a motivator for me to actually write about it. The basic gist of it is that while I enjoyed the book, it felt like he was preaching to the choir. I didnâ€™t find very much new information (though size-weight misperception was new to me and seems pretty interesting), and my guess would be the type of person thatâ€™s likely to pick up this book and read it is in the same boat. There are countless people who absolutely need to understand the concepts it contains, but Iâ€™m unconvinced they are a likely audience. I have to admit that I sort of hated the logistical aspects of this printing. There was an abundance of tables that largely seemed unnecessary, the top and bottom margins seemed to vary without a whole lot of rhyme or reason, and all of the notes were endnotes. Why would anyone publish a book with all of the interesting side bits shoved to the end (along with this one, see )? Have these publishers never read anything by ? Footnotes that are interspersed throughout the work rather than being relegated to 37 pages at the back a) are going to actually be read, and b) exponentially increase the quality of the work. Iâ€™m sorry, but keeping two bookmarks and constantly flipping back and forth just isnâ€™t worthwhile when Iâ€™m actively reading, and as a result Iâ€™m missing out on context and content. From briefly talking to about this, it seems that itâ€™s something on the publisher or printer or other non-author end, but seriously, for research-based writing footnotes are leaps and bounds above endnotes. Anyway, back to the content, Schneier does an excellent job of presenting the research, but he almost comes off timid. There seems to be a reluctance to really call out those making unrealistic and/or harmful security tradeoffs. Yes, many of the decisions can be rationalized and explained, but they are still bad tradeoffs. In his more informal writing, Schneier is blunt about security theater and all that it entails, and I felt like that bluntness was sorely lacking in much of this book. As someone who is familiar with a lot of the research in this area, I have to believe I am not the target audience, no matter how much I may have wanted to be. I think thatâ€™s probably my mistake in expecting something far more in depth, but the reality is that the audience for that level of research is considerably smaller than the audience for which it feels like this book was written. For someone ignorant to the field looking for an overview introduction to security in all its forms, I donâ€™t know of a better book."},
{"title": "Is Online Convenience Worth the Trade-Off for Less Cybersecurity?", "article": "Marcus Smith interviewed Bruce Schneier on BYU Radioâ€™s â€œConstant Wonder.â€"},
{"title": "Trust and Society", "article": "I used to think that was out of touch with industry CISOs, but now I think that they are out of touch with him. Heâ€™s come on tremendously in recent years. I saw him present to the United Nations last year and he was awesome, reflecting a lot of research and deep thinking about important issues such as trust, risk, surveillance and cyber warfare. I shall be ordering a copy of his new book . Itâ€™s about trust, a subject I find both relevant and fascinating. Trust is a phenomenon that few security researchers seem to understand. The problem is that itâ€™s a means to an end, and makes little sense when studied in isolation from its purpose. The nature of trust is also changing as we move from an industrial-age dominated business landscape to the information age. Â I find this paradigm shift is neatly captured by two Russian proverbs. The first, ascribed to both Stalin and Lenin, is â€œTrust is good, control is better,â€ which encapsulates industrial-age thinking for vertically integrated enterprises and societies. The second, made famous by Ronald Reagan, is â€œTrust, but verify,â€ which reflects our best endeavors for managing situations in a modern, diverse supply chain that is increasingly beyond our direct control."},
{"title": "\"Life is Insecurity\"", "article": "Schneier: I have always believed it takes a certain personality type to be interested in security. Iâ€™ve heard it called â€œpaid professional paranoid,â€ but itâ€™s more complicated than that. At its core, security is about figuring out how things work and how things can be made to not work. Itâ€™s about looking at systems and figuring out how to get around them. I think itâ€™s the no-rules intellectual challenge of security that intrigues me. Schneier: I donâ€™t think Crypto-Gram is so popular because itâ€™s the best source for security information. There are far better news sources out there, as well as far better publications devoted specifically to IT security. I think Crypto-Gram is popular because I write it in a no-bullshit style. I write about the security issues that interest me â€“ technological, social, political â€“ and spend a lot of time explaining the news. I think people appreciate the common sense, and that I donâ€™t have any corporate agenda. Schneier: Security is all about economics. All security decisions are trade-offs, and businesses make those trade-offs primarily economically. This is why we see, for example, so many thefts of personal information from corporate databases. The corporations are not liable for the losses, so theyâ€™re not going to spend a lot of money securing the data. The way to improve security is to recognize this economic truth and work with it: force the entity thatâ€™s in the best position to mitigate the risk to be responsible for the risk. And the security issue thatâ€™s really important right now: crime. Schneier: This question depends a lot on which country youâ€™re in, because the economics of security depend a lot on the regulatory environment. But whatever country youâ€™re in, regulation and compliance is probably your most important security-related topic right now. After that, crime. Crime crime crime. I donâ€™t think thereâ€™s a number three that even comes close to those two. Schneier: The second part of your question should be: â€œand should anyone even care?â€ The answer to that second part would be â€œno.â€ Encryption, even lousy encryption, is generally the strongest part of a computer security system. If a system gets broken, it will invariably be because of a vulnerability somewhere else: the software, the user interface, the network, the installation, etc. So thereâ€™s basically no point in losing sleep over the encryption. Itâ€™s like putting a tall spike into the ground and hoping your enemy will run right into it. You can argue about whether the spike should be a mile tall, or a mile-and-a-half tall, but the enemy is just going to go around the spike. That being said, I am sure of two things: First, that there are more powerful cryptanalytic tools against todayâ€™s encryption algorithms than we know of right now. The second point is that even with these tools the algorithms will be secure against any practical attack. Schneier: Of course technology canâ€™t solve the security problem. It will always come down to people. Until people are replaced by robots, I guess. Schneier: Microsoft will be a preferred target for hackers as long as they have a preferred position in the marketplace, but criminals donâ€™t care which systems they use to attack your networks. I think that customers of every product need to worry, and if SAP thinks it can hide, itâ€™s sadly mistaken. Schneier: Crime rarely pays more than crime prevention. For one, as a criminal you canâ€™t learn from your mistakes â€“ you go to jail for them. Because making mistakes is risky, criminals donâ€™t tend to learn very fast. And youâ€™re not going to see a lot of â€œnew ideasâ€ in Internet crime, just variants of old ideas. I donâ€™t think weâ€™ve seen anything new, really. What we have seen is conventional crime moving to the Internet and taking advantage of the economies of scale inherent in computers. Schneier: One without people. That is, one that will never exist. Life is insecurity, and the sooner we get used to that, the better."},
{"title": "Firm Finds Big Security Holes in Windows NT", "article": "Flaws in Microsoft Corp.â€™s Windows NT software threaten the security of companies using the Internet to tie together their far-flung corporate locations, a computer security consulting firm declared on Monday. â€œWe were able to sniff passwords, eavesdrop on the networks, and passively do traffic analysis,â€ said Bruce Schneier, president of Counterpane Systems Inc., of Minneapolis, Minn. â€œAny Microsoft NT server on the Internet is insecure.â€ Counterpane discovered the problems while doing a security analysis on a Windows NT, an operating system used by a swiftly growing number of corporations as the foundation for their computer networks. Microsoft confirmed the security problems later the same day. VPNs increasingly popular The flaws weaken the security of so-called â€œvirtual private networks,â€ or VPNs, based on NT and point-to-point tunneling protocol, or PPTP. These VPNs connect company networks from various locations and are quickly becoming popular in the corporate world as a low-cost solution to buying a dedicated phone line to connect computers between company sites. â€œA lot of people are creating their virtual private networks using NT,â€ said Schneier. â€œThat makes the flaw that much more serious.â€ The PPTP is Microsoftâ€™s homegrown way of securely sending and receiving data over the public Internet. Itâ€™s also used to identify whether the person logging in a valid user. But the software giant would have been better off using one of the publicâ€”and stress-testedâ€”standards, said Schneier. â€œDeveloping security implementations in-house is very difficult to do right,â€ said Schneier. â€œThatâ€™s why itâ€™s important to adopt a publicly tested and recognized standard.â€ Microsoft promises fix ASAP Windows NT system can use either a 40-bit or 128-bit encryption key to protect a companyâ€™s data. Those keys, in and of themselves, are extremely secure. The problem is that NT secures those keys with a flawed password system. â€œAnyone with a list of the top 10 million passwords can break over 99 percent of the systems out there,â€ he said. Microsoft promises to fix the flaws as soon as possible. â€œ(Part of the problem) is already fixed,â€ said Karan Khanna, product manager for Windows NT security at Microsoft. â€œWe will be releasing patches to fix the rest as soon as we can.â€ Khanna attempted to put the flaws in perspective. â€œThe amount of security an organization enforces depends on its needs,â€ he said. â€œThe CIA spends billions of dollars on securityâ€”our customers donâ€™t need the level.â€ Is fix worse than flaw? That you-get-what-you-pay-for philosophy could quickly backfire on the software giant, however. Despite the stress on getting fixes out as soon as possible, many times such patches just make more problems for system administrators, said Schneier. â€œLast time they released a fix, it broke so many other parts of Windows NT, Microsoft had to pull it off the Web site three weeks later,â€ he said."},
{"title": "Schneier: Beware Security Products", "article": "A leading security expert has warned businesses to beware of buying shoddy security products. Bruce Schneier, founder and chief technical officer of BT Counterpane, issued the warning at the RSA Conference Europe 2007 in London on Tuesday. He told delegates that to give a fair representation of the security of those products. â€œThere might be a political bent to security decisions, or there might be a marketing bent,â€ said Schneier, citing as an example people selling smart cards who â€œdo a lot to convince us that smart cards are the answer to security problems. For every company thatâ€™s secure, thereâ€™s at least one â€˜me too.'â€ Schneier said it was difficult for companies to judge the security of varying products because known attacks are relatively rare, making it hard to collect enough data for security-product evaluations. â€œIf events are high-damage and rare itâ€™s difficult to get data. Iâ€™m not going to know (the validity of a product) because I donâ€™t have the data. After 9/11 there was a huge inquiry into what went wrong, but itâ€™s hard to tell what went wrong because it was one event. Thereâ€™s not enough data,â€ said Schneier. â€œThe (security) market is asymetricalâ€”the seller knows a lot more than the buyer,â€ said Schneier. â€œIn the U.S., a lousy used car is called a lemonâ€”but you donâ€™t know until you drive it off the lot that itâ€™s a lemon.â€ If marketed correctly, bad products can drive good products out of the market, Schneier warned. â€œProducts can have the same claims, the same algorithms, the same buzzwords, and one is very secure while the other is just slapped together. If thereâ€™s no functional way to test a product, youâ€™ll buy the cheaper one,â€ said Schneier. Schneier said that due to market dynamics, good products tend to rise to the top, but that the market . He warned businesses not to get â€œcaught up in the feeling of security, driven by fear, rather than the reality.â€ â€œFundamentally, we are not rational,â€ said Schneier. â€œThe brain is just barely functioning in the security community. Itâ€™s still in beta testing. Thereâ€™s weird holes and shortcuts, and all sorts of patches and work-arounds.â€ Businesses should evaluate security products very carefully, said Schneier, and find trusted individuals with expertise who can make security decisions within a company. Eric Baize, senior director of the product security office of storage company EMC, agreed that there were both good- and bad-quality security products available. â€œThe law of statistics is such that in anything there are good- and bad-quality things,â€ said Baize. â€œThis applies to wine, food, and security products. There has been a lot of discussion about whether security should be added on to the infrastructure, or included as a core feature. Now in the security space companies are selling secure infrastructures.â€ Shannon Kellogg, director of information security policy for security company RSA, said that it was critical to build security into systems from the beginning. â€œBuilding core security functionalities is absolutely critical,â€ Kellogg said. â€œSystems in the past didnâ€™t have security functionalities, but it enables your company to do more. If your car has brakes, it enables you to go faster.â€"},
{"title": "The Coming AI Hackers. How Will They Put Society At Risk?", "article": "Bruce Schneier is an internationally renowned security technologist, author, fellow at the Berkman Klein Center for Internet and Society at Harvard University, and a lecturer in public policy at the Harvard Kennedy School. In this episode, he joins host Hillarie McClure to discuss his latest research and paper â€œThe Coming AI Hackers.â€"},
{"title": "No Name Podcast with Bruce Schneier", "article": "Bruce Schneier spoke with Ruslan Kiyanchuk on the No Name Podcast."},
{"title": "Is the Future Secure?", "article": "This week on The Futurists we get into the future of cybercrime and personal security in the smart world with renowned â€œsecurity guruâ€ Bruce Schneier. The author of over a dozen books (his latest bestseller being ), Lecturer on Public Policy at Harvard Kennedy School, Congressional advisor and Media personality. Will AI and Quantum kill passwords? How secure will your DNA records be? The answers might surprise you."},
{"title": "Bruce Schneier on His New Book, a Hackerâ€™s Mind", "article": "Bruce Schneier: These threads have been percolating in my head for a while now. I started writing about the psychology of security around 2008. That quote is something I have been saying for decades. The notions of socio-technical systems and how they can be attacked are just as old. In I am taking what we know about hacking in the computer field and applying it to our broader world: laws, economics, politics, society. Our world is both complex and technical, and taking advantage of rules is common everywhere. (As an example, think of tax loopholes.) I am drawing out that idea, and adding notions of wealth and power. Bruce Schneier: First, thank you for that. One of the most fun things about writing the book were the stories of hacking: from religion, from sports, from politics, from finance, from casino games Â… on and on and on. We humans have always been clever hackers. If there is a set of rules standing in our way, we try to get around them. Technologyâ€”or, more accurately, people using technologyâ€”exploits many of our cognitive systems. Social media companies exploit how our brain decides what to pay attention to, and how we become addicted to things. Terrorism exploits fear. Modern politics exploit both authority and tribalism. Itâ€™s not that anything here is new, itâ€™s that technology allows these exploitations to scale to an unprecedented degree. I think of all of these as hacks of our cognitive functions. Our brains are optimized for the environment we experienced living in small family groups in the East African highlands in 100,000 B.C.E. Weâ€™re less optimized for Boston in 2023. Bruce Schneier: AI is going to change our world unlike anything weâ€™ve experienced in our lifetimes, and it would be better if they werenâ€™t optimized for the near-term financial interests of a handful of tech billionaires. The ethical concerns are huge, and important. Regulation is vital. Deliberation is vital. Understanding what we want from our future is vital. The six-month moratorium is a red herringâ€”I donâ€™t think any of the signatories expected it to actually happenâ€”but itâ€™s a useful starting point for a conversation. I am advocating for an AI public option: a model funded by the government, designed in the open, and freely available. That will be an important counterbalance to the corporate models, and one that will become increasingly important as these AI systems start affecting how democracy functions. Bruce Schneier: Thatâ€™s the question that: 1) no one has any idea how to answer; and 2) is desperately trying to answer. Itâ€™s clear that AIâ€”ignore generalized AIâ€”is going to dramatically change many aspects of our lives. And while we can point to a few of them, we really donâ€™t know the extent of those changes. And we have no idea of the social changes that will result from those technological changes. Will it dismantle traditional power structures? If previous technological revolutions are any guide: yes. How? We donâ€™t know. What I can tell you is to be cautious about what the current AI systems have to teach us about the future. We know a lot about the strengths, weaknesses, and limitations, of a particular large language modelâ€”ChatGPT. That tells us nothing about in five years, or next year, or even the end of this year. Research is moving so fast, and things are being deployed at such a breakneck pace, that itâ€™s all going to change again and again. Never say: â€œthe AI cannot do X.â€ Remember to say: â€œThe AI I played with in Spring 2023 canâ€™t do X.â€ Bruce Schneier: We canâ€™t ban TikTok. We donâ€™t have the Internet censorship infrastructure necessary to enforce such a ban, and weâ€™re not going to build a China-level surveillance system in order to do so. We could ban U.S. companies from doing business with ByteDanceâ€”the company that owns TikTokâ€”but that wonâ€™t result in anyone not being able to use the service. (It would get the app off most peoplesâ€™ phones, so thatâ€™s a thing.) So largely, I view all the banning talk as posturing. Nothing TikTok does is different from what Google and Facebook do. Even the algorithms pushing people towards extreme content: we see the same things out of the Facebook and YouTube (owned by Google) algorithms. We have built an Internet that runs on surveillance. It seems kind of late for us to start complaining about who in particular is doing that surveillance. If we are serious about the risks of TikTokâ€”and there are risksâ€”I would like us to address surveillance capitalism as a whole. All of these companies are spying on us constantly, and all of them are using their recommendation algorithms to push us towards extreme content because thatâ€™s what makes them more money. The problem isnâ€™t China; itâ€™s the business models weâ€™re allowing."},
{"title": "Bruce Schneier Wants to Recreate Democracy", "article": "Arguing that American democracy has been hacked, the computer security expert doesnâ€™t want to just fiddle on the margins when it comes to re-envisioning what a new 21st-century American democracy should look like. Like many people cooped up at home during COVID-19, Bruce Schneier had a pandemic project. In this case, it was a new book called , which encourages readers to apply the hacker mentality to our various social, political, economic, and legal systems. Schneierâ€™s work on the book sparked deeper thinking about the suitability of our centuries-old democratic processes and institutions and whether they were still up to the task in our ever-increasing polarized and fractured political climate. â€œDemocracy has been hacked, mostly for the worse,â€ Schneier, a computer security specialist and privacy expert who is a faculty affiliate at the Ash Center, is quick to note. â€œOur democracy in the United States is really just not suited to the task anymore.â€ But if American democracy is no longer up to snuff in Schneierâ€™s mind, the question quickly arises: What should a new American democracy look like? â€œI donâ€™t have the details, but we need to set about building newer, more resilient democratic systems that are better suited for our current technology,â€ he says. To better tackle the challenge head-on, Schneier opened his voluminous address book and invited dozens of scholars, practitioners, advocates, and writers for an at the Kennedy School in December to discuss how, in essence, to recreate American democracy. Schneier revels in bringing together thinkers from all walks of life to tackle seemingly intractable political, policy, or technology problems. â€œI wanted to have political scientists, law professors, other social scientists, and academics. They all bring important perspectives to this question. But I wanted to hear from individuals you wouldnâ€™t traditionally associate with a Kennedy School workshop, like science fiction writers who have their own unique way of looking at problems and imagining solutions.â€ The , which covered ground far beyond the typical academic confab on democracy, examined fundamental questions about the structure of our democratic institutions. â€œI wanted to have discussions centered around issues that arenâ€™t typically in the spotlight, but get at the fundamental nature of our democracy, such as whether we should even have elected representatives, or maybe just vote for ideas and goals.â€ To frame the exercise, Schneier urged participants to â€œimagine youâ€™ve landed on a foreign planet and need to govern yourselves. How would you set it up?â€ He admits that it may be hard not lean on preconceived notions of what contemporary democracies should look like, but the exercise is intended to get people to think about a different set of ideas. â€œWhat weâ€™re missing now are ideas. We know what to do with our current system, whether itâ€™s reforming the electoral college or making election day a national holiday. But if we could sweep the board and start from scratch, how would you do it? That to me is whatâ€™s interesting.â€ Participants considered proposals for building new democratic spaces that could take the place of the sclerotic governing institutions that many argued are no longer capable of meeting the needs of modern society. â€œWhat are the alternatives? Citizen assemblies? How do we build an enduring system of collective governance,â€ Schneier recalls. â€œWe arenâ€™t talking about democracy 2.0 or 3.0, weâ€™re looking well ahead to 6.0 and 7.0.â€ As a public-interest technologist, Schneier is quick to note that the challenges facing our democracy canâ€™t just easily be chalked up to technology. â€œI donâ€™t think itâ€™s entirely a technology problem. And there was this tension in the room about whether technology is part of the solution. And I think it has to be. We live in a technological world.â€ To illustrate how assumptions about technology are embedded in the bedrock of American democracy, Schneier points to the very nature of representative government itself. â€œI came in with this thought that the modern constitutional republic is the best form of government mid-18th century technology could invent. Because travel and communications are hard, we need to pick one of us to go all the way over to a capital city and pass laws in our name. But that paradigm has been erased. Congress, statehouses, and town halls over the last three years suddenly realized they donâ€™t need to physically convene in a building to debate and pass laws. Now that travel and communications are easy, are there other ways?â€ Looking back, Schneier is quick to note the enormity of the challenge laid out before participants at the workshop. â€œPeople have been thinking really smart thoughts about this for thousands of years, whereas social media is only a few decades old,â€ he admits. â€œHumans havenâ€™t changed, even though technology has changed. And weâ€™re still searching for solutions. And that is very sobering.â€ Yet, Schneier is confident that there are solutions to be found. Another interactive and facilitated workshop is scheduled for May with additional future sessions already in the works. And he doesnâ€™t want the conversation to start and stop in Cambridge. To help others imagine the possibilities for recreating democracy, Schneier is already imagining what a public â€œAtlas of Democraciesâ€ might look like."},
{"title": "History of Hacking", "article": "I spoke about the history of hacking with Steve Morgan of the Cybercrime Magazine Podcast."},
{"title": "A Hackerâ€™s Mind. New Book. Bruce Schneier, Security Technologist and Cryptographer.", "article": "Bruce Schneier is a public-interest technologist, working at the intersection of security, technology, and people. He is the New York Times best-selling author of the book, â€œData and Goliath,â€ and author of the new book, â€œA Hackerâ€™s Mind: How the Powerful Bend Societyâ€™s Rules, and How to Bend Them Back.â€ In this episode, Schneier joins host Steve Morgan to discuss his new book, deep dive into a few chapters, and more."},
{"title": "Interview: New Threats to the Internet Infrastructure", "article": "Jean Friedman interviewed Bruce Schneier about his talk at RSA 2012."},
{"title": "\"Click Here To Kill Everybody\" Book Review by Cybersecurity Expert Scott Schober", "article": "Forget the fact that this esteemed security expert is also a cryptographer and author of seminal cybersecurity books including Data and Goliath and Liars and Outliersâ€¦does Click Here to Kill Everybody live up to its own hype or is is just all theatrics? Although Iâ€™ve never met Bruce Schneier, I can gather from his personality and the way my colleagues speak of him that he is the security expertâ€™s expert. Up until June of this year, Bruce was the CTO for Resilient Systems, a private company that offered incident response solutions. Basically, IBM saw that they were doing good work cleaning up corporate security messes all over the infosec world and entered into an agreement with them not too long before acquiring them back in 2016. Schneier, their CTO had already made a name for himself as a fellow at the Berkman Center for Internet and Society at Harvard Law School and also as a burgeoning writer of many technical publications on cryptography and books on cybersecurity. But those credentials donâ€™t necessarily translate into great reading of 300+ pages of a cybersecurity tome so how does Click Here To Kill Everybody: Security and Survival in a Hyper-connect World fare? Schneierâ€™s take on current and future security trends is so detailed, I sometimes needed a notepad to keep up. His meticulous reporting, cross referencing and modeling of security trends and breaches is to be admired although Iâ€™ll admit, I sometimes felt like I was sitting in one of his classrooms at Harvard. Schneier sets up his entire book by introducing Internet plus, a kind of idealized version of our current Internet. Here, it feels like Schneier is riffing off of common marketing speak including brands such as the Internet of Things. Brands like these promise minimal consumer frustration but actually deliver much more frustration than even archaic communications platforms such as email and SMS text do. This is because for all of IoTâ€™s promises and standardization, we the consumers get mostly cheap but unsecured devices for our dollars. An empty smart fridge is great if it can tell me exactly what I need to shop for but whatâ€™s the point if it can also be easily hacked to spread malware through other connected devices and eventually steal my passwords, data and more? Schneierâ€™s Internet Plus relies heavily on education, standards and regulation. Throughout this book, Schneier is not afraid to invoke strong government regulation as a means to both stimulate competition and encourage things like research and job creation. I find his reliance on governance a little refreshing but some of his suggestions such as comprehensive software product liability measures might overstep his optimism a bit. But how can the man that titles his book Click Here To Kill Everyone even be considered an optimist? One of the most enjoyable aspects of this book was the authorâ€™s adept warnings mixed with caution and optimism for our digital future. This book is not the work of a cynic. Otherwise, Schneier wouldnâ€™t have detailed numerous solutions for myriad problems the cybersecurity community faces as a whole. Click Here To Kill Everyone is an absurd premise when taken literally but as Bruce Schneier details the problems, it steadily becomes a grim possible reality. Fortunately, experts including the author feel confident that our collective security woes are not too great to overcome in the future. I give this book 5 out of 5 stars. Stay safe."},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": "Liars & Outliers", "article": "Iâ€™ve been a Bruce Schneier fan for years. I read his blog often enough that I donâ€™t feel the need to read his books. But then he offered a discount on a signed edition of his latest bookâ€“ with the one stipulation that I So hereâ€™s the review. A lot of brilliant thinkers tend to get stuck in their own perspective. There are plenty of mathematical geniuses who canâ€™t contemplate the implications of their ideas. Plenty of programmers who canâ€™t understand why users donâ€™t recognize the brilliance of their user interfaces. Bruce Schneier isnâ€™t one of them. His rose to fame with a book of algorithms. But because great encryption doesnâ€™t help when itâ€™s part of weak security, heâ€™s written written with increasing breadth about security. After 9/11, he wrote about how we individually and as a society make poor security trade-offs. Now heâ€™s written This takes an even wider view, as he asks: how is it that people can trust each other at all? For example, a man gets into a taxi in a foreign city. The man and the driver will never see each other again. The taxi driver could easily rob him and get away, or he could ride without paying. And yet every day all over the world, drivers and passengers can trust each other. When I got this book, my first thought was After all, Schneier has said he wants this to be read by all sorts of decision makers. Iâ€™ve heard that if you want to be read by busy people, make your book just long enough to be read in a single long plane trip. Then again, Iâ€™m not a best-selling author. But it turns out this isnâ€™t such a long read, itâ€™s 250 pages, plus another 100 pages of notes and references. (This book-to-notes ratio is up there with , which has a remarkable amount of overlap with ) Itâ€™s really two books in one: a quicker, drier, less technical read without endnotes, or a more colorful read with endnotes. I kept one bookmark in the endnotes at all times. is an overview of trust and cooperation. It draws primarily from sociology, economics, and psychology with a big helping of evolutionary biology, game theory, and security. Philosophy and theology also show up, to add a bit of color. Itâ€™s about the different strategies for enforcing conformity in a group, and when they do and donâ€™t work. This is practical stuff for anyone who needs to manage a large group, whether itâ€™s an online discussion group, a corporation, or a country of taxpayers. The book takes pains to discuss this from a neutral perspective: the forces are the same whether itâ€™s a just society versus murderers or a murderous society against saints. To this end, Schneier uses the rather awkward term â€œdefectorâ€ to refer to rule breakers. The book is summed up quite well in the first chapter. In fact, if youâ€™re in a hurry you could just read the first two and the last chapter. Occasionally the book starts to read like a taxonomy, as Schneier explains the breadth of, for example, security techniques. This is an academic book tryingâ€“ and usually succeedingâ€“ in being a general-audience book. While it does drag a little at times, there are plenty of popular just-for-fun nonfiction books that drag more. (Iâ€™ll admit, though I read a lot, Iâ€™m not much of a book reader.) And with the whole history of deceit and treachery to draw upon, he has plenty of colorful examples. I feel like Iâ€™ve said a lot about the book without saying much about the contents of the book. As someone who not only reads Schneierâ€™s blog, but reads many of the same sources he draws upon, there werenâ€™t many ah-ha moments for me. Perhaps the biggest one is this: trust is rare in nature. Itâ€™s hard to establish and easy to break, but once established it yields huge benefits. And humans are the most cooperative and trusting species on Earth. Why? In part because we have the greatest capacity to evaluate reputations, so we know when not to trust. In part because of a sense of morality which leads us to punish rule breakers. I hear plenty of people argue that corporations are inherently inhuman and inhumane. While Schneier doesnâ€™t say that exactly, he spells out each of the pressures society uses to enforce conformity to social norms, and how corporations respond to only two (reputational and institutional [legal.]) The overarching message of the book is that there are different ways of establishing a trusting relationship, and they work on different scales. Neighbors directly evaluate each other. People who know each other only by reputation can go by that. Less intimate groups use morals and institutions to maintain group norms. And when all else fails, security mechanisms can make non-compliance difficult. None of these completely eliminate non-conformity, which is good because even good rules can have bad consequences. The trick is using the right toolsâ€“ and the right amount of pressureâ€“ under the right circumstances. Itâ€™s depressing to consider all the ways that things go wrongâ€“ from suicide bombers to self-destructive investment banks to a lack of a global response to global warming. But at the same time, humans are amazing creatures in that we even have a cooperative arrangement as abstract as the United Nations or a coalition of corporations."},
{"title": "Summit 2019: Cybersecurity and Public Interest Tech with Bruce Schneier", "article": "Bruce Schneier is an internationally renowned security technologist who has testified before Congress and served on several government committees. Schneier is a fellow at the at Harvard University, a Lecturer in Public Policy at the , and a special advisor to . At this yearâ€™s , he will give a keynote presentation on bridging the gap between technology and public policy, in the cybersecurity world and beyond. The theme of Summit 2019 is â€œDesigning better governmentâ€â€”what does that look like to you? When I envision successful government in the information age, I see a government that is not afraid to try, fail, and iterate. I see this as a major cultural difference between policy makers and technologists. Technologists are not afraid to fail; failure is even a badge of honor, because it means that you tried and learned something. Technologists are not afraid to iterate; they know that itâ€™s easier to get something right by version 3.0 if you can learn from the mistakes of versions 1.0 and 2.0. Policymakers donâ€™t think like this. They want to get it right the first time. They know that if they get it wrong, their community is just as likely to abandon the idea entirely as to figure out how to improve it. This has to change. Whether itâ€™s bringing the idea of agile development to legislation and regulation, or just bringing to government the idea that failure is a form of learning, policymakers need to think more like engineers. They need to see their work as solving problems, and not fighting for their preconceived solutions. How does designing better government relate to your work? I am a security technologist, and also a public interest technologist. I try to bridge the gap between technology and policyâ€”in the world of cybersecurity and more broadly. I write about tech policy, and I teach cybersecurity to public policy students. In my way, I am helping to bring about an ecosystem where public interest technology is as viable a career path as public interest law is. What are the biggest challenges that our movement is facing today? Change is hard. Government has a set way of doing things, one that has evolved over generations. It works at speeds much slower than technology advances, and is suspicious of the â€œcreative destructionâ€ model that technology embraces. This isnâ€™t necessarily a bad thing. Technology companies are quick to jettisonâ€”or at least ignoreâ€”many of the rules we have in place to protect minority voices and marginalized communities in favor of the majority and the powerful. Getting that right is a big challenge, and not one we can ignore in our rush to build the future. Bridging technology and public policy is hard. There arenâ€™t enough people who want to do this work, and there arenâ€™t enough jobs for the few who want to. The money isnâ€™t there. The desire isnâ€™t there. But this is how we are we are going to solve the major public-policy problems of the information age. In the second half of last century, the fundamental question of society was this: â€œHow much of our lives should be governed by the state, and how much by the market?â€ Answering that question dominated both foreign and domestic policy, and is why institutions like the Harvard Kennedy School (where I teach) is filled with economists. In the first half of this century, the fundamental question of society is this: â€œHow much of our lives should be governed by technology, and under what rules?â€ Answering this question will require public interest technologists. What are you excited for about Summit this year? I am excited to talk to a group of technologists working in the public interest. There arenâ€™t a lot of usâ€”despite how large this Summit isâ€”and we come from a diversity of backgrounds. I want to meet the people who make up the Code for America community and learn how they are bridging the divide between technology and public policy. By the way, if you want to learn more about public interest technology, I maintain a resources page at . There you can read my writings on the subject, watch the videos from a one-day mini-track I hosted on the topic at the RSA Conference this year, read the reports that Freedman Consulting wrote for the Ford Foundation, look at all the university programs in this area, and see a list of all the NGOs that are doing public-interest tech. If you have any additions, please send them to me at . Iâ€™ll see you in Oakland!"},
{"title": "13 Security Myths You'll Hearâ€”But Should You Believe?", "article": "Security Myth No. 1: â€œMore Security is Always Better.â€ Bruce Schneier, security expert and author of several books, including his most recent, , explains why this security concept of â€œyou canâ€™t get enoughâ€ thatâ€™s often bandied about is off the mark to him. Schneier explains: â€œMore security isnâ€™t necessarily better. First security is always a trade-off, and sometimes additional security costs more than itâ€™s worth. For example, itâ€™s not worth spending $100,000 to protect a donut. Yes, the donut would be more secure, but it would make more sense to simply risk the donut.â€ He also notes that â€œadditional security is subject to diminishing returns. That is, measures that reduce a particular crimeâ€”say, shopliftingâ€”by 25% cost some amount of money; but additional measures to reduce it another 25% cost much more. There will always be a point where more security isnâ€™t worth it. And as a corollary, absolute security is not achievable.â€ Sometimes security may even become a moral choice and being in compliance might be an immoral decision, as it could pertain to a totalitarian system, for example. â€œSecurity enforces compliance, and sometimes complying isnâ€™t the right thing to do.â€"},
{"title": "2006 Dr. Dobb's Journal Excellence in Programming Award", "article": "The Excellence in Programming Award is an annual award that acknowledges individuals who, in the spirit of innovation and cooperation, have made significant contributions to the advancement of software development. Past recipients include leaders and thinkers in the development community such as Linus Torvalds, James Gosling, Erich Gamma, Guido van Rossum, Jon Bentley, Anders Hejlsberg, P.J. Plauger, and Guy Steele Jr., among others. This yearâ€™s recipientâ€”Bruce Schneierâ€”is unique in that he has long been a member of the s family, so to speak. But while Bruce has been a contributing editor, columnist, and writer for , thatâ€™s not why he is receiving the award. Bruce is this yearâ€™s award recipient because of the many important contributions he has made in his chosen specialty of computer security. These include designing the Blowfish and Twofish encryption algorithms, both of which he wrote about in and the latter of which was a finalist for the Federal Advanced Encryption Standard. Bruce is also the author of eight books, including , which is a seminal work for software developers. Bruce is the founder of and chief technical officer for Counterpane Internet Security. He has served on the board of directors of the International Association for Cryptologic Research, and is an Advisory Board member for the Electronic Privacy Information Center. Bruce holds a Bachelorâ€™s degree in physics from the University of Rochester, and a Masterâ€™s degree in computer science from American University. In short, by never wavering from focus on technical excellence and open communication, Bruce Schneier represents the finest the software development community has to offer."},
{"title": "Twofish Heads to Washington", "article": "A team led by author Bruce Schneier has invented a new block encryption algorithm and submitted it for consideration as the next new federal government standard for data scrambling. , the sequel to Schneierâ€™s 5-year-old Blowfish block cypher, was submitted last week to the National Institute of Standards and Technology (NIST) for consideration as the Advanced Encryption Standard. Twofish is designed to be flexible with respect to the necessary performance tradeoffs between the creation of a â€œsecret keyâ€ and execution of the actual encryption. As such, it is well suited to large microprocessors, smart cards, and dedicated hardware. â€œWe designed Twofish with performance in mind,â€ said Schneier, president of the security consulting firm Counterpane Systems. â€œYou can get faster encryption with more setup time, or you could do no setup and get slower encryption,â€ he said. â€œYou can trade off efficiencies between hardware, software, RAM vs ROM, encryption speed â€¦ all of these are interoperable,â€ Schneier said. The free, publicly available algorithm is suitable for 128-, 192- or 256-bit key lengths. The Advanced Encryption Standard is designed to replace the current government specification, known as the Data Encryption Standard, which was first introduced in 1977. In submitting Twofish, Schneierâ€™s group joins about a dozen others aiming for the NIST standard, including IBM, RSA Data Security and Cylink."},
{"title": "Everything about IT Security Will Change", "article": "Bruce Schneier, leading cryptologist described as a â€œsecurity guruâ€ and a â€œleading counterterrorism contrarianâ€ by the media, shares his thoughts about the future of information security. â€œCrime, Crime, Crime!â€ Bruce Schneier is adamant when asked to talk about the worst security threats. Itâ€™s not coming from fanatics, but from people out to steal for money, he insists. â€œIt doesnâ€™t matter what form it takes,â€ he says. â€œItâ€™s wrong that we defend ourselves against the tactics, because then these guys change tactics.â€ He describes a worst scenario where â€œthe crime is so bad that people stop doing commerce on the net.â€ Information security is there to prevent this from happening. As a leading cryptologist, Schneier is the CTO of BT Counterpane, a security service firm and author of . He believes the security industry will undergo a transformation: â€œIn a vibrant security market, security research and security companies no longer sell to consumers.â€ He believes that end users will soon expect that services they use over networks, such as online banking, will come with a guarantee of security from the service provider. â€œThe hardest thing about working in IT security is convincing users to buy our technologies,â€ Schneier says. â€œAn enormous amount of energy has been focused on this problemâ€”risk analyses, ROI models, auditsâ€”yet critical technologies still remain uninstalled and important networks remain insecure.â€ Schneier is constantly asked how to solve this by frustrated security vendors but he says he has no good answer. â€œBut I know the problem is temporary,â€ he shares. â€œIn the long run, the information security industry as we know it will disappear.â€ Security as Utility Schneier thinks the entire IT security industry is an artifact of how the computer industry developed. â€œComputers are hard to use, and you need an IT department staffed with experts to make it work.â€ Whereas for other mature high-tech products, such as those for power and lighting, heating and air conditioning, automobiles and airplanes, the job of installation and maintenance is outsourced as a service. â€œNo company has an automotive-technology department, filled with car geeks to install the latest engine mods and help users recover from the inevitable crashes,â€ he comments. According to Schneier, IT is heading in that direction of becoming a utility where users are buying more services than products. â€œBy their nature, services are more about results than technologies,â€ he says. â€œService customersâ€”from home users to multinational corporationsâ€”care less about the technological specifics and just expect IT to work.â€ Counterpane, the internet security company that Schneier formed eight years ago on the premise that â€œlarge IT departments donâ€™t really want to deal with network securityâ€, was acquired by BT last year to have their network security services embedded in the service portfolio. â€œMany customers donâ€™t want to deal with network management at all; they just want the network to work,â€ he says. â€œThey want the Internet to be like a phone network, a power grid, or a water systemâ€”in short, they want it to be a utility.â€ Schneier goes on to explain that for these customers, security isnâ€™t even something they purchase, but a small part of a larger IT services deal. Thatâ€™s why IBM has bought ISS and EMC has purchased RSA -to create a more integrated solution for customers. â€œSomeone is going to buy Symantec,â€ Schneier says firmly. â€œAnd someone is going to buy Network Associates.â€ Bruce Schneier Schneier uses email as an example as some corporations have outsourced their corporate email to companies like Google. â€œIf you have a new email security solution, convincing Google to embed it in its email service is far more efficient than trying to sell it to users.â€ He believes when the IT industry matures, thereâ€™ll be no point in user conferences like InfoSec and RSA. â€œThey wonâ€™t disappear; theyâ€™ll simply become industry conferences,â€ he says. â€œIf you want to measure progress, look at the demographics of these conferences. A shift toward infrastructure-geared attendees is a measure of success.â€ Meanwhile, security products wonâ€™t disappear. â€œThere will still be firewalls, antivirus software, and all sorts of new technologies and products,â€ notes Schneier. â€œBut users wonâ€™t care about them.â€ Instead, the new technologies will be embedded within the services sold by large IT outsourcing companies or ISPsâ€”â€just like new automotive technologies are marketed to automobile manufacturers, rather than individual car owners.â€ Schneier believes this is progress. â€œAs IT fades into the background and becomes just another utility, users will simply expect it to work,â€ he notes. â€œThe details of how it works wonâ€™t matter.â€ Security will become a commodity, â€œjust like an airbag in a car.â€ Schneier explains that in the US companies advertise their airbags as a differentiator. â€œSecurity will come out as a critical differentiator. You buy a product because it comes more secure,â€ he says. â€œIt wonâ€™t be a separate thing, but itâ€™s there to make a difference.â€ Schneier goes on to explain that when customers buy a service, they donâ€™t need to tick a check box on whether they want securityâ€”security comes as part of what the service provider is offering. â€œNobody wants to buy a house because of a door lock,â€ he says. â€œBut you will never buy a house that didnâ€™t come with a door lock.â€ Schneier also believes that security as utility solves the problemoffinger-pointingwhich is now happening around a lot of security issues. â€œIf a company provides both, it doesnâ€™t matter whose fault it is but they need to make it good.â€ Security Signals Schneier explains that for end users, because they are not experts in security, they will rely on the â€œsignalsâ€â€”such as consultants, certification and analysesâ€”to help them make decisions. â€œItâ€™s dangerous for customers to rely too much on this signal, but there are no better options,â€ he explains. â€œYour option is either learn it yourself or buy after the signals.â€ Schneier uses the analogy of an ill person seeing a doctorâ€”â€what are your other options? Learn medicine?â€ he explains. â€œYou could, but itâ€™s not realistic.â€ â€œI have no clue about pharmaceuticals, so I have to trust a doctor.â€ Schneier also believes vendors should be held responsible for security failure because â€œthere is no other mature industry on the planet where this is not true.â€ He uses the electricity industry as an example to support his viewpoint and points out there are many liability models organisations can work on. Security as Part of Risk Management Schneier says the security people in the company wonâ€™t be obsolete: â€œthey may have different bosses, but they are not going to go away.â€ While technical people will more likely be moved to some of the outsourcing companies, he also affirms that CIOs will not disappear from companies, because â€œyou can never outsource the business risk management, you just outsource how it works.â€ Schneier also asserts that there is currently too much focus on technology. â€œBusinesses are seeing security just as IT or information security,â€ he adds. â€œThatâ€™s changingâ€”you see a lot of security go into general risk management.â€ Security, ultimately, is part of enterprise risk management and business continuity. He says though IT person is the best one to be in charge of IT security, someone above him has to approve his budget and that person will not be an IT person, â€œso that the IT person has to speak a language of the people above you.â€ However, the overall risk management stretches much more than merely IT. â€œYou need to be secure against burglary, you need to consider risks of fire and hurricane,â€ Schneier says. â€œAll those are basic business risks and information security should be seen as part of this larger landscape.â€ Bruce Schneierâ€™s Top Ten Trends in Information Security It always has had value but we never realised before, or at least not in the same way we do now. For a lot of banks, their database has become their only sellable asset. The recent blackout in the United States saw lots of companies take long time to resume operations, not because of the power outage, but the data outage. Most legal statements are designed for owners of the property, but nowadays you donâ€™t have the paper anymore, you have an outsourcing company. When your data resides on an outsourcerâ€™s server, your own rules of security donâ€™t apply anymore. Things get more complex, and less secure; the internet is the most complex machine humankind has ever built. There is an arms race. You wait a couple of years, computers get better, but security gets worse. When the security gets better, the system gets more complex, we lose the ground. Threats have evolved from hobby threats to more criminal threats: Identity theft, fraud due to impersonation, bot networks. There is a career path to become a hacker. The command and control of worms is so sophisticated that we donâ€™t know how to take it down. The criminal worm is quiet, unlike hacker worms. Lots of embedded systems are now out in the market. Patches have to be released fastâ€”thatâ€™s fundamentally impossible as on-the-spot patching canâ€™t have both quality and speed. In 2004, Microsoft started once-a-month patchingâ€”they release patches on a schedule, which has proven to be effective. People are squandering too much effort on technologies such as SSL and PGP to protect the communications channel, while end points are now easier targetsâ€”hackers try to get decrypted information sitting on the harddisks. Secrecy is not securityâ€”something that is truly secure should be made public. For example, if the value of an encryption algorithm depends on its secrecy, all security will be lost once itâ€™s no longer secret. Most security protects you from the bad guys; but now there are security roadblocks on your computer to prevent you from doing â€œbadâ€ things. In 2005, Sony secretly installed a root key in music CDs to prevent users from copying the songs. The company viewed viewed users as potential attackers, and is an example of â€œI can either protect you or protect against youâ€ thinking. The root kit had a side effect of opening the PC to malicious attack. Greed and fear are the two things based on which you can sell something; security has already been and is fundamentally a fear sale; regulation gives IT guys a stick to force their manager to give them budget; lots of money goes to regulations, but not security itself."},
{"title": "Going Meta: A Conversation and AMA with Bruce Schneier", "article": "In this episode, Perry Carpenter interviews cybersecurity guru Bruce Schneier. Perry and Bruce explore how cybersecurity is about so much more than technologyâ€”Itâ€™s about people, so we benefit by taking a multidisciplinary approach. In preparing for this interview, Perry solicited his LinkedIn network to see what questions people had for Bruce. This is a wide ranging conversation covering everything from Bruceâ€™s thoughts on cybersecurityâ€™s â€œfirst principlesâ€ to the impact that the pandemic had on society to need for regulation to help raise the overall standards for security and privacy. Transcript Hi, Iâ€™m Perry Carpenter, and youâ€™re listening to 8th Layer Insights. This is a cybersecurity show about people and human nature and how we canâ€™t afford to ignore human nature in the way that we designed security and in the way that we build our programs. Todayâ€™s episode is going to be a little bit different than previous episodes but, if I were to summarize the theme of todayâ€™s episode and use one word to describe it, that word would be synthesis. Now, if this was a nature documentary, I might open with some type of imagery depicting a chemical combination that has this visually stunning reaction, or I might show an animation of molecules combining to form a new molecular structure. But, this is a podcast and our virtual paintbrushes are sounds and ideas, and those introductory establishing shots can be really hard, especially when talking about an abstract topic like synthesis. So, letâ€™s jump ahead and get straight to the point, because youâ€™re probably asking yourself â€œwhat is all of this talk about synthesis?â€ and â€œwhat do we mean in this context?â€ Let me explain. Synthesis is really what 8th Layer Insights is all about. And by that, I mean the heart of this podcast, the intent behind it, is to bring ideas and concepts together from several different disciplines, covering a wide swath of topics, all coming together to shed light on the human condition as it relates to security and risk. In the couple of decades that Iâ€™ve been involved in cybersecurity, Iâ€™ve seen the industry mature quite a bit, but one of the stumbling blocks that we seem to face again and again and again, is that we tend to believe that somehow cybersecurity is different than everything else. Itâ€™s its own unique animal. Itâ€™s so unique, so new, that we have to figure things out ourselves. And that can have the effect of creating tunnel vision and echo chambers, where we donâ€™t think to learn from the past or look at how other disciplines have grappled with similar problems, or even ask ourselves how our new fangled security controls will collide with human nature. Sometimes we focus on the individual trees so much, that we forget about the forest and the entire ecosystem that drives the forest and sustains the forest. And that reminds me of a story. You may have heard it before but, even if thatâ€™s true, go ahead and listen again with fresh ears. This is the story of the blind man and the elephant. The blind man and an elephant. The earliest versions of the parable of blind men and elephant is found in Buddhist, Hindu and Jain texts, as they discuss the limits of perception and the importance of complete context. The parable has several Indian variations, but basically goes like this. A group of blind men heard that a strange animal, called an elephant, had been brought to the town, but none of them were aware of its shape and form. Out of curiosity, they said, â€œwe must inspect and know it by touch.â€ So, they started out and found it. The first person whose hand landed on the trunk said, â€œthis being is like a thick snake.â€ For another one whose hand reached its ear, it seemed like a kind of fan. As for another person, whose hand was upon its leg, said â€œthe elephant is a pillar, like a tree trunk.â€ The blind man, who placed his hand upon its side, said â€œthe elephant is a wall.â€ Another, who felt its tail, described it as a rope. The last felt its tusk, stating â€œthe elephant is that which is hard, smooth, and like a spear.â€ There you have it, sometimes we donâ€™t perceive things correctly, because we are too close. We lack context and that lack of context means that we make mistakes. Back to you, Perry. So, we make mistakes when we lack context, and thatâ€™s where big picture thinking becomes so critical, and big picture thinking and synthesis and meta analysis is what todayâ€™s guest is known for. If youâ€™re a cybersecurity professional, itâ€™s very likely that youâ€™ve heard the name Bruce Schneier before, and itâ€™s hard to underestimate Bruceâ€™s impact in the field of cybersecurity. This is so much so that most intros for him end up defaulting to words like â€œguruâ€ and â€œluminaryâ€. He made a big splash in the computer security world as a cryptographer when he published Applied Cryptography back in 1991, but most people donâ€™t know that Bruceâ€™s earliest training was actually as a physicist. He earned a Bachelors in physics in 1984 before moving on to study computer science for his Masterâ€™s degree, which he earned in 1998. Bruce created a few popular cryptographic cyphers, which youâ€™ve probably heard of: Blowfish and Twofish and Threefish among others. In 1999, he invented a cryptographic algorithm called solitaire, which is designed to manually encrypt data using a deck of cards, and this algorithm was a key plot point in Neal Stephensonâ€™s book Cryptonomicon. Bruce even wrote an afterword to that book describing the cypher. Hereâ€™s where the big picture thinking comes in. Despite all of Bruceâ€™s success on the cryptography side of things, he realized something. He realized that cryptography and technology alone will never solve security issues. They face the same issues that the blind men faced when trying to describe the elephant. Over the past couple of decades and change, Bruce has been focusing on security at more of a macro level. Heâ€™s been taking the 30,000 ft and above level, that big picture view and heâ€™ll be the first to tell you that we need to approach security in a multidisciplinary manner. Security is inherently about people. It means a lot of technology, but security has to take people into account, and itâ€™s often economic psychology, sociology non-security topics that explain how security works and fails. Security theater is a phrase I invented post-911 to describe security measures that look good but donâ€™t accomplish anything. This is regulation, it gets a bad name, but actually it keeps us alive. Thereâ€™s a lot of technologies we have for authentication that arenâ€™t being used because the market doesnâ€™t reward it. All men are not angels, all people are not angels. Security is a tax on the honest, and something we have to pay for, even though we get nothing for it, but we get everything for it. Iâ€™m Bruce Schneier, I work at the intersection of security technology and people. Weâ€™ll be touching on several topics with Bruce today but, first, letâ€™s cue the intro. Hi there, my name is Perry Carpenter. Join me for a deep dive into what cybersecurity professionals refer to as the 8th Layer of security: humans. This podcast is a multidisciplinary exploration into the complexities of human nature and how those complexities impact everything from why we think the things that we think, to why we do the things that we do. And how we can all make better decisions every day. Welcome to 8th Layer Insights. Iâ€™m your host, Perry Carpenter. Weâ€™ll be right back after this message. So, whatâ€™s a con game? Itâ€™s a fraud that works by getting the victim to misplace their confidence in the con artist. In the world of security, we call confidence tricks social engineering. And as our sponsors at KnowBe4 can tell you, human error is how most organizations get compromised. What are some of the ways organizations are victimized by social engineering? Weâ€™ll find out later in the show. Welcome back. I mentioned in the opening section that todayâ€™s episode is a little bit different than most of the other shows and thatâ€™s because today we have one guest, and that one guest is driving the agenda. Back before I officially launched the podcast, I solicited LinkedIn with a request for topics and potential guests, and Bruce Schneierâ€™s name came up. Thankfully, he agreed to be interviewed, and because that suggestion came to me from LinkedIn, I reached back out and I asked people what kind of questions they would like to have Bruce answer. This is a little bit of a guided AMA session, an ask me anything session with Bruce. And I think this comes at a great time, because most of us have not been going to conferences this year. and so many of us that have become habitualized to hearing Bruce speak from the stage every year, have not had that. As a result, you might be feeling like his voice has been missing from some of the conversations, and I think thatâ€™s what makes todayâ€™s episode special. What Iâ€™ll be doing is Iâ€™ll be playing clips from the interview and Iâ€™ll be giving some context or some commentary before or after some of these questions, but these are largely Bruceâ€™s words unfiltered and Bruceâ€™s thoughts unfiltered, being brought directly to you. Thank you so much to those of you who submitted questions for Bruce. I hope that you enjoy this interview as much as I did. With that, letâ€™s go to the interview. Iâ€™m Bruce Schneier. I am a security technologist. I do a lot of things. I teach at the Harvard Kennedy school. I teach security to public policy students. I have a company Inrupt, that is trying to commercialize Tim Berners-Leeâ€™s Solid, distributed data ownership technology. I write, I speak, I have a website. Iâ€™m a security technologist. I work at the intersection of security technology and people. Fantastic. You know, Bruce, one of the things that I think people really appreciate about you is that youâ€™re extremely multi-faceted in the way that you think about security and in the way that you opine on security. I want to go philosophical for just a couple of minutes. In philosophy, a first principle is a basic proposition or assumption that cannot be deduced from any other basic proposition or assumption. For you, what would be a couple of first principles for security? Security is inherently about people. It means a lot of technology, but security has to take people into account, and itâ€™s often economic psychology, sociology, non-security topics that explain how security works and fails. Security is a way of incenting people to behave in a certain manner, and we need to understand. That might be my first principle, that security actually is rarely about security, itâ€™s about other things. Okay, if you donâ€™t mind, letâ€™s drill into that for a minute. Youâ€™ve got a pretty famous quote. I think it was in the preface for Secrets and Lies, where you said â€œif you think technology can solve your security problems, then you donâ€™t understand the problems and you donâ€™t understand technology.â€ But then a bit later in a 2013 blog post, you also mention that at the time you felt like training users was a little bit of a waste of time and that the money could be better spent elsewhere. What is the through line between those two opinions? One being that technology canâ€™t solve the problem, so we need to focus on humanity; and the other being that training isnâ€™t necessarily the answer to everything either. Iâ€™m going to pull that apart and give you two separate answers you can use. Thatâ€™s not even my quote. I believe thatâ€™s Roger Needham at Cambridge University who first said that. It is something that we in security say a lot, and I think I popularized it, but I wonâ€™t take credit for it. Okay, quick note, after that reminder from Bruce, I went and looked at the preface to Secrets and Lies again and here is how he phrases it. He says â€œa few years ago, I heard a quotation and Iâ€™m going to modify it here.â€ Then he goes on to say â€œif you think technology can solve your security problems, then you donâ€™t understand the problems and you donâ€™t understand the technology.â€ What I did after that is I went on a search for the original quote and, of course, I used some of those key words and I used Roger Needhamâ€™s name and I eventually found it. It looks like this is attributed to both Roger Needham and then also, in a lot of cases, to Peter G. Newman. The original quote is â€œif you think cryptography will solve your problem, you either donâ€™t understand cryptography or you donâ€™t understand your problem.â€ Okay, back to Bruce for the second part of his answer. Let me do the second now. Our society is so complex that weâ€™ve built it in a way that you donâ€™t have to be an expert in things to take advantage of them. I can get on an airplane â€“ wow remember airplanes â€“ without knowing anything about aircraft, safety, or maintenance or pilot training, or any of that. Right, thereâ€™s an entire infrastructure that I can safely ignore and know that Iâ€™m walking onto a plane and itâ€™s not going to crash. The same thing with cars, pharmaceuticals, restaurants. Society has my back with a lot of expertise so I donâ€™t need it, and computers, networks need to be the same. It canâ€™t be that everyone has to be an information security expert to use a computer securely. Just, it canâ€™t be that everyone needs to be a food hygiene sanitation expert in order to eat at a restaurant and buy at a grocery store. When I think about user training, thatâ€™s what I think about. Why are we training the user? Why donâ€™t we train the experts and have them decide whatâ€™s right, like we do everywhere else. This is regulation, it gets a bad name, but actually it keeps us alive. A lot of user education covers for bad security design. You think about the sort of advice we try to give people. â€œDonâ€™t plug strange USB sticks into computers.â€ Itâ€™s a USB stick. What are you supposed to do with it? â€œDonâ€™t click on URLs you donâ€™t know.â€ Itâ€™s a URL, youâ€™re supposed to click on it. â€œDonâ€™t open attachments.â€ Itâ€™s an attachment. In every one of those cases, thereâ€™s a pretty serious design problem, and why is it that a malicious URL can hack a computer? Or an attachment? Or a USB stick? Those are design issues. Those are failures in how the computers were built, and blaming the user for not doing the obvious thing, doesnâ€™t make any sense. So, I donâ€™t like user education, because it goes hand in hand with use blaming, and because it implies a level of expertise that we shouldnâ€™t expect. Yes, thatâ€™s an insightful perspective because it recognizes that the user behavior problem is actually a technology failure. Itâ€™s a failure of design, but unless and until those fundamental design issues are addressed, the user ends up bearing the brunt of dealing with the issue. It does seem like every year some vendor says that theyâ€™ve solved this issue and user behavior wonâ€™t be something that we have to think about anymore. But, to date none of those vendor promises or technology problems stand up to the ultimate test of reality, where persistent attackers continue to prevail by manipulating the technologies in unexpected ways, or by breaking the tech outright, or by bypassing the technology through social engineering. Give some of your thoughts on how we get to secure behavior by design, or how we deal with these technology based holes that for decades havenâ€™t yet been plugged? We in society deal with these things through government. Weâ€™ve lived through decades where food wasnâ€™t safe, where cars and airplanes crashed all the time, and we changed that through government regulation. When you stare at a security hole or vulnerability, it hasnâ€™t been fixed in 40 years, you have to realize that the system incensed it not to be fixed. Weâ€™ve designed a market where not fixing it is more profitable, so itâ€™s never going to get fixed. If we want more security, we need to require it and we need to require it collectively as citizens, that means government, and then we need to enforce those regulations. I mean this shouldnâ€™t be a surprise. Itâ€™s what we do everywhere else, but somehow in the computer world, we have this weird belief that the market will solve problems even if the market doesnâ€™t reward those solutions. Yes, I think when it comes down to that, thereâ€™s still some complex issues that weâ€™ve been grappling with for a long time. I mean, one of those main things that we grapple with is authentication. I mean, why havenâ€™t we solved authentication yet? Iâ€™m not sure what solve authentication yet means? So, Iâ€™ll ask you what does solve authentication mean? My bank works pretty well, it feels solved. I go home for Thanksgiving, I know everybody. That all seems solved. What do you mean by solve authentication? Yes, to where people canâ€™t easily socially engineer passwords or reroute two-factor authentication tokens or something like that, to where if thereâ€™s a data breach, or if I know something about somebody that I canâ€™t trick that person into giving up keys to the kingdom. So, youâ€™re saying why havenâ€™t we made humans smarter? Weâ€™ve been trying to do that for centuries. Thatâ€™s hard. My guess is just genetic engineering isnâ€™t good enough yet? I mean I think that comes down to part of the problem though. If thereâ€™s a place where training canâ€™t fix it because weâ€™re dealing with very complex systems and the user canâ€™t be expected to do that. Then, how do we get to a situation where the technology is secure by design in the very basic ways that we just interact with our systems? Thereâ€™s a lot there. One, again, my bank works pretty well. So, I think we have it. Where you donâ€™t have it, itâ€™s because it is more profitable for the companies to have lousy security than to have good security. The reason your phone is so easy to hijack, sim swapping, all those tricks where you call up the phone company and pretend to be somebody else and then hijack their phone. The phone company likes it that way. They make more money, because the authentication is bad. Banks make less money, thatâ€™s why bank authentication is better. It really comes down to economics. Facebook makes a lot more money spying on people, having misinformation, than they do fixing any of those problems. We have a lot of authentication systems, and some work better, some work worse, and companies will use the one that is most profitable for them, not the one that is in the userâ€™s best interest. You want to fix that, pass a law. Some of this is inherent. Remote authentication will always be harder than in person. If you and I meet in person, it will be harder for someone else to impersonate you, than it will be if we meet on the telephone, or we meet, I donâ€™t know, in a text app. Thatâ€™s always going to be true. Thereâ€™s a lot of technologies we have for authentication that arenâ€™t being used, because the market doesnâ€™t reward it. Then, it comes back to basic economic incentives and the fact that thereâ€™s not necessarily the regulation in place for some of the advances that we need to make all of this more ubiquitous. From your perspective, weâ€™ve been in various forms of lockdown around the world for the past year. what have we learned about ourselves and security during this time? Has anything stood out to you? I think weâ€™ve learned that a lot of things are possible remotely that we never imagined or never allowed; remote learning, remote medicine. And that weâ€™re not going to go back to before, weâ€™re going to go back to some hybrid. Weâ€™ve learned that organizations can thrive, even without people sitting in an office together, for prescribed hours, and weâ€™re not going back there. And weâ€™ve learned that our network infrastructure is incredibly important to society and thatâ€™s not going back. What it means for security is that more things are going to happen remotely, are going to happen without the same social lubricant that gives us so much security in these settings, and thatâ€™s going to be more dangerous, from personal corporate and national security. We do need to start facing these challenges. Weâ€™re pretty good at muddling through and making things sort of work. We might have to up our game, because looking around the threat actors certainly have upped theirs. Can you go a bit deeper on that? How are you seeing threat actors up their game and take advantage of this more disconnected form of society that weâ€™re in right now? Where does remote work and everything else fit in with that? Supply chain is the new attack factor. We are seeing the Russians, the Chinese â€“ Iâ€™m sure the US are doing the same thing â€“ criminals, all going after the supply chain in different ways. Whether itâ€™s code libraries that end up into a piece of software; update mechanisms; distribution mechanisms. Itâ€™s all being attacked and sometimes the attacks are very fruitful. The Russiansâ€™ solar winds campaign was really impressive. The Chinese attack against Microsoft exchange, was very successful. We are seeing different actors go after the supply chain, so itâ€™s less disconnected and more interconnected. Things are massively interconnected, massively global in a way that makes us more vulnerable. I donâ€™t know whether the pandemic fits into this or if it was just by coincidence. The pandemic made us more vulnerable by changing everything so fast. Criminals especially took advantage of the chaos and confusion in those first few months after the shutdowns mid-March 2020. To take advantage of peopleâ€™s not knowing whatâ€™s going on or how to do things, to sort of trick them into doing things they might not want to do. I think weâ€™re better about that now, but we still are accessing our networks via VPNs, our data is all across the internet at various cloud providers. That brings with it new vulnerabilities, and weâ€™re slowly dealing with those. Fantastic. Letâ€™s go ahead and transition to some of the questions that came in over LinkedIn. Iâ€™m going to give you a bunch of rapid fire questions. If you could just give some of your first thoughts that come to mind. The first one is, if you could put one thing on a billboard for everyone to see related to security, what would that be? Oh wow, what an odd question. Itâ€™s so funny, my first thought is security isnâ€™t free. Because itâ€™s not, you have to pay for security, and we all expect it to be free. The more esoteric way of saying that is security is a tax on the honest. Security is something that the honest have to pay for in order for systems to work. It was, I think, Thomas Jefferson who said this, â€œif all men were angels, then no government would be necessaryâ€, and if all men were angels, no security would be necessary. But all men are not angels, all people are not angels and security is a tax on the honest and something we have to pay for, even though we get nothing for it, but we get everything for it. Alright, itâ€™s a long billboard and pretty philosophical. Yeah, long billboard, but it speaks of the fracturing of the social contract. So, I think that thatâ€™s super, super important to recognize. Alright, if you could change three things about our industry today, what would they be? I think whatâ€™s missing in our industry is government regulation. Right now the security industry is designed with profit in mind. That works well as far as it goes, but there are some enormous market failures that lead to bad outcomes because people are making individual decisions and no oneâ€™s making collective societal decisions. What has been missing for decades in cybersecurity, is government, and I think we need to fix that, and that is the first thing we need to fix. Let me think about number two. I think we need more understanding on human psychology, that security is inherently about people. Too much security has been tech focused and not people focused, and that really means putting social scientists in security development teams. Thatâ€™s psychologists, sociologists, anthropologists, polisci people who know how people work, and I think that will help us build a lot better security systems. The third is, I think we need way more diversity in our industry. That we fall into a lot of bad rabbit holes because this has been the one voice in thinking and designing security. If you watch AI, Iâ€™ve seen a new area of security spring up, which showed the problems of a mono culture. But the diversity and voices that are now talking about AI security, really show the promise of ways of thinking and solutions, and Iâ€™d like to clone that for the rest of cybersecurity. Okay, so this is a divergence from the LinkedIn questions, but since youâ€™ve mentioned regulation several times now, let me ask you this. You believe that regulation is a chunk of the answer, but weâ€™ve seen regulators try to get involved with things like encryption, from letâ€™s say an overly US centric perspective, where they want to essentially break security by adding things like back doors to preserve what they believe, or what the regulators or the intelligence community believe is going to be within the national interest or what is best for them. How do we deal with that when it comes to regulation? Well, this is a problem, right. Regulation should be for security and safety, not for insecurity. The problem with the encryption debate is itâ€™s not regulation for defense, itâ€™s regulation for attack, for offense. Yes, if the department of justice is in charge, if the NSA is in charge, youâ€™re going to get lousy security because everyone likes to attack and no one likes to defend. I see the internet as part of our critical infrastructure, that protecting it is paramount. That is a challenge we have, that we really donâ€™t have any agency that is wholly on the side of defense. I mean in the US, I suppose, we have CISA, but they are still really finding their way, and you do have the offensive nature of cyber much more in the ascended. Iâ€™m just finished on the Nicole Perlrothâ€™s new book, a great book on the offensive cyberspace, with the awesome title of â€œThis is how they tell me the world ends.â€ Her titles are better than mine, which I write great titles, but this is a fantastic title. And itâ€™s something that internationally we need to figure out. Youâ€™re right that kind of US focused, offense focused, anti security regulation is not going to get us anywhere. Back to questions from LinkedIn. Youâ€™ve been outspoken especially post-911, about security theater. Do you have any examples of security theater that youâ€™re seeing today? If you want to expand on that, can you give some ideas on where those might be helpful psychologically, harmful or benign? Security theater is a phrase I invented post-911 to describe security measures that look good but donâ€™t accomplish anything. Random ID checks in buildings, National Guard troops in airports holding guns with no bullets, a lot of examples of that; making people feel better, even though they donâ€™t actually do anything. Now, thatâ€™s not necessarily bad. We want people to have a feeling of security that is line with the reality. Post-911, everyone was scared. The risk didnâ€™t increase that much actually, so a little theater went a long way. In the past year, weâ€™ve really seen a lot of the same thing, Iâ€™ve heard it called hygiene theater. Itâ€™s the scrubbing down surfaces for Covid did absolutely nothing and it made people feel better. I know people who would wipe down their groceries when they brought them home or leave them outside for three days. You know, complete nonsense, made no sense, given the science even back then, and now we know it was a waste of time. But that was a piece of theater. Even today, some of the measures people take outdoors are security theater. We know how the virus spreads, it spreads through the air, it spreads indoors, it spreads with not good airflow. Iâ€™m not worried about people at the beach; Iâ€™m worried about people in the crowded bars in the evening. We saw a lot of that kind of theater in the last year with Covid, and it was interesting to watch what worked, what didnâ€™t, what people did anyway. Some of it is social signaling. I think, right now, we wear masks even though weâ€™re vaccinated to signal to others who donâ€™t know if weâ€™re vaccinated. I walk into a restaurant, of course Iâ€™m going to wear a mask, even if Iâ€™m vaccinated because weâ€™re indoors and we donâ€™t know who else is, and those restaurant workers are really at great risk. So, thereâ€™s a little theater there and thereâ€™s a place where theater is valuable. You talked a little bit about some of the hygiene theater that you saw and potentially some things that may even erode trust in authorities. How do we deal with these fractures in the view of authority in society when youâ€™ve got hundreds of millions of people probably that may not trust the experts as much as they used to, when, in the long run, itâ€™s probably better to trust the experts? How do we start to get some of that trust back? Thatâ€™s an area that I really donâ€™t understand. Itâ€™s so far from my field, itâ€™s not security, itâ€™s really psychology. But youâ€™re right, weâ€™re now living in a world where science is disputed. I think of it as the counter enlightenment, that there is a group of people, pretty skewed along political lines, that donâ€™t trust science, that donâ€™t trust math, that donâ€™t trust experts, that have their own answers and damned anybody that contradicts them, even if they have facts. Itâ€™s something we as a society have to deal with, and I donâ€™t know how. Itâ€™s exacerbated by the press, by social media, by a world where everyone can be a publisher. Right, thereâ€™s value in that, but thereâ€™s risk in that. Yeah. Do you have any thoughts then on the rampant spread of disinformation? Is that accelerating the way that I feel like it might be? Or is it more the same that itâ€™s always been, but we are just more aware of it now? Itâ€™s hard to measure. We know that more disinformation is spreading, but we donâ€™t know the effects of it. We have a lot of data, but no real good analysis on what does what. Take this very broadly, you go back a bunch of 100 years, being a publisher was hard, and being a recipient of publications was hard. You had a world of information that didnâ€™t really flow at all. You move into the world of the printing press and increase literacy and you had broadcast, like publishing was hard, but being the recipient was easier. That moved into radio and then television and, again, one to many was the way it all worked. Now weâ€™re going half backwards; itâ€™s very easy to publish, and itâ€™s very easy to receive information. Now weâ€™re in this world where everybody is speaking. We had history nobody speaking, and nobody listening; to 100 years ago, a few people speaking, everyone else listening; today everyone speaking, everyone listening. This is new and I think we donâ€™t fully understand the ramifications of the world weâ€™re in. This is something that people way smarter than me are studying and itâ€™s very outside my area, but I think I gave you my philosophy on it. Philosophy welcomed. This is a meta type of interview. Iâ€™d love to get your thoughts on how technology might be increasing polarization or some of the social issues that we are seeing today. Itâ€™s pretty clear that technology is increasing polarization just by allowing more discrimination. By that word, I mean the ability for people to segment themselves and to be segmented. That you can live a lot of your life now and not come into contact with an idea you disagree with, except in a divisive way. That is something that just wasnâ€™t true before the modern internet technologies so that is affecting things. Weâ€™re not sure how but it obviously is. What about the intentional or unintentional algorithmic encouragement of that? What about it? Do you have any thoughts on whether that should be something thatâ€™s addressed because it does seem like social media is intentionally or unintentionally encouraging that polarization based on social mediaâ€™s engagement models. This gets back to regulation. It seems odd that we would organize our societal political discourse around the short-term financial benefit of a handful of tech billionaires. That seems a really weird way to organize our political system and to organize politics. Yes, I would like to see government regulation here, because the full profit model of political speech isnâ€™t serving our country very well. Weâ€™ll be right back after the break. And now we return to our sponsorâ€™s question about forms of social engineering. KnowBe4 will tell you that where thereâ€™s human contact, there can be con games. Itâ€™s important to build the kind of security culture in which your employees are enabled to make smart security decisions. To do that, they need new school security awareness training. See how your security culture stacks up against Knowbe4â€™s free phishing test. Get it at knowbe4.com/phishingtest. Thatâ€™s knowbe4.com/phishingtest. Welcome back to our discussion with Bruce Schneier. Okay, so back in your wheelhouse with cryptography for a minute, we had multiple questions on LinkedIn from people asking about quantum computing and when quantum computing will begin to threaten existing commercial encryption. Then, of course, when our currently trusted encryption models, because of that, wonâ€™t be able to be trusted anymore. Quantum computing is a new way of doing computing and it does threaten some cryptography, but not all of it. I wrote an essay on this, and I urge everyone to go find it. The title is â€œCryptography after the aliens land.â€ Just type that into your favorite search engine and my name and itâ€™ll pop up. If you want to check out that essay, go ahead and check our show notes. Iâ€™ve included a link to the essay there. I go through the various promises and perils of quantum computing and what it means. The long and short of it is that weâ€™ll be okay, that it will break some encryption, but not all of it, that we are already working on post quantum algorithms and we donâ€™t actually know if itâ€™ll work at all. We know quantum computing is hard, but we donâ€™t know if it is put a person on the moon hard, or put a person on the sun hard. And I mean that really truly. If itâ€™s put a person on the moon hard, we should have breakthroughs over the coming years, in a decade or so weâ€™ll have working quantum computers actually solving problems. If itâ€™s put a person on the sun hard, itâ€™s going to be centuries and weâ€™re not going to solve it. Itâ€™s a very speculative technology right now. My money is on put a person on the moon hard, if weâ€™re going to have a betting pool, but cryptography is going to be okay. We have a lot of things we can do that are quantum resistant, even theoretically, and weâ€™ll be fine. Great, thanks. In this last section I want to be a little bit reflective and speculative. Over the past few decades as you look back, do you see any critical tipping point moments with respect to security that stand out to you? Nothing comes to mind. Iâ€™m not convinced had I thought about it a lot more, I wouldnâ€™t come up with any, but I canâ€™t think of any huge moments. Certainly the terrorist attacks 911 were a moment where a lot of things changed. Iâ€™m not sure it changed in cryptography or internet security in the same way. We might want to talk about some of the famous worms or malware. But, again, all of this feels like trends to me, and I donâ€™t think of any flash bulb moments where everything shifted. Maybe that itself is interesting, that things havenâ€™t shifted, that itâ€™s been the same stuff weâ€™ve been dealing with for decades. Yeah, so as you consider this and think about it, there could potentially be some small inflection points like this year where digital transformation had to speed up a bit and people embrace new technology, but itâ€™s more of a continuation rather than a complete break or shift. Then, I guess, letâ€™s pivot to the future. As we look to the future, what are you most excited about or worried about? I mean to me the future is the continuation of the present. I worry about the threat actors, I worry about nation states, I worry about criminals. I worry about all of the legal security threats that come from companies following the law and doing things that are bad for our security: the Googles, the Facebooks, surveillance capitalism. I worry about the Internet of Things, and the computerization and networkization of everything and how that will change the threat landscape. I think there is promise in governments getting involved. Not the US, weâ€™re way too dysfunctional, but the EU is the regulatory superpower on the planet, and they pass comprehensive privacy laws and theyâ€™re looking at Internet of Things, looking at vulnerability disclosure, looking at AI security. They are large enough that they will move the planet in ways that are good so thatâ€™s really where Iâ€™m looking. Alright, so these last few questions that came in from LinkedIn are really just about you. Youâ€™ve been at this for a long time now. How do you maintain such a prolific level of output and where does the passion come from to sustain that? For me, writing is understanding and also writing is a way I can channel my energies. Often I write about a topic because I want to figure it out and the act of writing is how I figure it out. Putting my thoughts in a coherent essay form or book form, helps me explore the issue. If something happens that pisses me off, the way for me to calm down is to write something. Writing is not hard, writing is easy and it actually helps me know whatâ€™s going on and I want to affect the debate. Itâ€™s how I can talk about the issues in ways that people understand and maybe I can move a political needle. Do you ever deal with something like imposter syndrome as youâ€™re a cyber security expert speaking into all these other areas that have to do with human dynamics and psychology, sociology and economics? Do you ever feel like youâ€™re speaking into areas where you really donâ€™t deserve a platform? Or do people accuse you of speaking into areas where you donâ€™t have a platform? I try to stay in my lane. The questions you asked me about misinformation is a good example. I could pontificate about it, but I know there are people other than me who are really researching this and itâ€™s enough outside my area that I donâ€™t want to opine. In a lot of ways, Iâ€™m a synthesist; Iâ€™m a meta person and I work in the ideas of security that apply in all domains. I actually have opinions on, I donâ€™t know, stadium security, or security at rock concerts, based on some of the systemic things I know about security, even though Iâ€™m not a domain expert, even though I know nothing about how stadiums work. I try to maintain an interest and a humility and know where I am speaking and what I can and canâ€™t speak about, and then be honest about it. I can tell you what I think and then couch that with saying â€œand there are people who are better than me, so if they contradict me, believe them and not me.â€ I think thatâ€™s all part of maintaining what you know. In our world generalists have it hard, because specialists almost always trump them, even though I think generalists have important things to say. Fantastic. Last question is probably the easiest and simultaneously most important question. Is there any question that you wish that I had asked that I didnâ€™t think to ask? Or is there a last word or thought that you want people to be thinking about and having discussions about? In the past couple of years, Iâ€™ve started thinking about the role of technology and public policy, and the importance of what weâ€™re coming to call public interest technologists; people who bridge the gap between technology and public policy. I am one of those people. I know many of those people, but there arenâ€™t nearly enough of them. We need a career path in public interest tech. We need the ability for lots of people to move into the space that requires expertise in both camps, because all of the important societal political problems of this century are fundamentally technological, like climate change, future of work, robotics, medical. We have a lot of bad policy coming from people who donâ€™t understand the tech, and a lot of bad tech from people who donâ€™t understand the policy. We need a cadre of people who understand both, and thatâ€™s what Iâ€™m trying to advocate for. Itâ€™s been on hold during the pandemic, because everythingâ€™s been on hold, but itâ€™s something I feel passionate about and what to get back to. Well, that was Bruceâ€™s last question and last response and that brings us to the end of todayâ€™s show. Bruceâ€™s perspective is always fascinating. As he mentioned, itâ€™s important to recognize that security is about so much more than just technology. Itâ€™s about people and taking a meta level approach and seeking a synthesis of ideas across multiple disciplines, is ultimately how we can evolve security and reduce risk. Bruce mentioned fields like psychology and sociology and economics and political science as being critical to improving our security posture. He also didnâ€™t shy away from mentioning the need for regulation as a critical step. His analogy to how regulation elevated the safety and reliability in other industries like the automotive industry, pharmaceutical industry, food industry and finance, is compelling. The reason that regulation works in these circumstances is because most organizations within a specific vertical, donâ€™t want to bear the brunt of being the first to do something and the only to do something. At that point, they have to pass on the cost and the friction to their customer, and theyâ€™ll be the only one that does it. In those circumstances the others that arenâ€™t doing it may get a benefit, because people donâ€™t know why theyâ€™re doing the new thing that theyâ€™re having to do, why they have these new requirements, and if they can go somewhere else, then they may. Regulation comes in and it deals with that by leveling the playing field, and it puts the economic piece in its place, and also ensures that all organizations within that industry are held to the same standards. Thatâ€™s been proven to work. Well, I hope that you enjoyed this interview with Bruce. Next time weâ€™ll be back in our regular format and Iâ€™ve got several great guests lined up for that show. Thanks so much for listening and thank you to my guest, Bruce Schneier. Iâ€™ve loaded up the show notes with links to all the relevant topics from todayâ€™s discussion, including Bruceâ€™s books and much more. Be sure to check those out. If youâ€™ve been enjoying 8th Layer Insights, please go ahead and take a couple of seconds to head over to Apple Podcast and rate and consider leaving a review. That does so much to help. You can also help by posting about it on social media, recommending it within your network, and, heck, maybe even referring an episode to a friend or a family member. If you havenâ€™t, go ahead and subscribe or follow, wherever you like to get your podcasts. Lastly, if you want to connect with me, feel free to reach out on LinkedIn or Twitter. I also participate in a group on Clubhouse. We meet once a week on Friday. Itâ€™s called the Human Layer Club, and you can just search for it on Clubhouse and find it pretty easily. Well, until next time, thank you so much. Iâ€™m Perry Carpenter signing off."},
{"title": "The Coming AI Hackers", "article": "AI hackers are coming, and itâ€™s not just our computer networks at risk â€“ our laws and regulations are also vulnerable. Bruce Schneier, internationally renowned security technologist and fellow at Harvardâ€™s Berkman Klein Center for Internet and Society, joins Azeem Azhar to explore how humans have always exploited loopholes in rule-based systems, and how that will change as AIs become more powerful. They also discuss:"},
{"title": "Everything We Know About Security Is Wrong", "article": "So says counterterrorism contrarian Bruce Schneier. And the Transportation Security Administration is listening. In late July, Transportation Security Administration chief Kip Hawley announced a change in his agencyâ€™s air travel screening policy: Effective August 4, cigarette lighters would no longer be banned from airplanes. Explaining the measure in an interview with the New York Times, Hawley acknowledged that confiscating lighters at security checkpointsâ€”the TSAâ€™s policy for the last two years in the wake of a failed shoe-bombing attemptâ€”had been a waste of resources. Terrorists, he noted, might just as well ignite bombs on airplanes using small batteries (or, as he didnâ€™t note, matches). â€œTaking lighters away is security theater,â€ Hawley told the Times. â€œIt trivializes the security process.â€ Among those struck by Hawleyâ€™s about-face was Bruce Schneier, a Minneapolis man alternately called a â€œsecurity guruâ€ (The Economist), â€œthe smartest guy in the room on securityâ€ (the ACLU), and â€œunquestionably the worldâ€™s foremost security technologistâ€ (Connections). Schneier, who wears the graying beard and thinning ponytail of a computer geek chieftain, didnâ€™t earn such accolades by mincing words. â€œThere have been exactly two things since 9/11 that have made air travel safer,â€ Schneier said recently over spring rolls at a favorite Vietnamese restaurant on Nicollet Avenue. â€œReinforcing the cockpit door and telling people to fight back in the event of an attack.â€ After a brief pause, half-devoured roll in hand, he reconsidered. â€œWell, maybe three,â€ he said. â€œIâ€™m on the fence about sky marshals.â€ One thing Schneier isnâ€™t on the fence about is the billions of dollars that the TSA has spent making air travelers pour out their water, take off their shoes, and until recently, throw out their cigarette lighters. All of this, Schneier argues, might make people feel safer, but it does little to actually improve security. Waiting for his bowl of pho to arrive, a triumphal smile crept across Schneierâ€™s face when he brought up Hawleyâ€™s recent announcement. It wasnâ€™t just that the TSA head had shifted policy. It was also that phrase: â€œsecurity theater.â€ Schneier coined it back in 2003, to encapsulate what by his lights was a parade of new measures that conveyed safety but accomplished little. Such elegantly blunt criticisms have helped make Schneier a leading counterterrorism contrarian. A prolific writerâ€”he has published several books, maintains regular columns for Wired.com and Forbes.com, and has a blog and electronic newsletter with a combined monthly readership of about 200,000â€”Schneier is also a seasoned public speaker, having addressed, among other august bodies, the House of Lords, the World Economic Forum, and the U.S. Congress. And thatâ€™s just in his spare time. Schneierâ€™s paying job is chief technology officer for BT Counterpane, a network security company he founded in 1999 that last year was bought out for tens of millions of dollars. In addition, Schneier is quoted almost daily in one media outlet or another, on everything from data mining (usually a bad idea), to paperless voting (always a bad idea), to buying stuff with a credit card online (in the grand scheme of things, not such a bad idea). But heâ€™s most passionate about the governmentâ€™s response to terrorism since September 11, which he says has been both out of proportion to the threat and overly governed by our collective fears. His pho placed in front of him, Schneier picked up his spoon and jabbed the air with it. â€œWeâ€™re one terrorist attack away from a police state,â€ he said. On a recent morning at the Minneapolis-St. Paul International Airport, Schneier set out to foil airport security. Dressed in a black blazer and jeans, Schneier approached a stone-faced Northwest Airlines ticket agent and informed her that heâ€™d lost his ID. â€œDo you have a credit card in your name?â€ she asked. â€œNo,â€ Schneier answered. In accordance with airline policy, the agent printed Schneierâ€™s boarding pass, scrawling â€œNO IDâ€ on it. Schneier thanked her and headed to the security line, where he would receive extra scrutiny. In the end, though, Schneier was allowed to board his plane with little difficulty, even though the airline had no idea who he was. In so doing, Schneier demonstrated why the so-called â€œNo Flyâ€ listâ€”the backbone of the airport security systemâ€”is, as he puts it, â€œa complete waste of time.â€ The No Fly list is a confidential database of people deemed by the federal government to be too dangerous to fly under any circumstances (albeit, as Schneier wryly points out, â€œtoo innocent to arrestâ€). A secondary classification, the lesser-known â€œSelecteeâ€ list, requires passengers to submit to a luggage search and wanding. But because, as Schneier demonstrated, anyone can check in without an ID and be treated as a selectee (not to mention board as a normal passenger by bribing a DMV worker for a fake license, as some of the 9/11 hijackers did), the No Fly list is easily circumvented. The government knows this, of course, and has pledged to overhaul the system by taking it out of the hands of the airlines. However, as Schneier points out, people will always lose their IDs, and there will always have to be a system in place to allow them to fly without one. Skeptical? Just imagine having your wallet stolen in Tulsa and being stuck there for weeks while waiting for a replacement driverâ€™s license. Imagine that happening to hundreds of people a day, and the subsequent angry calls to congressmen and congresswomen demanding a change in the law. Which, says Schneier, is why any form of air travel security based on identifying passengers will never work. It will always be just a form of â€œsecurity theater.â€ In a recent series of email exchanges with TSA chief Hawley that Schneier posted on his blog, he scolded Hawley for engaging in â€œcover your assâ€ security measures: A guy tries to blow up an airplane with his shoes, so now everyone has to take their shoes off; some people think of smuggling liquid explosives on a plane, so now everyone has to put liquids in three-ounce containers (unless the bottle is labeled â€œsaline solution,â€ which counts as medication, and thus can be brought aboard in a vaguely defined â€œreasonable quantityâ€). As Cory Doctorow, the co-editor of the popular tech blog Boing Boing, puts it: â€œBruce has a particular gift for puncturing ridiculous statements about security.â€ But though Schneier has been winning converts, his views are hardly gospel in government circles. Clark Kent Ervin, the former inspector general of the Department of Homeland Security, accuses Schneier of downplaying the terrorist threat. â€œItâ€™s true that the chance of being killed by a terror attack is much smaller than being stricken by cancer,â€ says Ervin, who heads the homeland security program at the Aspen Institute, a Washington, D.C.-based think tank. â€œBut itâ€™s comparing apples and brass buttons.â€ Terror attacks, he says, â€œhave a huge psychological as well as an economic impact. Itâ€™s silly talk to say that the chances of being killed in a terrorist attack are so small, and to infer from that that we neednâ€™t worry about it.â€ Ultimately, Ervin says, Schneierâ€™s legacy may be to lull people into a false sense of security. â€œHis kind of thinking might be excusable in a pre-9/11 world,â€ Ervin says. â€œBut in the post-9/11 world, itâ€™s irresponsible and dangerous.â€ Bruce Schneier has had a fascination with security since childhood. As a boy in Brooklyn in the 1960s, he would crack secret codes written for him by his father. When he got older, he found himself studying the placement of security cameras to figure out the best strategy for shoplifting (a purely intellectual exerciseâ€”he says he never followed through on the idea). After graduating from SUNY Rochester with a degree in physics, Schneier spent the latter half of the 1980s at the Defense Department. He wonâ€™t elaborate on his time there, other than to say it involved â€œimplementing security solutions at military installations.â€ A few years later, in 1993, Schneier penned his first best-selling book. The mathematics-heavy Applied Cryptography quickly became the seminal how-to guide for writing ciphersâ€”complex algorithms that scramble data, protecting it when sent from one computer to another. In the years that followed, computer programmersâ€”many looking to Schneierâ€™s book for instructionâ€”designed ever-more-impenetrable ciphers, with an eye toward keeping the data of multinational companies secure. This posed a problem for the U.S. government, which considered such so-called â€œstrong cryptoâ€ a risk to national security. The Clinton administration, following in the footsteps of its predecessors, sought to put a stop to it, asserting that selling the encryption programs to foreign companies amounted to a breach of the International Traffic in Arms Regulations. A loose affiliation of mathematicians, civil libertarians, and antigovernment hard-liners fought back, giving rise to what came to be known as the â€œCrypto Wars.â€ In the ensuing public debate, Schneier found himself firmly in the fray, writing opinion papers and testifying before Senate and House committees. â€œHe could respond to the governmentâ€™s experts tit for tat,â€ says Jim Dempsey, policy director for the Center for Technology and Democracy, which advocated for strong crypto. â€œAnd nobody could say that he didnâ€™t know what he was talking about, because he literally wrote the book on cryptography.â€ In 1999, after an appellate court ruled that restricting encryption was illegal, the Clinton administration surrendered. Encryption technology was allowed to flourish. But as Schneierâ€™s co-combatants celebrated a hard-won victory, he found himself unable to join them. â€œWe won the war,â€ he says, â€œbut it was the wrong war.â€ Schneier had realized that the most important component of any security system is not its strengths but its weaknesses. Strong crypto is nearly impossible to penetrate. But the computer, the network, and even the user are far more fallible. Take, for example, the case of Dennis Alba and Mark Forrester. In 2001, the DEA investigated the middle-aged pairâ€”who had become friends while in prisonâ€”on suspicion of setting up and running a large-scale, sophisticated Ecstasy ring in Escondido, California. The partners used code words to communicate and shielded their computer files with stong crypto. The DEAâ€™s extensive investigation included obtaining a search warrant to break into their office and install a â€œkeystroke loggerâ€ on a computer. That piece of software, which records whatâ€™s typed on the keyboard, enabled the government to get the key that unlocked their encryption. In 2005, both men were sentenced to 30 years in federal prison. (Forresterâ€™s conviction was later overturned on a technicality.) The lesson was clear: All the crypto in the world is powerless to protect you if the front door is so easily pried open. Taking this to heart, Schneier, with a few million dollars of venture capital in tow, set up Counterpane Internet Security. The mission of the Silicon Valley-based firm was to monitor computer networks in much the same way ADT Home Security protects houses: by having human beings work in concert with technology. J.P. Vossen, a senior engineer at Counterpane, was inspired to join the company after hearing Schneier speak at a computer security conference. â€œHe can explain some counterintuitive stuff very clearly,â€ Vossen says. â€œI like Bruceâ€™s approach. Thatâ€™s the largest reason Iâ€™m working here.â€ In the years that followed, Counterpane grew to a 115-employee firm worth tens of millions of dollars. This is where Schneier may have lived happily ever after, as a successful businessman and computer security geek extraordinaire. But then came the events of September 11, and what Schneier calls the â€œsilly security season.â€ Schneierâ€™s house, which has no more security than the locks on the doors, is a handsome stone structure on a leafy street across from Minnehaha Creek. Along with his wife, Karen Cooper, Schneier has lived here for the past 11 years. Most of the downstairs is one large open space, with no walls separating the living room, dining area, and sunroom, the last of which serves as Schneierâ€™s office. Sitting near a window facing the creek on a recent afternoon, Schneierâ€™s blue eyes opened wide and his voice rose as he explained his frustration with how the media covers would-be terrorists. â€œWeâ€™re just getting scared over idiots, like the London bombers,â€ he argued, referring to the clique of foreign-born medical personnel in Britain who tried to set off three car bombs. â€œNothing would have exploded in those cars. You would have had a bunch of hot nails.â€ Then there was the group of men who planned to blow up Kennedy airport in New York. â€œYou ever been to Kennedy airport?â€ Schneier asked. â€œItâ€™s acres wide. You canâ€™t blow up Kennedy airport. And they had a stupid plan that wouldnâ€™t have worked. But we get all panicky. We end up saying, â€˜Oh God! These people are going to blow up Kennedy airport!'â€ Such fears are examples of what Schneier calls â€œmovie-plot threatsâ€â€”grandiose scenarios that capture the imagination but are highly unlikely to succeed. â€œTheyâ€™re good for scaring people, but itâ€™s just silly to build national security policy around them,â€ Schneier says. On his blog last year, Schneier took issue with the idea that terrorists might target school buses. To protect against this dreamed-up threat, the Department of Homeland Security started training school bus drivers to be on the lookout for hijackers. In addition to being a waste of resources, Schneier pointed out, the measure may have actually put kids at greater risk, because bus drivers distracted by phantom terrorists could be more vulnerable to the much more realistic danger of oncoming traffic. For an example of how to spend money appropriately, Schneier says one need look no further than the I-35W bridge collapse. By investing federal Homeland Security money in communications equipment and a disaster preparedness planâ€”a response mechanism useful in any type of attack or catastropheâ€”local and state authorities were able to coordinate their efforts and, in all likelihood, save lives. â€œIf theyâ€™d have put all that money into protecting the Foshay Tower, it would have been a complete waste,â€ Schneier says. In January, Schneier visited his newly born godson, Nicholas Quillen Perry, at Abbott Northwestern hospital in Minneapolis. As he looked at the snoozing infants in the maternity ward, he noticed that each one had an electronic bracelet around its ankle, which would trigger an alarm if the newborn were taken out of the ward. Sizing up this anti-infant-abduction measure, Schneier was initially struck by the stupidity of it. Hospital baby snatchings, after all, are extremely rare. Since 1983, there have been fewer than 250 reported cases in the United States, out of more than 80 million babies born in that time. In other words, the chance of a baby being abducted from a hospital is less than three in a million. But as Schneier watched the babies being removed from their cribs for one test or another, he began to wonder if this blatant display of security theater was such a bad thing after all. Parents of newborns are in a highly anxious state, prone to feeling less secure than they really are. Electronic bracelets, while not providing much actual security, can do wonders for the emotional well-being of the frazzled parents. Which has led to Schneierâ€™s most recent revelation: Security theater can actually be a good thing when it brings our feelings of safety into line with the actual threat. Take the tamper-evident seals on over-the-counter medicine. In 1982, seven people in the Chicago area died after someone slipped cyanide into packages of Extra Strength Tylenol. Responding to widespread fears, manufacturers introduced tamper-evident seals. Although these were security theaterâ€”they donâ€™t protect against syringes, for instanceâ€”the sense of safety they brought made the publicâ€™s comfort level come back in line with the actual threat, which was, statistically speaking, quite minimal. Sitting at a coffee shop around the corner from his house, Schneier considered the implications of his turnabout. Here he was, having spent years deriding security theater in all its manifestations, now saying that, in some cases, itâ€™s actually a good thing. Did that mean he owed TSA chief Kip Hawley an apology? Schneier let out a chortle. His answer was true to form: so self-evident that it left the questioner feeling somewhat silly for even asking. The baby bracelets and tamper-evident medications, Schneier explained, are there to calm people. Banning lighters but not matches does nothing to relax fears. â€œItâ€™s bad theater,â€ Schneier said. â€œEveryone sees what the TSAâ€™s doing is a joke.â€"},
{"title": "Cryptographer Slams NT Security", "article": "A top cryptographer said Microsoftâ€™s version of a key protocol in Windows NT is so flawed that users should avoid using virtual private network software based on Microsoftâ€™s Point to Point Tunneling Protocol. Bruce Schneier, a noted cryptographer, said the PPTP in Windows NT 4.0 is so broken it canâ€™t be fixed with patchesâ€”a position that Microsoft disputes. â€œI believe itâ€™s fundamentally broken,â€ said Schneier, who authored a widely used cryptography textbook. â€œWhat weâ€™re seeing is the basic problem of proprietary security standards. These are really dumb mistakes, kindergarten crypto.â€ He advised users interested in setting up virtual private networks to buy software that supports the emerging IPSec standard, not PPTP.  Both of those protocols are designed for securely linking remote users to corporate networks over the Internet. But Microsoft said several of Schneierâ€™s criticisms of Windows NT security have already been addressed in product updates or bug patches. â€œSeveral items in the press release are no longer issues,â€ said Microsoftâ€™s Ed Muth, group product manager for security marketing. However, industry observers say only a handful of companies are implementing VPNs today, and many VPN firms are beginning to support IPSec, though some also back PPTP for interoperability. Microsoft said NT version 5.0, due next year, will support both protocols. â€œLarge enterprise customers know PPTP is a relatively weak security protocol,â€ said Evan Kaplan, president of VPN firm . \"Our belief is that Microsoft will come around and fix it.\" Kaplan and others noted that some of Schneierâ€™s critiques have been circulating for months on Internet newsgroups. â€œThe security problems are very real, but almost all customers know about this security problem,â€ Kaplan added. Schneier contends weaknesses in Microsoftâ€™s original NT 4.0 operating system could lead to stolen passwords, disclosure of private data, and server crashes when running some VPN software. Schneier stressed that the issues are in Microsoftâ€™s version of PPTP, not in the protocol itself. But Microsoftâ€™s Muth said the firm has fixed most of the problems alreadyâ€”one related to requiring strong passwords in December 1996, one in February to prevent attackers from crashing a PPTP server, and another last week on scrambling passwords sent over the Internet. Password-based security has inherent weaknesses, Muth said, which is why Microsoft is incorporating digital certificates using public key encryption into its operating systems. â€œPPTP is one small part of Windows NT, and Bruce made no general indictment of Windows NT,â€ Muth said. â€œWe think the glass is half full, and we intend to fill it the rest of the way.â€ But Schneier, who has done consulting work for Microsoft and other major firms around the world, discounted Microsoftâ€™s reaction. â€œTheir normal tactic is either to ignore a security problem, claim itâ€™s not a problem, or claim itâ€™s being fixed,â€ Schneier said. â€œThen it blows over and they ignore it.â€ VPN vendors such as and , which is licensing its IPSec-compliant software to giant , have opted to make IPSec their strategic direction. â€œWeâ€™re in the IPSec business and figure that if you want to do it right, go to the trouble of using IPSec,â€ said Jim Hart, Red Creekâ€™s director of engineering. â€œEven if itâ€™s a perfect implementation of PPTP, if youâ€™re a CIO or network administrator, youâ€™re always going be criticized for doing PPTP.â€ John McCown, technical director for network security of the , which is testing IPSec-based VPNs, noted that IPSec has been widely discussed at the standards body . That kind of scrutiny tends to weed out errors, he said. â€œHaving Bruce Schneier criticize something moves it out of the realm of, â€˜This in general has problemsâ€™ to â€˜Yes, the people who really know about this stuff have looked at it,â€™ and thatâ€™s serious,â€ McCown said."},
{"title": "Thought Leadership: Bruce Schneier on â€˜A Hackerâ€™s Mindâ€™", "article": "Welcome to Cyber Security America, the podcast where we delve deep into the world of cybersecurity and provide insights on past trends, current challenges, and areas for improvement. Our goal is to help you stay informed and prepared for the next cyber threat. In this episode, we have a very special guest, Bruce Schneier, an internationally renowned security technologist, known as a â€œsecurity guruâ€ by The Economist. With over a dozen books and hundreds of articles and academic papers under his belt, Bruce is a true legend in the information security field. Heâ€™s also the author of the latest book, â€œA Hackerâ€™s Mind,â€ where he takes hacking out of the world of computing and uses it to analyze the systems that underpin our society. During our conversation, Bruce provides us with valuable insights on the current state of cybersecurity. He discusses the impact of coordinated takedowns by federal forces on ransomware actors, and how less payment transactions on the blockchain related to ransomware actors is a promising sign. He also highlights an emerging threat, Black Lotus, and shares his thoughts on how artificial intelligence thinking like a hacker could be catastrophic. This episode is packed with expert tips and lessons learned. So tune in now to Cyber Security America and join the conversation."},
{"title": "Bruce Schneier on Regulating at the Pace of Tech", "article": "Harvard cyber security expert Bruce Schneier spoke with Transform for the publicationâ€™s inaugural issue on trust in tech. He said that, given how central technology is to our daily lives, we should be able to trust that tech systems are secure â€“ in the same way we trust that food from the grocery store is safe to eat and planes are safe to fly in. If those things are safe, itâ€™s only because governments regulate them. â€œWe walk into a restaurant and donâ€™t have to check the kitchen ourselves,â€ Schneier says. â€œGovernments perform a valuable function in our stead: they are our experts.â€ But sometimes technology evolves too quickly for regulations to keep up. â€œThe tech industry moves quickly. And we really donâ€™t know how to regulate at the speed of tech. Tech moves much faster than government, and we donâ€™t really have a good theory of agile regulation. The best we have, at least in the United States, are regulatory agencies, which are able to move faster than Congress can, but still slower than technology. So I think right now we in society, weâ€™re working out how to regulate technology at technologyâ€™s pace.â€"},
{"title": "Sociotechnical Exploitation with Bruce Schneier", "article": "The Sociotechnical Theory is an organizational theory that emphasizes the importance of both social and technical factors in designing and managing systems. Sociotechnical systems are deeply embedded within society and prone to â€œhacking,â€ a term meaning to subvert a systematic rules in unintended way.Â  In his most recent book, , Bruce Schneier takes hacking beyond computer systems and uses it to analyze the systems that underpin our society. He stops by and we define the true definition of hacking, who has the edge in the endless arms race, revealing who the worldâ€™s best hackers are, how AI will impact the future of hacking, and the truth about AI democratization. TIMESTAMPS 0:02:37 â€“ Exploring the Hackerâ€™s Mindset and How to Bend Societyâ€™s Rules 0:04:53 â€“ The Importance of System Hacking in Todayâ€™s World 0:06:42 â€“ The Inevitability of System Hacks and the Impact of AI 0:14:41 â€“ Digital Simulation Technology on Policy and Legal Code 0:16:21 â€“ Impact of Hacking on Existing Inequalities 0:18:21 â€“ Hacking Resources and Loopholes"}
]