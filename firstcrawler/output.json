[
{"title": null, "article": ""},
{"title": "Evaluating the Effectiveness of AI Legislation in Cybersecurity with Bruce Schneier", "article": "The rapid evolution of artificial intelligence in cybersecurity presents both significant opportunities and daunting challenges. On this episode, I’m joined by Bruce Schneier, who is renowned globally for his expertise in cybersecurity and is dubbed a “security guru” by the Economist. Bruce, a best-selling author and lecturer at Harvard Kennedy School, discusses the fast-paced world of AI and cybersecurity, exploring how these technologies intersect with national security and what that means for future regulations. (00:00) I discuss with Bruce the challenges of regulating AI in the US. (02:28) Bruce explains the role and future potential of AI in cybersecurity. (05:05) The benefits of AI in defense, enhancing capabilities at computer speeds. (07:22) The need for robust regulations akin to those in the EU. (12:56) Bruce draws analogies between AI regulation and pharmaceutical controls. (19:56) The critical role of knowledgeable staff in supporting legislators. (22:24) The challenges of effectively regulating AI. (26:15) The potential of AI to transform enforcement across various sectors. (30:58) Reflections on the future of AI governance and ethical considerations."},
{"title": "History of RSA Conference. Bruce Schneier. The First ‘Exhibitor’ in 1994.", "article": "Bruce Schneier was at the first ever RSA Conference in 1991, and he was the first ‘exhibitor’ in 1994 when he asked Jim Bidzos, Creator of the RSA Conference, if he could sell copies of his book “Applied Cryptography.” Bidzos set Schneier up in the hotel lobby where the conference was being held—and the rest is history. Listen to some great RSA Conference memories on this episode of the History of RSA Conference."},
{"title": "The Hacking of Organizational Systems", "article": "“There are only two types of organizations. Those that have been hacked and those that don’t know it yet.”—John Chambers Comcast said nearly 36 million U.S. Xfinity accounts were compromised after hackers accessed its systems through a vulnerability in third-party cloud-computing software. The breach occurred between October 16 and October 19, 2023. On Sunday, February 18, 2024, at the Munich Security Conference, FBI Director Christopher Wray said China’s cyberattacks on U.S. infrastructure are “unprecedented.” AT&T announced that the cause of its 12-hour nationwide outage on February 22, 2024, was the “execution of an incorrect process,” not a cyberattack. In simpler terms, the company admitted to human error. What’s the difference between cyberattacks and hacking? Cybercriminals hack and infiltrate computer systems with malicious intent, while hackers supposedly seek new and innovative ways to use a system, good or bad. (Micro Trend) According to Security Magazine, there are over 2,200 attacks daily, which breaks down to nearly 1 cyberattack every 39 seconds. On average, 1.4 billion social media accounts are hacked every month. All systems are vulnerable In (Norton), a book worth your time, Bruce Schneier defines hacking as “an activity allowed by the system that subverts the goal or interest of the system.” Anything from medical records to the U.S. tax code can be hacked. “Hacking is how the rich and powerful subvert the rules to increase their wealth and power. It’s not that the wealthy and powerful are better at their hacks; they’re less likely to be punished for doing so,” adds Schneier, a fellow at the Berkman Center for Internet and Society at Harvard. As F. Scott Fitzgerald observed, “The rich are different from you and me.” Schneier says that hacking is not the same as cheating. “Hacking targets a system and turns it against itself without breaking it. It’s gaming the system and occupies a middle ground between cheating and innovation. A hack follows the letter of the system’s rules but violates their spirit and intent,” he concludes. Systems evolve through hacking, especially when less critical and on a smaller scale. They might actually benefit from hacking as a way to improve their functionality and security. A breach shows where to patch, as it’s impossible to think of every susceptibility when designing a system. Here are three stories illustrating different types of hacking Who has access? The focus in moves from IT to organizational systems. For hacking to occur, a system of rules, such as corporate policies, must be hacked. And policies are plentiful. The author explains that it’s “one short step from hacking computers to hacking economics, politics, and social systems,” as they are just as vulnerable to hacking as technology. Protecting the integrity of any system is rooted in the character and values of those in charge. Hiring decisions, which are extremely important but imperfect, are often a door ajar. To quote one observer, “People are honest most of the time but become dishonest in some situations when they perceive there is an advantage to be gained from it.” The book’s critical point is that not all systems are equally hackable. Complex systems with many rules are the most vulnerable because there are more possibilities for unanticipated and unintended consequences. Schneier makes clear: “Complexity is the worst enemy of security.” Questions to ask— If you are responsible for an enterprise, know it can and will be hacked (rules bent or ignored, boundaries stretched, goals subverted). Therefore, keep policies and procedures simple to reduce security risks. Cognitive hacking is powerful Schneier wants everyone to know that any time something can alter information, choice, and agency, it represents a danger to the human mind. “If you can hack a mind, you can hack any system governed by human action,” he writes. Can AI machines think? AI, or artificial intelligence, is defined in as (a) computers that can generally sense, think, or act and (b) as an umbrella term encompassing a broad array of decision-making technologies that stimulate human thinking. An example of that last point is how specialized AI is designed for a specific task, like controlling a self-driving car. Tech writer Andy Kessler says, “Computers win in realms with defined rules, but humans have free will and make choices.” The AI insight: A corporate plan Steve Durbin, Chief Executive of the Information Security Forum, recommends that AI be viewed from the lens of corporate strategy and risk. “Before you can chart an AI strategy, develop a thorough understanding of its potential, its current usage across the organization, and the security challenges and threats that lie ahead,” he emphasizes. At the corporate level, there is a need to integrate ethical considerations into policy and procedures. “Fairness, transparency, accountability, and privacy are the most ethical considerations surrounding AI,” Durbin concludes. In the AI gold rush (Nvidia and OpenAI), programming and security are the next frontiers. Bringing untold financial gain, higher-than-average risk, and opportunities for hacking systems previously unconceived."},
{"title": "Harvard Technologist Encourages Use of AI to Protect Democracy", "article": "Exploring ways in which generative artificial intelligence will affect democracy, prominent Harvard lecturer and public-interest technologist Bruce Schneier said it’s important for people to look both ways and to be unafraid of using the technology when it can help. Schneier said he foresees an “arms race” where those who fail to engage with the technology will quickly lose ground to those who do. He offered examples of how AI can be used throughout the democratic process, including to augment polling, fundraising and campaign strategies in electoral politics, and to more routinely submit comments to regulatory agencies, craft legislation, and improve law enforcement. “It lowers the barrier to lobbying, I think that’s a good thing,” Schneier said during facilitated by the Harvard Kennedy School. Schneier is at work on a new book about generative AI and Democracy. His remarks come as members of Congress discuss , and as some democracy and consumer the technology’s search and summarization abilities will further decimate journalism and the broader marketplace for information if it is controlled only by a limited number of corporate giants. Schneier balanced his recommendations encouraging use of the technology by imploring policymakers to ensure equity is centered and reiterating his . But as the technology becomes more accessible, he sees greater use of AI potentially shining light on the corners of government that have gone dark due to the drop off in human journalists, for example. “A lot of what happens in government happens in secret now because we’ve lost journalism,” he said. “Well this can be used to get it back.” More broadly, Schneier sees “enormous potential” for AI to be used as a moderator and consensus builder. It’s “a perfectly reasonable human job where there aren’t enough humans,” he said noting experiments done on such abilities at MIT. “You can imagine a conversation  where the AI serves as the moderator. It ensures all voices are heard, can block hateful or off topic comments, highlight areas of agreement and disagreement and help groups reach consensus.” In another example, Schneier noted ways AI can be used to enhance law enforcement, including the possibility of the IRS using it to better catch tax cheats. “We have a lot of rules and regulations, but not a lot of enforcement, AI can scale that,” he said. But, as with most of his examples, there were cautionary notes such as the danger of false positives and proprietary software. Asked about the risk of bias associated with AI, Schneier highlighted the importance of transparency and auditing. “We talk about AI being racist, but human policemen are racist, so maybe we can do better. Or at least we can see how well we’re doing in a way we can’t with a human,” he said. Describing the human brain as the ultimate “black box,” Schneier said, “There’s a lot of bias embedded in us, structural bias is a thing, and it’s hard to test. I don’t know why someone’s racist, I just see the outputs. At worst, AIs are no worse than humans.” “I’m optimistic, because at least [with] AIs I can open them up and look at the details in a way that I can’t with a human brain,” Schneier said noting scientists are getting better at examining those details. For now, “the AI’s are trained on human output and human output is kind of gross,” but “by cleaning the human output so it’s less gross, and looking at the AI system, we can find the grossness and manually remove it,” Schneier said. Noting the way a human brain—the neural networks of which AIs mimic—can be trained, it turns out, to “forget addiction,” he said maybe, eventually, we can get AI to “forget racism.”"},
{"title": "Bruce Schneier Predicts a Future of AI-Powered Mass Spying", "article": "If the internet helped create the era of mass surveillance, then artificial intelligence will bring about an era of mass spying. That’s the latest prediction from noted cryptographer and computer security professional Bruce Schneier, who, in December, where artificial intelligence—AI—will be able to comb through reams of surveillance data to answer the types of questions that, previously, only humans could. “Spying is limited by the need for human labor,” Schneier wrote. “AI is about to change that.” As theorized by Schneier, if fed enough conversations, AI tools could spot who first started a rumor online, identify who is planning to attend a political protest (or unionize a workforce), and even who is plotting a crime. But “there’s so much more,” Schneier said. “To uncover an organizational structure, look for someone who gives similar instructions to a group of people, then all the people they have relayed those instructions to. To find people’s confidants, look at whom they tell secrets to. You can track friendships and alliances as they form and break, in minute detail. In short, you can know everything about what everybody is talking about.” Today, on the Lock and Code podcast with host David Ruiz, we speak with Bruce Schneier about artificial intelligence, Soviet era government surveillance, personal spyware, and why companies will likely leap at the opportunity to use AI on their customers."},
{"title": "Why Is Regulation Slower Than Technology?", "article": ""},
{"title": "23andMe DNA Data Hack", "article": "Watch cybersecurity expert Bruce Schneier and genetics expert John Greally, M.D. discuss the 23andMe DNA data hack consequences, how and why it happened, who did it, and how to protect yourself."},
{"title": "The Best Information Security Books of 2023", "article": "It’s been a year since I wrote , two years since , which was preceded by and With that, as the year is coming to a close, here’s my list of the Best Information Security Books of 2023. When it comes to information security rock stars, Bruce Schneier is on everyone’s list. He’s written numerous books over the decades, the most important of which may be his classic . The underlying theme Schneier makes in his excellent book is that hacking is, in fact, a universal trait. While those in the information security field think of hacking in terms of zero days and Windows vulnerabilities, finding gaps in things is a normal human response. Schneier writes that all systems will have ambiguities, inconsistencies, and oversights, and they will always be exploitable. Systems of rules, in particular, have to tread the fine line between being complete and being comprehensive within the many limits of human language and understanding. Combine that with the natural human need to push against constraints and test limits, and with the inevitability of vulnerabilities, and you get everything being hacked all the time. This is a delightful and readable book where he discusses how hacking is pervasive across all systems. From hacking financial and legal systems, to political systems, cognitive systems, and more. Not only that, creating an unbreakable system, based on Gödel’s incompleteness theorems, is fundamentally unattainable. A fascinating and engaging read, is my choice for the best information security book of 2023."},
{"title": "Due to AI, “We Are About to Enter the Era of Mass Spying,” Says Bruce Schneier", "article": "Schneier: AI will enable a shift from observing actions to interpreting intentions, en masse. In an editorial for Slate Monday, renowned security researcher warned that AI models may enable a new era of mass spying, allowing companies and governments to automate the process of analyzing and summarizing large volumes of conversation data, fundamentally lowering barriers to spying activities that currently require human labor. In the piece, Schneier notes that the existing landscape of electronic surveillance has already transformed the modern era, becoming the , where our digital footprints are constantly tracked and analyzed for commercial reasons. Spying, by contrast, can take that kind of economically inspired monitoring to a completely new level: “Spying and surveillance are different but related things,” Schneier writes. “If I hired a private detective to spy on you, that detective could hide a bug in your home or car, tap your phone, and listen to what you said. At the end, I would get a report of all the conversations you had and the contents of those conversations. If I hired that same private detective to put you under surveillance, I would get a different report: where you went, whom you talked to, what you purchased, what you did.” Schneier says that current spying methods, like phone tapping or physical surveillance, are labor-intensive, but the advent of AI significantly reduces this constraint. Generative AI systems are increasingly adept at summarizing lengthy conversations and sifting through massive datasets to organize and extract relevant information. This capability, he argues, will not only make spying more accessible but also more comprehensive. “This spying is not limited to conversations on our phones or computers,” Schneier writes. “Just as cameras everywhere fueled mass surveillance, microphones everywhere will fuel mass spying. Siri and Alexa and ‘Hey, Google’ are already always listening; the conversations just aren’t being saved yet.” From action to intent We’ve recently seen a movement from companies like and to feed what users create through AI models for the purposes of assistance and analysis. Microsoft is also building into Windows, which require remote cloud processing to work. That means private user data goes to a remote server where it is analyzed outside of user control. Even if run locally, sufficiently advanced AI models will likely the contents of your device, including image content. Microsoft recently , “Soon there will be a Copilot for everyone and for everything you do.” Despite assurances of privacy from these companies, it’s not hard to imagine a future where AI agents probing our sensitive files in the name of assistance start phoning home to help customize the advertising experience. Eventually, government and law enforcement pressure in some regions could compromise user privacy on a massive scale. Journalists and human rights workers could become initial targets of this new form of automated surveillance. “Governments around the world already use mass surveillance; they will engage in mass spying as well,” writes Schneier. Along the way, AI tools can be replicated on a large scale and are continuously improving, so deficiencies in the technology now may soon be overcome. What’s especially pernicious about AI-powered spying is that deep-learning systems introduce the ability to analyze the intent and context of interactions through techniques like . It signifies a shift from observing actions with traditional digital surveillance to interpreting thoughts and discussions, potentially impacting everything from personal privacy to corporate and governmental strategies in information gathering and social control. In his editorial, Schneier raises concerns about the chilling effect that mass spying could have on society, cautioning that the knowledge of being under constant surveillance may lead individuals to alter their behavior, engage in self-censorship, and conform to perceived norms, ultimately stifling free expression and personal privacy. So what can people do about it? Anyone seeking protection from this type of mass spying will likely need to look toward government regulation to keep it in check since commercial pressures technological safety and ethics. President Biden’s mentions AI-powered surveillance as a concern. The European Union’s also may this issue to some extent, although apparently not directly, to our understanding. Neither is currently in legal effect. Schneier isn’t optimistic on that front, however, closing with the line, “We could prohibit mass spying. We could pass strong data-privacy rules. But we haven’t done anything to limit mass surveillance. Why would spying be any different?” It’s a thought-provoking piece, and you can read the on Slate."},
{"title": "Leading Public-Interest Technologist Sees National Research Resource as a Potential Foundation for an “AI Public Option”", "article": "As a chorus of transatlantic public interest groups calls for governments to build their own bedrock artificial intelligence systems, the Harvard Kennedy School’s Bruce Schneier says the National Artificial Intelligence Research Resource backed by key U.S. policymakers could lay the necessary groundwork. \"It’s a start, and [could] serve as a foundation for an AI Public Option,\" Schneier told referring to the NAIRR, a pilot for which is included in the Oct. 30 executive order on artificial intelligence. The NAIRR has also been highlighted in a series of closed-door AI \"insight forums\" hosted by Senate Majority Leader Charles Schumer (D-NY) who has said there was agreement with top Republicans to spend establishing it over the next five years. The idea that the public sector should provide AI systems that can be used to generate all manner of applications to benefit society—not just those that would enrich the handful of powerful companies currently in control of the technology—is being explored by individuals from certain startup companies as well as groups like Public Knowledge in the U.S. and Chatham House in the United Kingdom. Schneier is also a contributor to the new teasing out the associated policy details. \"A Public Option means building AI systems such as foundational large language models that would serve as an alternative to corporate-controlled AI,\" he said. \"Like public roads and a postal system, a public AI option can guarantee universal access to the technology that is fast becoming fundamental for participation in the economy.\" The idea has also taken hold within the UK’s Labor Party, with , proposing the government spend billions to build a \"Great British Cloud,\" and \"BritGPT.\" In the U.S., while some have said the NAIRR, as described in the executive order, represents to that kind of industrial policy, others are concerned that a plan the National Science Foundation produced for implementing the NAIRR would continue to rely on the major commercial cloud service providers who are primarily building foundational AI models for data storage and computing power. Schneier has establishing Public AI needn’t rely on governments \"owning and operating the entire AI supply chain,\" putting resources toward building data centers and special, super expensive, computer chips. He sees value in building Public AI to set an example for how to responsibly deploy the technology in a variety of policy areas. It \"could set an implicit standard that services offered by private entities must surpass,\" he said. \"Widely available public models and compute infrastructure would yield numerous benefits to the U.S. and to broader society. It would provide a mechanism for public input and oversight on the critical ethical questions facing AI development, such as whether and how to incorporate copyrighted works in model training, how to distribute access to private users whose insatiable appetites for AI integrations may outstrip cloud computing capacity, and how to license access for sensitive applications ranging from policing to medical use.\" \"It would serve as an open platform for innovation, on top of which researchers and small businesses ­as well as mega-corporations ­could experiment with novel training approaches and new user-facing applications,\" he said. \"An AI Public Option, administered by a competent and accountable public agency, would offer greater guarantees about the availability, equitability, and sustainability of AI technology for all of society than would exclusively private AI development.\""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": "The Hacker’s Mind with Bruce Schneier", "article": ""},
{"title": null, "article": ""},
{"title": "Obligatorisk Læsning", "article": "Jeg har lige lagt Bruce Schneiers “ ” fra mig og det bliver ikke nemt at gøre den retfærdighed i en boganmeldelse. De fleste af jer har aldrig hørt om før, men blandt IT folk er han et , hvis om kryptografi er obligatoriske klassikere i branchen. Denne gang har han skrevet en bog om sikkerhed der ikke handler om computere og faktisk kun halvvejs handler om sikkerhed. Bogen er i bund og grund en analyse af hvordan mennesker omgås hinanden, hverken mere eller mindre, men det er ikke nogen særlig hjælpsom opsummering, for det dækker alt fra affaldshåndtering over skattelovgivning til computersikkerhed. Det er et meget gammelt emne, men hvor langt de fleste bøger, fra Sun Tzu til Clausewitz behandler “os imod dem” problematikken, handler denne bog om “os imod os”: Konflikten imellem gruppen dens deltagere. Forfatteren opstiller en simpel model for hvorledes man kan analysere spændingsfeltet imellem fælleskabets og deltagernes interesser og bruger derefter modellen til at gennemgå en lang række eksempler, fra overfiskning til produktsikkerhed, for at vise hvorledes fælleskabets forskellige virkemidler over et stort felt kan tunes til at fremme fælles interesser, men aldrig helt undgå at nogen går over stregen. På sin vis er det “bare” en forlængelse af Von Neumans og Morgensterns spilleteori, men frem for analysen af spillet, der oftest ender med uafgjort eller uafgørligt, handler det her om designet af spillet: Hvorledes spilleregler indrettes og hvorledes kan de påvirke spillernes opførsel, således at resultatet bliver til fællesskabets fordel. Og reglerne skal der skrues på, for et fællesskab er altid under “angreb” fra fraktioner der modarbejder fællesskabets bedste, hvad enten de gør det af uvidenhed (invasive arter i haven), snævertsyn (børnevaccinationer) eller ond hensigt (kriminalitet). Man kommer vidt omkring, ikke mindst hvis man giver sig tid til at tænke de opstillede eksempler igennem selv, inden man læser Scheneiers bud på svarene, der bestemt ikke alle kan siges at være indlysende eller intuitive. Et af eksemplerne er en kemisk fabrik der er så uheldigt placeret, at et uheld vil have meget større omkostninger for samfundet, end den værdi virksomhedens ejere kan risikere at miste derved. Det rammer præcis situationen med Sojakagefabrikken. Virksomhedsledelsen havde ingen rationel grund til at gøre noget ved den uheldige placering: Det værste der kunne ske for dem var at firmaet gik fallit og den risiko kunne aldrig gøre en flytning af firmaet til en fornuftig beslutning. København havde meget mere på spil end nogle få millioner kroner og måtte derfor skride til forholdsvis drastiske midler, for at varetage gruppens interesse. Terminologien og analysen er hele vejen igennem værdineutral, hvilket tillader bogen at være præcis, uanset emnets moralske placering og dermed også relevant i de tilfælde hvor det er “outliers” der vinder, f.eks vedr. slaveriets ophør. Det er faktisk lidt af et kendemærke for Schneiers bøger, at de nøgternt blotlægger mekanismerne uden at tage parti for den ene eller den anden side, bogen er derfor en glimrende ballast hvad enten man skal holde et fællesskab sammen (f.eks en afdeling i et firma), splitte det ad (f.eks en rockergruppe) eller bare prøver at forstå hvad der foregår i de mange grupper vi indgår i. Et klart pletskud som bør læses af alle, både minoriteret og majoriteter. Bruce Schneier: Liars and Outliers Enabling the Trust that Society Needs to Thrive John Wiley & Sons ISBN: 978-1118143308 PS: Jeg har modtaget bogen som gratis anmeldereksemplar men det har på ingen måde forandret eller påvirket min holdning til den."},
{"title": "傳奇密碼學大師專訪：別輕信物聯網", "article": "在訪問被CNN譽為「全球最頂尖的密碼學家」布魯斯．施奈爾（Bruce Schneier）之前，我們很容易開始聯想各種神秘形象。畢竟，密碼這個關鍵字，總出現在情報、特務與駭客電影中，連電影《不可能的任務：鬼影行動》扯到核彈交易案時，也會有密碼學家的戲分。 小檔案_施奈爾 出生：1963年 學歷：英國西敏大學榮譽博士、美國美利堅大學電腦科學碩士 經歷：Counterpane網路安全公司創辦人 現職：IBM Resilient 技術長暨資安事業部特別顧問、哈佛大學伯克曼網路與社會研究中心研究員 著作：《應用密碼學》、《隱形帝國》 施奈爾的一生很有趣，他最暢銷的《應用密碼學》（Applied Cryptography）一書，被《連線》雜誌（Wired）評為：美國國家安全局最不希望被出版的書。 《達文西密碼》小說裡提到，如何以電腦加密保護資料時，必提的密碼學家是他。 當初揭露美國國安局秘密文件的愛德華．史諾登（Edward Snowden），被美國通緝後，極少數願意露面的場合之一，就是與施奈爾在哈佛大學的講堂視訊對談，討論主題還是：政府監控與隱私。 結果，當我們與這位傳奇人物對談時，他卻一點都不神秘，而且有夠明快，立場分明。 比如，他談最熱門的物聯網議題。他擺明：「危機比好處大更多！」他認為，從冰箱到烤箱，當所有東西都變成電腦連線，將會異常脆弱，容易被駭入，不夠安全。 比如，他看大家認為最安全的區塊鏈，他說：「沒有任何好的理由要去信任（區塊鏈）。」 比如，我們問，他如何保護自己的隱私時，他直說，不能答，「因為我的隱私一部分就是，不回答我如何保護我個人隱私的這些問題。」 這樣直來直往的人，為什麼會被資安業界推崇？連《經濟學人》都說，他是「安全大師」。 他是如何養成的？ 施奈爾出生在紐約，他的父親在布魯克林區擔任法官，父親形容，「他總是著迷於數字之間。」小時候，他會一直嘗試數數的極限，把數字寫在紙上，試著能不能數到100萬。 施奈爾曾說，他和父親在隱私、公民權利領域，有著共同的信仰。他不想和他父親一樣成為法官，「但我被法律的雄辯、寫作、邏輯、論證所啟發。」 他坦言，年輕時候，他熱愛數學更勝與人交流。從數學出發，數學會再延伸到密碼學，密碼學原理的一環，其實就是將訊息原本的樣子，經過加密運算之後，轉為讓人無法直接辨認出來的訊息，也就是密文，「理解密碼學就真的是理解數學。」 「本來，沒人把這些密碼學的知識這樣整理起來，可能就是在一些國防單位、國安單位，大家關起門來研究。」精通密碼學的台大數學系兼任助理教授陳君明說。但施奈爾卻想把他理解的密碼邏輯普及化，30歲的那年，他出版《應用密碼學》，在這之前，業界沒有一本完整、易懂的密碼學教科書。 「我因為這本書而認識了密碼學，」陳君明說。 只是，施奈爾又是如何從密碼學，變成資訊安全大師？ 陳君明解釋密碼與資訊安全的關係。他舉例，在我們生活之中，連到Google首頁時，網址是https，而不是http，字尾的「s」，就是加密，進入網頁之後，所有的通信都會有密碼系統保障安全，將資料變成密文傳送，但，如果你的電腦沒有密碼系統保護，在Google查詢或傳送的資料可能會被其他人看到！ 施奈爾說，「我的職涯可以說是一系列將事物『普遍化』的過程，從密碼學的數字安全開始，我的第一本書是《應用密碼學》，關於密碼學。然後我著手寫電腦安全，再來是一般的安全科技、安全經濟學、安全心理學、安全社會學、安全公共政策。」 他不斷探索，對安全有異於常人的執著。 他用經濟學角度談安全。「當我們在裝設防盜系統，我們付出的是時間、金錢、方便性抑或是自由的代價。我們要想的，不是它能不能讓我們安全，而是值不值得。」 施奈爾還用心理學看安全。「安全分為兩種，實際上的安全跟心理上的安全。」施奈爾提出「安全劇院」（Security Theater）的概念，因為很多讓人們感到安全的措施，並非是真的安全。 比如，你到機場安檢，先看你有無攜帶炸藥、槍枝之類的物品，他認為，其實一般人的時間都被浪費在沒收如剪刀、打火機之類的物品，但沒有人能證明，這樣是真的安全，只是「感受」起來比較安全，這讓安全劇院的實踐成本通常遠大於實際利益。 他又說，安全感是有錯覺的，我們時常放大不常見的危險，但卻忽略常見的危機。例如，大家常常擔心被陌生人綁架，但數據顯示，熟人綁架的可能性更高。我們也對於地球暖化或是個資被掌握的危機，常常輕忽。 當數字組成密碼，密碼形成安全，他發揮父親教給他的態度：追根究柢，反覆辯證。 陳君明回憶，他曾到美國參加知名的Defcon駭客大會，當時觀眾對施奈爾的一個提問是：「標準對稱式的加解密，安不安全？」提問者隨後補上一句：「美國政府講的我不信，但你講的我信！」 在施奈爾的部落格上，他有一篇文章是這樣寫的：未來，大型科技公司就像是地主一樣，亞馬遜想要讓Alexa智慧音箱成為智慧家居的中心，蘋果及Google要它們的手機成為唯一能控制你所有物聯網裝置的設備。這些大型科技公司保護我們免於外在攻擊，但是卻掌握著我們能看見的、或者能做的事。 他警告，若給予科技怪獸太多權力，後者也會給予我們更多限制，「（今日）HP印表機不再讓你使用非官方墨水匣，明天，他們可能要你用官方的紙張。」 在他眼裡，沒有為了安全就該犧牲隱私，使數據都被大廠掌握的道理，「我們需要隱私，隱私就是安全的一部分。」他說，他保證蔡英文總統用的絕對是iPhone，她的隱私，也就等同於台灣國家安全的一部分，沒有孰重孰輕；核電廠營運商也一樣，這些人的隱私都是國家安全的一部分。 這一生，他一路從數學延伸，談安全，現在還延伸到人與人彼此的信任機制。他解釋，安全的存在是為了促進信任，信任是目標，在現今社會，我們信任各式的人、機構和系統待我們是誠實的，「（信任）是讓一個社會生存的必需，沒有信任，社會會崩解。」至今，他一連寫了十幾本書。 這種超乎一般數學家埋頭苦算的態度，讓施奈爾不只是一個安全技術家，更是一個安全哲學家。他的一生不是忙著編密碼，而是忙著解密，找到「安全，如何與人類互利」的使命感，這或許，正是他被信任的原因。"},
{"title": "Liars & Outliers, o cómo se articula la confianza", "article": "es el nuevo libro de Bruce Schneier que por cortesía de Wiley. Aunque el libro sale a la venta en los próximos días, ya se y ciertamente si ya han leído otros libros de , seguramente no se arrepentirán de buscar este . Pero antes de que nadie se aburra, vamos con algo de chicha sobre el libro 🙂 Schneier, como es habitual, construye un ensayo sólido al que dota de un relato y un hilo conductor que te va desglosando en capítulos-píldora de unas 20 páginas. Por hacer un símil rápido, y salvando las (enormes) distancias, es a la seguridad lo que es a la economía. La comparación es un tanto injusta, porque Freakonomics es una simple colección de casos curiosos donde hace un enorme y valioso trabajo de conceptualización, pero ambos ilustran con gran certeza la enormísima dificultad de construir un sistema que equilibre los incentivos en la dirección deseada. Schneier lo hace, además, cumpliendo gratamente con las expectativas: continúa la línea de sus últimos libros, alejándose cada vez más de los aspectos técnicos de la seguridad e indagando en los sistemas que sirven para controlar nuestra reacción ante los riesgos que, en último término, nos llevan a evaluar cómo se gestionan y organizan los sistemas para respaldar las normas sociales. En ese último punto es donde transcurre este ensayo, en el que Schneier consigue explicar de forma sencilla lo que, en realidad, es bastante complicado. Siguiendo esquemas habituales en teoría de juegos (que enfrentan la norma de grupo frente a los intereses que entran en competencia con los mismos), Schneier va construyendo el marco teórico en el que se apoya para detallar algunas cosas interesantes. Hay muchas ideas en el libro, hoy quiero mencionar sólo algunas: Hay muchas otras ideas y prometo más adelante dedicar algún post a comentarlas. Pero ya me estoy alargando bastante así que por ahora esto es todo, sólo reiterar el hecho de que el libro me ha parecido una lectura de lo más recomendable. De alguna forma creo que no leía nada tan interesante desde que hace ya varios años me leí . : En un nuevo post he comentado ."},
{"title": "Interview with Bruce Schneier", "article": "Bruce Schneier is an expert for cryptography and computer security, developer of popular crypto algorithms, author of many books and co-founder of Counterpane Internet Security. Bruce Schneier: Most of my travel involves some speaking these days. I’ve just come back from participating in a seminar called “The Politics of Fear” at Tufts University. Next week I am speaking to staffers in Congress about data mining, and giving a lecture at “The Politics of Fear”. Later in the month, I head to Europe for a series of conferences. Quite a bit of my working life is like that these days. Cryptography was always a hobby, and I can remember the various children’s cryptography books I used to own. I did some work in cryptography for the U.S. government, but I didn’t become seriously immersed in the field until the early 1990s, when I was writing Applied Cryptography. Like many engineers, I’m proud of the algorithms I developed that are being used. For that reason I would point to Twofish and Blowfish. But cryptography has an interesting twist on that: the true measure of a cryptographer is his breaks. When I look back at my work, it is my cryptanalysis papers—the papers that broke other people’s algorithms in new and interesting ways—that I am most proud of. Most of what I am doing these days is about how security works in context. It’s not enough to have a good technical security solution, because so much of security has nothing to do with security. It’s important to understand the economics of security, the psychology of security decision making, and the legal framework in which security works. I’ve written a conceptual sequel; it’s called Practical Cryptography (John Wiley & Sons, 2003). It’s not the same book, though. Applied Cryptography was broad; it tried to survey the whole field of cryptography. Practical Cryptography is much more focused. It takes the basic problem of cryptography—setting up a secure channel between two people—and examines every aspect of it. I think it’s a better book for someone who wants to understand cryptography, and a much better book for an engineer who is trying to learn how to code a cryptographic system. Certainly cryptanalysis was a huge factor in World War II. Modern historians think that it shortened the war by two years or so. The reason is unique in history: the encryption machines were adding machine-era electromechanical devices, and the code-breaking machines were the world’s first digital computers. That isn’t true any more. While computer and network security techniques, both offensive and defensive, will play an important role in any hypothetical future “world war,” I think cryptanalysis will play a minor role. As to the “war on terror,” that’s a rhetorical war and not a real war. It makes no sense to declare war on an abstract noun. It makes no sense to declare war on a tactic. Further, it makes no sense to declare war on a tactic that has been with us for thousands of years, and will be with us for as long as we are a human civilization. Wars are declared by countries against countries, and end when one country is defeated or the two countries mutually declare peace. We already know what to call a tactic that has been with us since the beginning of civilization and will be with us until the end of civilization: crime. Terrorism is a particularly heinous and awful crime, but it is still a crime. Calling it a war only clouds our ability to deal with terrorism. All technologies have good and bad uses. In this way, cryptography is no different from anything else. We all use cars to drive around, and the bad guys use them to flee from robberies. We all use telephones to communicate, and the bad guys use them to plan crimes. This is okay because society is overwhelmingly made up of good and honest people, and the positive uses of these technologies far outweigh the negative uses. Restricting cryptography is, as you said, a tool of a dictatorship. It’s a tool of a police state. Any rhetoric about it being a way to fight terrorism is a lie; it’s a way to control the rest of the population. Of course. Security testing of closed-source systems can be very effective in improving products. It’s time-consuming and expensive, but it’s very effective. And it’s exactly the same with open-source testing. The difference is the economic model. In the closed-source model, the company developing the software pays experts to test and evaluate the security. In the open-source model, the developers throw the software out there and hope experts evaluate it out of the goodness of their hearts. Both models can result in well-tested software, and both models can result in lousy software. It’s not whether the code is open or closed; it’s who has evaluated it. PGP is not a flop. PGP Corporation is a viable company, and PGP is selling better than ever. It’s taken this long for PGP to be successful for a number of reasons, primarily the fact that it was too hard to use. PGP Corporation has spent a lot of effort making its user interface better, and in many cases invisible. It’s also true that e-mail encryption doesn’t solve the most pressing security problems. Your data isn’t likely to be eavesdropped on when it is in transit. It’s much more likely to be eavesdropped on when it is sitting on your computer, by some bad guy who hacks into your network. PGP doesn’t solve this problem. PGP has another product, PGP Disk, that encrypts files and directories and drives—but that’s something different entirely. Security is a process, not a product. The industry shows no sign of disappearing, but the solutions are looking more like services and less like products. This makes sense; the solutions need to react quickly as the threats evolve, and service-based solutions are better suited for that. I see this trend continuing. Another, parallel, trend is outsourcing. Organizations simply don’t have the expertise or resources to deal with security issues directly, and it makes far more sense to outsource it. These trends have combined in Managed Security Services, which will take over more and more of security. Every tool has its purpose, and no tool is useful in every situation. There are problems that lend themselves to math, and there are problems that don’t. Security is one of those problems that doesn’t, by the way. We can use the mathematics of cryptography to solve a very specific class of security problems, but the really hard ones are people problems. I’m not a good chess player, either, although I couldn’t tell you if it is from lack of ability or lack of practice. And I’ve never been good at memorization, which is what separates the serious Scrabble players from the hobbyists."},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": "Is This A Hack? Generating Income From Your Home. Bruce Schneier, Author of “A Hacker’s Mind”", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of “A Hacker’s Mind,” which answers the question. In this episode, we cover the phenomenon known as “house hacking,” which is—according to an article from Rocket Mortgage—“a modern lifestyle choice that borrows heavily from old-school ways and has been reimagined with the help of modern home-sharing platforms.”"},
{"title": "Is This A Hack? Beating The Customer Service Phone Line. Bruce Schneier, Author of “A Hacker’s Mind”", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of “A , which answers the question. In this episode, we cover an “ingenious hack,” according to The Daily Mail, that “helps callers bypass the endless automated questions now used by most major firms’ helplines and get straight through to a human being.”"},
{"title": "Is This A Hack? Password Sharing On Netflix. Bruce Schneier, Author of “A Hacker’s Mind”", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of “A Hacker’s Mind,” which answers the question. In this episode, we cover a common practice among Netflix users: password sharing, which gained popularity for allowing friends and family members to access a wider variety of content without having to pay for additional accounts."},
{"title": "Is This A Hack? Increased AirBnB Bookings. Bruce Schneier, Author of “A Hacker’s Mind”", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of “A Hacker’s Mind,” which answers the question. In this episode, we talk about AirBnB listings and how some property owners are increasing their property’s occupancy."},
{"title": "Is This A Hack? Lower Hotel Costs. Bruce Schneier, Author of “A Hacker’s Mind”", "article": "What is hacking? We asked Bruce Schneier, New York Times best-selling author of “A Hacker’s Mind,” which answers the question. In this episode, we talk about hotel costs and how some travelers are getting the most out of their stay."},
{"title": "Bruce Schneier on Trust", "article": "Modern society depends on trust more than we realise, and the basis for that trust is security. The trick, says the security guru, is preserving the forces that allow us to trust one another, while also knowing who not to trust Security exists to facilitate trust. Trust is the goal, and security is how we enable it. Think of it this way: As members of modern society, we need to trust all sorts of people, institutions and systems. We have to trust that they’ll treat us honestly, won’t take advantage of us and so on – in short, that they’ll behave in a trustworthy manner. Security is how we induce trustworthiness, and by extension enable trust. An example might make this clearer. For commerce to work smoothly, merchants and customers need to trust each other. Customers need to trust that merchants won’t misrepresent the goods they’re selling. Merchants need to trust that customers won’t steal stuff without paying. Each needs to trust that the other won’t cheat somehow. Security is how we make that work, billions of times a day. We do that through obvious measures like alarm systems that prevent theft and anti-counterfeiting measures in currency that prevent fraud, but I mean a lot of other things as well. Consumer protection laws prevent merchants from cheating. Other laws prevent burglaries. Less formal measures like reputational considerations help keep merchants, and customers in less anonymous communities, from cheating. And our inherent moral compass keeps most of us honest most of the time. In my new book , I call these societal pressures. None of them are perfect, but all of them – working together – are what keeps society functioning. Of course there is, and always will be, the occasional merchant or customer who cheats. But as long as they’re rare enough, society thrives. These notions of trust and trustworthiness are as old as our species. Many of the specific societal pressures that induce trust are as old as civilisation. Morals and reputational considerations are certainly that old, as are laws. Technical security measures have changed with technology, as well as details around reputational and legal systems, but by and large they’re basically the same. What has changed in modern society is scale. Today we need to trust more people than ever before, further away – whether politically, ethnically or socially – than ever before. We need to trust larger corporations, more diverse institutions and more complicated systems. We need to trust via computer networks. This all makes trust, and inducing trust, harder. At the same time, the scaling of technology means that the bad guys can do more damage than ever before. That also makes trust harder. Navigating all of this is one of the most fundamental challenges of our society in this new century. It might be the first rule of security, but it’s the worst rule of society. I don’t think I could even total up all the people, institutions and systems I trusted today. I trusted that the gas company would continue to provide the fuel I needed to heat my house, and that the water coming out of my tap was safe to drink. I trusted that the fresh and packaged food in my refrigerator was safe to eat – and that certainly involved trusting people in several countries. I trusted a variety of websites on the Internet. I trusted my automobile manufacturer, as well as all the other drivers on the road. I am flying to Boston right now, so that requires trusting several major corporations, hundreds of strangers – either working for those corporations, sitting on my plane or just standing around in the airport – and a variety of government agencies. I even had to trust the TSA [US Transportation Security Administration], even though I know it’s doing a lousy job – and so on. And it’s not even 9:30am yet! The number of people each of us trusts every day is astounding. And we trust them so completely that we often don’t even think about it. We don’t walk into a restaurant and think: “The food vendors might have sold the restaurant tainted food, the cook might poison it, the waiter might clone my credit card, other diners might steal my wallet, the building constructor might have weakened the roof, and terrorists might bomb the place.” We just sit down and eat. And the restaurant trusts that we won’t steal anyone else’s wallet or leave a bomb under our chair, and will pay when we’re done. Without trust, society collapses. And without societal pressures, there’s no trust. The devil is in the details, of course, and that’s what my book is about. My primary concerns are threats from the powerful. I’m not worried about criminals, even organised crime. Or terrorists, even organised terrorists. Those groups have always existed, always will, and they’ll always operate on the fringes of society. Societal pressures have done a good job of keeping them that way. It’s much more dangerous when those in power use that power to subvert trust. Specifically, I am thinking of governments and corporations. Let me give you a few examples. The global financial crisis was not a result of criminals, it was perpetrated by legitimate financial institutions pursuing their own self-interest. The major threats against our privacy are not from criminals, they’re from corporations trying to more accurately target advertising. The most significant threat to the freedom of the Internet is from large entertainment companies, in their misguided attempt to stop piracy. And the cyberwar rhetoric is likely to cause more damage to the Internet than criminals could ever dream of. What scares me the most is that today, in our hyper-connected, hyper-computed, high-tech world, we will get societal pressures wrong to catastrophic effect. This could be considered a companion book to my own. I write from the perspective of security – how society induces cooperation. Benkler takes the opposite perspective – how does this cooperation work and what is its value? More specifically, what is its value in the 21st century information-age economy? He challenges the pervasive economic view that people are inherently selfish creatures, and shows that actually we are naturally cooperative. More importantly, he discusses the enormous value of cooperation in society, and the new ways it can be harnessed over the Internet. I think this view is important. Our culture is pervaded with the idea that individualism is paramount – Thomas Hobbes’s notion that we are all autonomous individuals who willingly give up some of our freedom to the government in exchange for safety. It’s complete nonsense. Humans have never lived as individuals. We have always lived in communities, and we have always succeeded or failed as cooperative groups. The fact that people who separate themselves and live alone – think of Henry David Thoreau in – is so remarkable indicates how rare it is. Benkler understands this, and wants us to accept the cooperative nature of ourselves and our societies. He also gives the same advice for the future that I do – that we need to build social mechanisms that encourage cooperation over control. That is, we need to facilitate trust in society. , by the biologist Robert Trivers. Trivers has studied self-deception in humans, and asks how it evolved to be so pervasive. Humans are masters at self-deception. We regularly deceive ourselves in a variety of different circumstances. But why? How is it possible for self-deception – perceiving reality to be different than it really is – to have survival value? Why is it that genetic tendencies for self-deception are likely to propagate to the next generation? Trivers’s book-long answer is fascinating. Basically, deception can have enormous evolutionary benefits. In many circumstances, especially those involving social situations, individuals who are good at deception are better able to survive and reproduce. And self-deception makes us better at deception. For example, there is value in my being able to deceive you into thinking I am stronger than I really am. You’re less likely to pick a fight with me, I’m more likely to win a dominance struggle without fighting, and so on. I am better able to bluff you if I actually believe I am stronger than I really am. So we deceive ourselves in order to be better able to deceive others. The psychology of deception is fundamental to my own writing on trust. It’s much easier for me to cheat you if you don’t believe I am cheating you. There have been a number of books about the violent nature of humans, particularly men. I chose both because it is well-written and because it is relatively new, published in 2005. David M Buss is a psychologist, and he writes well about the natural murderousness of our species. There’s a lot of data to support natural human murderousness, and not just murder rates in modern societies. Anthropological evidence indicates that between 15% and 25% of prehistoric males died in warfare. This murderousness resulted in an evolutionary pressure to be clever. Here’s Buss writing about it: “As the motivations to murder evolved in our minds, a set of counterinclinations also developed. Killing is a risky business. It can be dangerous and inflict horrible costs on the victim. Because it’s so bad to be dead, evolution has fashioned ruthless defences to prevent being killed, including killing the killer. Potential victims are therefore quite dangerous themselves. In the evolutionary arms race, homicide victims have played a critical and unappreciated role – they pave the way for the evolution of anti-homicide defences.” Those defences involved trust and societal pressures to induce trust. is Steven Pinker’s explanation as to why, despite the selection pressures for murderousness in our evolutionary past, violence has declined in so many cultures around the world. It’s a fantastic book, and I recommend that everyone read it. From my perspective, I could sum up his argument very simply: Societal pressures have worked. Of course it’s more complicated than that, and Pinker does an excellent job of leading the reader through his analysis and conclusions. First, he spends six chapters documenting the fact that violence has in fact declined. In the next two chapters, he does his best to figure out exactly what has caused the “better angels of our nature” to prevail over our more natural demons. His answers are complicated, and expand greatly on the interplay among the various societal pressures which I talk about myself. It’s not things like bigger jails and more secure locks that are making society safer. It’s things like the invention of printing and the resultant rise of literacy, the empowerment of women and the rise of universal moral and ethical principles. , by the neuroscientist Patricia Churchland. This book is about the neuroscience of morality. It’s brand new – published in 2011 – which is good because this is a brand new field of science, and new discoveries are happening all the time. Morality is the most basic of societal pressures, and Churchland explains how it works. This book tries to understand the neuroscience behind trust and trustworthiness. In her own words: “The hypothesis on offer is that what we humans call ethics or morality is a four dimensional scheme for social behavior that is shaped by interlocking brain processes: (1) caring (rooted in attachment to kin and kith and care for their well-being), (2) recognition of other’s psychological states (rooted in the benefits of predicting the behavior of others) (3) problem-solving in a social context (e.g., how we should distribute scarce goods, settle land disputes; how we should punish the miscreants) and (4) learning social practices (by positive and negative reinforcement, by imitation, by trial and error, by various kinds of conditioning, and by analogy).” Those are our innate human societal pressures. They are the security systems that keep us mostly trustworthy most of the time – enough for most of us to be trusting enough for society to survive. Of course not. There are two parts to the question. One: Are we doing the right thing? That is, does it make sense for America to focus its anti-terrorism security efforts on airports and airplanes? And two: Are we doing things right? In other words, are the anti-terrorism measures at airports doing the job and preventing terrorism? I say the answer to both of those questions is no. Focusing on airports, and specific terrorist tactics like shoes and liquids, is a poor use of our money because it’s easy for terrorists to switch targets and tactics. And the current TSA security measures don’t keep us safe because it’s too easy to bypass them. There are two basic kinds of terrorists – random idiots and professionals. Pretty much any airport security, even the pre-9/11 measures, will protect us against random idiots. They will get caught. And pretty much nothing will protect us against professionals. They’ve researched our security and know the weaknesses. By the time the plot gets to the airport, it’s too late. Much more effective is for the US to spend its money on intelligence, investigation and emergency response. But this is a shorter answer than your readers deserve, and I suggest they read more of my . Like everything else, cloud computing is all about trust. Trust isn’t new in computing. I have to trust my computer’s manufacturer. I have to trust my operating system and software. I have to trust my Internet connection and everything associated with that. I have to trust all sorts of data I receive from other sources. So on the one hand, cloud computing just adds another level of trust. But it’s an important level of trust. For most of us, it reduces our risk. If I have my email on Google, my photos on Flickr, my friends on Facebook and my professional contacts on LinkedIn, then I don’t have to worry much about losing my data. If my computer crashes I’ll still have all my email, photos and contacts. This is the way the iPhone works with iCloud – if I lose my phone, I can get a new one and all my data magically reappears. On the other hand, I have to trust my cloud providers. I have to trust that Facebook won’t misuse the personal information it knows about me. I have to trust that my data won’t get shipped off to a server in a foreign country with lax privacy laws, and that the companies who have my data will not hand it over to the police without a court order. I’m not able to implement my own security around my data; I have to take what the cloud provider offers. And I must trust that’s good enough, often without knowing anything about it. Seven."},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": "Liars and Outliers", "article": "I got signed, for cheap, direct from on the condition that I write a review of it. He sold 100 of them this way as a pretty clever way to stir up some publicity. It also worked as a motivator for me to actually write about it. The basic gist of it is that while I enjoyed the book, it felt like he was preaching to the choir. I didn’t find very much new information (though size-weight misperception was new to me and seems pretty interesting), and my guess would be the type of person that’s likely to pick up this book and read it is in the same boat. There are countless people who absolutely need to understand the concepts it contains, but I’m unconvinced they are a likely audience. I have to admit that I sort of hated the logistical aspects of this printing. There was an abundance of tables that largely seemed unnecessary, the top and bottom margins seemed to vary without a whole lot of rhyme or reason, and all of the notes were endnotes. Why would anyone publish a book with all of the interesting side bits shoved to the end (along with this one, see )? Have these publishers never read anything by ? Footnotes that are interspersed throughout the work rather than being relegated to 37 pages at the back a) are going to actually be read, and b) exponentially increase the quality of the work. I’m sorry, but keeping two bookmarks and constantly flipping back and forth just isn’t worthwhile when I’m actively reading, and as a result I’m missing out on context and content. From briefly talking to about this, it seems that it’s something on the publisher or printer or other non-author end, but seriously, for research-based writing footnotes are leaps and bounds above endnotes. Anyway, back to the content, Schneier does an excellent job of presenting the research, but he almost comes off timid. There seems to be a reluctance to really call out those making unrealistic and/or harmful security tradeoffs. Yes, many of the decisions can be rationalized and explained, but they are still bad tradeoffs. In his more informal writing, Schneier is blunt about security theater and all that it entails, and I felt like that bluntness was sorely lacking in much of this book. As someone who is familiar with a lot of the research in this area, I have to believe I am not the target audience, no matter how much I may have wanted to be. I think that’s probably my mistake in expecting something far more in depth, but the reality is that the audience for that level of research is considerably smaller than the audience for which it feels like this book was written. For someone ignorant to the field looking for an overview introduction to security in all its forms, I don’t know of a better book."},
{"title": "Is Online Convenience Worth the Trade-Off for Less Cybersecurity?", "article": "Marcus Smith interviewed Bruce Schneier on BYU Radio’s “Constant Wonder.”"},
{"title": "Trust and Society", "article": "I used to think that was out of touch with industry CISOs, but now I think that they are out of touch with him. He’s come on tremendously in recent years. I saw him present to the United Nations last year and he was awesome, reflecting a lot of research and deep thinking about important issues such as trust, risk, surveillance and cyber warfare. I shall be ordering a copy of his new book . It’s about trust, a subject I find both relevant and fascinating. Trust is a phenomenon that few security researchers seem to understand. The problem is that it’s a means to an end, and makes little sense when studied in isolation from its purpose. The nature of trust is also changing as we move from an industrial-age dominated business landscape to the information age.  I find this paradigm shift is neatly captured by two Russian proverbs. The first, ascribed to both Stalin and Lenin, is “Trust is good, control is better,” which encapsulates industrial-age thinking for vertically integrated enterprises and societies. The second, made famous by Ronald Reagan, is “Trust, but verify,” which reflects our best endeavors for managing situations in a modern, diverse supply chain that is increasingly beyond our direct control."},
{"title": "\"Life is Insecurity\"", "article": "Schneier: I have always believed it takes a certain personality type to be interested in security. I’ve heard it called “paid professional paranoid,” but it’s more complicated than that. At its core, security is about figuring out how things work and how things can be made to not work. It’s about looking at systems and figuring out how to get around them. I think it’s the no-rules intellectual challenge of security that intrigues me. Schneier: I don’t think Crypto-Gram is so popular because it’s the best source for security information. There are far better news sources out there, as well as far better publications devoted specifically to IT security. I think Crypto-Gram is popular because I write it in a no-bullshit style. I write about the security issues that interest me – technological, social, political – and spend a lot of time explaining the news. I think people appreciate the common sense, and that I don’t have any corporate agenda. Schneier: Security is all about economics. All security decisions are trade-offs, and businesses make those trade-offs primarily economically. This is why we see, for example, so many thefts of personal information from corporate databases. The corporations are not liable for the losses, so they’re not going to spend a lot of money securing the data. The way to improve security is to recognize this economic truth and work with it: force the entity that’s in the best position to mitigate the risk to be responsible for the risk. And the security issue that’s really important right now: crime. Schneier: This question depends a lot on which country you’re in, because the economics of security depend a lot on the regulatory environment. But whatever country you’re in, regulation and compliance is probably your most important security-related topic right now. After that, crime. Crime crime crime. I don’t think there’s a number three that even comes close to those two. Schneier: The second part of your question should be: “and should anyone even care?” The answer to that second part would be “no.” Encryption, even lousy encryption, is generally the strongest part of a computer security system. If a system gets broken, it will invariably be because of a vulnerability somewhere else: the software, the user interface, the network, the installation, etc. So there’s basically no point in losing sleep over the encryption. It’s like putting a tall spike into the ground and hoping your enemy will run right into it. You can argue about whether the spike should be a mile tall, or a mile-and-a-half tall, but the enemy is just going to go around the spike. That being said, I am sure of two things: First, that there are more powerful cryptanalytic tools against today’s encryption algorithms than we know of right now. The second point is that even with these tools the algorithms will be secure against any practical attack. Schneier: Of course technology can’t solve the security problem. It will always come down to people. Until people are replaced by robots, I guess. Schneier: Microsoft will be a preferred target for hackers as long as they have a preferred position in the marketplace, but criminals don’t care which systems they use to attack your networks. I think that customers of every product need to worry, and if SAP thinks it can hide, it’s sadly mistaken. Schneier: Crime rarely pays more than crime prevention. For one, as a criminal you can’t learn from your mistakes – you go to jail for them. Because making mistakes is risky, criminals don’t tend to learn very fast. And you’re not going to see a lot of “new ideas” in Internet crime, just variants of old ideas. I don’t think we’ve seen anything new, really. What we have seen is conventional crime moving to the Internet and taking advantage of the economies of scale inherent in computers. Schneier: One without people. That is, one that will never exist. Life is insecurity, and the sooner we get used to that, the better."},
{"title": "Firm Finds Big Security Holes in Windows NT", "article": "Flaws in Microsoft Corp.’s Windows NT software threaten the security of companies using the Internet to tie together their far-flung corporate locations, a computer security consulting firm declared on Monday. “We were able to sniff passwords, eavesdrop on the networks, and passively do traffic analysis,” said Bruce Schneier, president of Counterpane Systems Inc., of Minneapolis, Minn. “Any Microsoft NT server on the Internet is insecure.” Counterpane discovered the problems while doing a security analysis on a Windows NT, an operating system used by a swiftly growing number of corporations as the foundation for their computer networks. Microsoft confirmed the security problems later the same day. VPNs increasingly popular The flaws weaken the security of so-called “virtual private networks,” or VPNs, based on NT and point-to-point tunneling protocol, or PPTP. These VPNs connect company networks from various locations and are quickly becoming popular in the corporate world as a low-cost solution to buying a dedicated phone line to connect computers between company sites. “A lot of people are creating their virtual private networks using NT,” said Schneier. “That makes the flaw that much more serious.” The PPTP is Microsoft’s homegrown way of securely sending and receiving data over the public Internet. It’s also used to identify whether the person logging in a valid user. But the software giant would have been better off using one of the public—and stress-tested—standards, said Schneier. “Developing security implementations in-house is very difficult to do right,” said Schneier. “That’s why it’s important to adopt a publicly tested and recognized standard.” Microsoft promises fix ASAP Windows NT system can use either a 40-bit or 128-bit encryption key to protect a company’s data. Those keys, in and of themselves, are extremely secure. The problem is that NT secures those keys with a flawed password system. “Anyone with a list of the top 10 million passwords can break over 99 percent of the systems out there,” he said. Microsoft promises to fix the flaws as soon as possible. “(Part of the problem) is already fixed,” said Karan Khanna, product manager for Windows NT security at Microsoft. “We will be releasing patches to fix the rest as soon as we can.” Khanna attempted to put the flaws in perspective. “The amount of security an organization enforces depends on its needs,” he said. “The CIA spends billions of dollars on security—our customers don’t need the level.” Is fix worse than flaw? That you-get-what-you-pay-for philosophy could quickly backfire on the software giant, however. Despite the stress on getting fixes out as soon as possible, many times such patches just make more problems for system administrators, said Schneier. “Last time they released a fix, it broke so many other parts of Windows NT, Microsoft had to pull it off the Web site three weeks later,” he said."},
{"title": "Schneier: Beware Security Products", "article": "A leading security expert has warned businesses to beware of buying shoddy security products. Bruce Schneier, founder and chief technical officer of BT Counterpane, issued the warning at the RSA Conference Europe 2007 in London on Tuesday. He told delegates that to give a fair representation of the security of those products. “There might be a political bent to security decisions, or there might be a marketing bent,” said Schneier, citing as an example people selling smart cards who “do a lot to convince us that smart cards are the answer to security problems. For every company that’s secure, there’s at least one ‘me too.'” Schneier said it was difficult for companies to judge the security of varying products because known attacks are relatively rare, making it hard to collect enough data for security-product evaluations. “If events are high-damage and rare it’s difficult to get data. I’m not going to know (the validity of a product) because I don’t have the data. After 9/11 there was a huge inquiry into what went wrong, but it’s hard to tell what went wrong because it was one event. There’s not enough data,” said Schneier. “The (security) market is asymetrical—the seller knows a lot more than the buyer,” said Schneier. “In the U.S., a lousy used car is called a lemon—but you don’t know until you drive it off the lot that it’s a lemon.” If marketed correctly, bad products can drive good products out of the market, Schneier warned. “Products can have the same claims, the same algorithms, the same buzzwords, and one is very secure while the other is just slapped together. If there’s no functional way to test a product, you’ll buy the cheaper one,” said Schneier. Schneier said that due to market dynamics, good products tend to rise to the top, but that the market . He warned businesses not to get “caught up in the feeling of security, driven by fear, rather than the reality.” “Fundamentally, we are not rational,” said Schneier. “The brain is just barely functioning in the security community. It’s still in beta testing. There’s weird holes and shortcuts, and all sorts of patches and work-arounds.” Businesses should evaluate security products very carefully, said Schneier, and find trusted individuals with expertise who can make security decisions within a company. Eric Baize, senior director of the product security office of storage company EMC, agreed that there were both good- and bad-quality security products available. “The law of statistics is such that in anything there are good- and bad-quality things,” said Baize. “This applies to wine, food, and security products. There has been a lot of discussion about whether security should be added on to the infrastructure, or included as a core feature. Now in the security space companies are selling secure infrastructures.” Shannon Kellogg, director of information security policy for security company RSA, said that it was critical to build security into systems from the beginning. “Building core security functionalities is absolutely critical,” Kellogg said. “Systems in the past didn’t have security functionalities, but it enables your company to do more. If your car has brakes, it enables you to go faster.”"},
{"title": "The Coming AI Hackers. How Will They Put Society At Risk?", "article": "Bruce Schneier is an internationally renowned security technologist, author, fellow at the Berkman Klein Center for Internet and Society at Harvard University, and a lecturer in public policy at the Harvard Kennedy School. In this episode, he joins host Hillarie McClure to discuss his latest research and paper “The Coming AI Hackers.”"},
{"title": "No Name Podcast with Bruce Schneier", "article": "Bruce Schneier spoke with Ruslan Kiyanchuk on the No Name Podcast."},
{"title": "Is the Future Secure?", "article": "This week on The Futurists we get into the future of cybercrime and personal security in the smart world with renowned “security guru” Bruce Schneier. The author of over a dozen books (his latest bestseller being ), Lecturer on Public Policy at Harvard Kennedy School, Congressional advisor and Media personality. Will AI and Quantum kill passwords? How secure will your DNA records be? The answers might surprise you."},
{"title": "Bruce Schneier on His New Book, a Hacker’s Mind", "article": "Bruce Schneier: These threads have been percolating in my head for a while now. I started writing about the psychology of security around 2008. That quote is something I have been saying for decades. The notions of socio-technical systems and how they can be attacked are just as old. In I am taking what we know about hacking in the computer field and applying it to our broader world: laws, economics, politics, society. Our world is both complex and technical, and taking advantage of rules is common everywhere. (As an example, think of tax loopholes.) I am drawing out that idea, and adding notions of wealth and power. Bruce Schneier: First, thank you for that. One of the most fun things about writing the book were the stories of hacking: from religion, from sports, from politics, from finance, from casino games  on and on and on. We humans have always been clever hackers. If there is a set of rules standing in our way, we try to get around them. Technology—or, more accurately, people using technology—exploits many of our cognitive systems. Social media companies exploit how our brain decides what to pay attention to, and how we become addicted to things. Terrorism exploits fear. Modern politics exploit both authority and tribalism. It’s not that anything here is new, it’s that technology allows these exploitations to scale to an unprecedented degree. I think of all of these as hacks of our cognitive functions. Our brains are optimized for the environment we experienced living in small family groups in the East African highlands in 100,000 B.C.E. We’re less optimized for Boston in 2023. Bruce Schneier: AI is going to change our world unlike anything we’ve experienced in our lifetimes, and it would be better if they weren’t optimized for the near-term financial interests of a handful of tech billionaires. The ethical concerns are huge, and important. Regulation is vital. Deliberation is vital. Understanding what we want from our future is vital. The six-month moratorium is a red herring—I don’t think any of the signatories expected it to actually happen—but it’s a useful starting point for a conversation. I am advocating for an AI public option: a model funded by the government, designed in the open, and freely available. That will be an important counterbalance to the corporate models, and one that will become increasingly important as these AI systems start affecting how democracy functions. Bruce Schneier: That’s the question that: 1) no one has any idea how to answer; and 2) is desperately trying to answer. It’s clear that AI—ignore generalized AI—is going to dramatically change many aspects of our lives. And while we can point to a few of them, we really don’t know the extent of those changes. And we have no idea of the social changes that will result from those technological changes. Will it dismantle traditional power structures? If previous technological revolutions are any guide: yes. How? We don’t know. What I can tell you is to be cautious about what the current AI systems have to teach us about the future. We know a lot about the strengths, weaknesses, and limitations, of a particular large language model—ChatGPT. That tells us nothing about in five years, or next year, or even the end of this year. Research is moving so fast, and things are being deployed at such a breakneck pace, that it’s all going to change again and again. Never say: “the AI cannot do X.” Remember to say: “The AI I played with in Spring 2023 can’t do X.” Bruce Schneier: We can’t ban TikTok. We don’t have the Internet censorship infrastructure necessary to enforce such a ban, and we’re not going to build a China-level surveillance system in order to do so. We could ban U.S. companies from doing business with ByteDance—the company that owns TikTok—but that won’t result in anyone not being able to use the service. (It would get the app off most peoples’ phones, so that’s a thing.) So largely, I view all the banning talk as posturing. Nothing TikTok does is different from what Google and Facebook do. Even the algorithms pushing people towards extreme content: we see the same things out of the Facebook and YouTube (owned by Google) algorithms. We have built an Internet that runs on surveillance. It seems kind of late for us to start complaining about who in particular is doing that surveillance. If we are serious about the risks of TikTok—and there are risks—I would like us to address surveillance capitalism as a whole. All of these companies are spying on us constantly, and all of them are using their recommendation algorithms to push us towards extreme content because that’s what makes them more money. The problem isn’t China; it’s the business models we’re allowing."},
{"title": "Bruce Schneier Wants to Recreate Democracy", "article": "Arguing that American democracy has been hacked, the computer security expert doesn’t want to just fiddle on the margins when it comes to re-envisioning what a new 21st-century American democracy should look like. Like many people cooped up at home during COVID-19, Bruce Schneier had a pandemic project. In this case, it was a new book called , which encourages readers to apply the hacker mentality to our various social, political, economic, and legal systems. Schneier’s work on the book sparked deeper thinking about the suitability of our centuries-old democratic processes and institutions and whether they were still up to the task in our ever-increasing polarized and fractured political climate. “Democracy has been hacked, mostly for the worse,” Schneier, a computer security specialist and privacy expert who is a faculty affiliate at the Ash Center, is quick to note. “Our democracy in the United States is really just not suited to the task anymore.” But if American democracy is no longer up to snuff in Schneier’s mind, the question quickly arises: What should a new American democracy look like? “I don’t have the details, but we need to set about building newer, more resilient democratic systems that are better suited for our current technology,” he says. To better tackle the challenge head-on, Schneier opened his voluminous address book and invited dozens of scholars, practitioners, advocates, and writers for an at the Kennedy School in December to discuss how, in essence, to recreate American democracy. Schneier revels in bringing together thinkers from all walks of life to tackle seemingly intractable political, policy, or technology problems. “I wanted to have political scientists, law professors, other social scientists, and academics. They all bring important perspectives to this question. But I wanted to hear from individuals you wouldn’t traditionally associate with a Kennedy School workshop, like science fiction writers who have their own unique way of looking at problems and imagining solutions.” The , which covered ground far beyond the typical academic confab on democracy, examined fundamental questions about the structure of our democratic institutions. “I wanted to have discussions centered around issues that aren’t typically in the spotlight, but get at the fundamental nature of our democracy, such as whether we should even have elected representatives, or maybe just vote for ideas and goals.” To frame the exercise, Schneier urged participants to “imagine you’ve landed on a foreign planet and need to govern yourselves. How would you set it up?” He admits that it may be hard not lean on preconceived notions of what contemporary democracies should look like, but the exercise is intended to get people to think about a different set of ideas. “What we’re missing now are ideas. We know what to do with our current system, whether it’s reforming the electoral college or making election day a national holiday. But if we could sweep the board and start from scratch, how would you do it? That to me is what’s interesting.” Participants considered proposals for building new democratic spaces that could take the place of the sclerotic governing institutions that many argued are no longer capable of meeting the needs of modern society. “What are the alternatives? Citizen assemblies? How do we build an enduring system of collective governance,” Schneier recalls. “We aren’t talking about democracy 2.0 or 3.0, we’re looking well ahead to 6.0 and 7.0.” As a public-interest technologist, Schneier is quick to note that the challenges facing our democracy can’t just easily be chalked up to technology. “I don’t think it’s entirely a technology problem. And there was this tension in the room about whether technology is part of the solution. And I think it has to be. We live in a technological world.” To illustrate how assumptions about technology are embedded in the bedrock of American democracy, Schneier points to the very nature of representative government itself. “I came in with this thought that the modern constitutional republic is the best form of government mid-18th century technology could invent. Because travel and communications are hard, we need to pick one of us to go all the way over to a capital city and pass laws in our name. But that paradigm has been erased. Congress, statehouses, and town halls over the last three years suddenly realized they don’t need to physically convene in a building to debate and pass laws. Now that travel and communications are easy, are there other ways?” Looking back, Schneier is quick to note the enormity of the challenge laid out before participants at the workshop. “People have been thinking really smart thoughts about this for thousands of years, whereas social media is only a few decades old,” he admits. “Humans haven’t changed, even though technology has changed. And we’re still searching for solutions. And that is very sobering.” Yet, Schneier is confident that there are solutions to be found. Another interactive and facilitated workshop is scheduled for May with additional future sessions already in the works. And he doesn’t want the conversation to start and stop in Cambridge. To help others imagine the possibilities for recreating democracy, Schneier is already imagining what a public “Atlas of Democracies” might look like."},
{"title": "History of Hacking", "article": "I spoke about the history of hacking with Steve Morgan of the Cybercrime Magazine Podcast."},
{"title": "A Hacker’s Mind. New Book. Bruce Schneier, Security Technologist and Cryptographer.", "article": "Bruce Schneier is a public-interest technologist, working at the intersection of security, technology, and people. He is the New York Times best-selling author of the book, “Data and Goliath,” and author of the new book, “A Hacker’s Mind: How the Powerful Bend Society’s Rules, and How to Bend Them Back.” In this episode, Schneier joins host Steve Morgan to discuss his new book, deep dive into a few chapters, and more."},
{"title": "Interview: New Threats to the Internet Infrastructure", "article": "Jean Friedman interviewed Bruce Schneier about his talk at RSA 2012."},
{"title": "\"Click Here To Kill Everybody\" Book Review by Cybersecurity Expert Scott Schober", "article": "Forget the fact that this esteemed security expert is also a cryptographer and author of seminal cybersecurity books including Data and Goliath and Liars and Outliers…does Click Here to Kill Everybody live up to its own hype or is is just all theatrics? Although I’ve never met Bruce Schneier, I can gather from his personality and the way my colleagues speak of him that he is the security expert’s expert. Up until June of this year, Bruce was the CTO for Resilient Systems, a private company that offered incident response solutions. Basically, IBM saw that they were doing good work cleaning up corporate security messes all over the infosec world and entered into an agreement with them not too long before acquiring them back in 2016. Schneier, their CTO had already made a name for himself as a fellow at the Berkman Center for Internet and Society at Harvard Law School and also as a burgeoning writer of many technical publications on cryptography and books on cybersecurity. But those credentials don’t necessarily translate into great reading of 300+ pages of a cybersecurity tome so how does Click Here To Kill Everybody: Security and Survival in a Hyper-connect World fare? Schneier’s take on current and future security trends is so detailed, I sometimes needed a notepad to keep up. His meticulous reporting, cross referencing and modeling of security trends and breaches is to be admired although I’ll admit, I sometimes felt like I was sitting in one of his classrooms at Harvard. Schneier sets up his entire book by introducing Internet plus, a kind of idealized version of our current Internet. Here, it feels like Schneier is riffing off of common marketing speak including brands such as the Internet of Things. Brands like these promise minimal consumer frustration but actually deliver much more frustration than even archaic communications platforms such as email and SMS text do. This is because for all of IoT’s promises and standardization, we the consumers get mostly cheap but unsecured devices for our dollars. An empty smart fridge is great if it can tell me exactly what I need to shop for but what’s the point if it can also be easily hacked to spread malware through other connected devices and eventually steal my passwords, data and more? Schneier’s Internet Plus relies heavily on education, standards and regulation. Throughout this book, Schneier is not afraid to invoke strong government regulation as a means to both stimulate competition and encourage things like research and job creation. I find his reliance on governance a little refreshing but some of his suggestions such as comprehensive software product liability measures might overstep his optimism a bit. But how can the man that titles his book Click Here To Kill Everyone even be considered an optimist? One of the most enjoyable aspects of this book was the author’s adept warnings mixed with caution and optimism for our digital future. This book is not the work of a cynic. Otherwise, Schneier wouldn’t have detailed numerous solutions for myriad problems the cybersecurity community faces as a whole. Click Here To Kill Everyone is an absurd premise when taken literally but as Bruce Schneier details the problems, it steadily becomes a grim possible reality. Fortunately, experts including the author feel confident that our collective security woes are not too great to overcome in the future. I give this book 5 out of 5 stars. Stay safe."},
{"title": null, "article": ""},
{"title": null, "article": ""},
{"title": "Liars & Outliers", "article": "I’ve been a Bruce Schneier fan for years. I read his blog often enough that I don’t feel the need to read his books. But then he offered a discount on a signed edition of his latest book– with the one stipulation that I So here’s the review. A lot of brilliant thinkers tend to get stuck in their own perspective. There are plenty of mathematical geniuses who can’t contemplate the implications of their ideas. Plenty of programmers who can’t understand why users don’t recognize the brilliance of their user interfaces. Bruce Schneier isn’t one of them. His rose to fame with a book of algorithms. But because great encryption doesn’t help when it’s part of weak security, he’s written written with increasing breadth about security. After 9/11, he wrote about how we individually and as a society make poor security trade-offs. Now he’s written This takes an even wider view, as he asks: how is it that people can trust each other at all? For example, a man gets into a taxi in a foreign city. The man and the driver will never see each other again. The taxi driver could easily rob him and get away, or he could ride without paying. And yet every day all over the world, drivers and passengers can trust each other. When I got this book, my first thought was After all, Schneier has said he wants this to be read by all sorts of decision makers. I’ve heard that if you want to be read by busy people, make your book just long enough to be read in a single long plane trip. Then again, I’m not a best-selling author. But it turns out this isn’t such a long read, it’s 250 pages, plus another 100 pages of notes and references. (This book-to-notes ratio is up there with , which has a remarkable amount of overlap with ) It’s really two books in one: a quicker, drier, less technical read without endnotes, or a more colorful read with endnotes. I kept one bookmark in the endnotes at all times. is an overview of trust and cooperation. It draws primarily from sociology, economics, and psychology with a big helping of evolutionary biology, game theory, and security. Philosophy and theology also show up, to add a bit of color. It’s about the different strategies for enforcing conformity in a group, and when they do and don’t work. This is practical stuff for anyone who needs to manage a large group, whether it’s an online discussion group, a corporation, or a country of taxpayers. The book takes pains to discuss this from a neutral perspective: the forces are the same whether it’s a just society versus murderers or a murderous society against saints. To this end, Schneier uses the rather awkward term “defector” to refer to rule breakers. The book is summed up quite well in the first chapter. In fact, if you’re in a hurry you could just read the first two and the last chapter. Occasionally the book starts to read like a taxonomy, as Schneier explains the breadth of, for example, security techniques. This is an academic book trying– and usually succeeding– in being a general-audience book. While it does drag a little at times, there are plenty of popular just-for-fun nonfiction books that drag more. (I’ll admit, though I read a lot, I’m not much of a book reader.) And with the whole history of deceit and treachery to draw upon, he has plenty of colorful examples. I feel like I’ve said a lot about the book without saying much about the contents of the book. As someone who not only reads Schneier’s blog, but reads many of the same sources he draws upon, there weren’t many ah-ha moments for me. Perhaps the biggest one is this: trust is rare in nature. It’s hard to establish and easy to break, but once established it yields huge benefits. And humans are the most cooperative and trusting species on Earth. Why? In part because we have the greatest capacity to evaluate reputations, so we know when not to trust. In part because of a sense of morality which leads us to punish rule breakers. I hear plenty of people argue that corporations are inherently inhuman and inhumane. While Schneier doesn’t say that exactly, he spells out each of the pressures society uses to enforce conformity to social norms, and how corporations respond to only two (reputational and institutional [legal.]) The overarching message of the book is that there are different ways of establishing a trusting relationship, and they work on different scales. Neighbors directly evaluate each other. People who know each other only by reputation can go by that. Less intimate groups use morals and institutions to maintain group norms. And when all else fails, security mechanisms can make non-compliance difficult. None of these completely eliminate non-conformity, which is good because even good rules can have bad consequences. The trick is using the right tools– and the right amount of pressure– under the right circumstances. It’s depressing to consider all the ways that things go wrong– from suicide bombers to self-destructive investment banks to a lack of a global response to global warming. But at the same time, humans are amazing creatures in that we even have a cooperative arrangement as abstract as the United Nations or a coalition of corporations."},
{"title": "Summit 2019: Cybersecurity and Public Interest Tech with Bruce Schneier", "article": "Bruce Schneier is an internationally renowned security technologist who has testified before Congress and served on several government committees. Schneier is a fellow at the at Harvard University, a Lecturer in Public Policy at the , and a special advisor to . At this year’s , he will give a keynote presentation on bridging the gap between technology and public policy, in the cybersecurity world and beyond. The theme of Summit 2019 is “Designing better government”—what does that look like to you? When I envision successful government in the information age, I see a government that is not afraid to try, fail, and iterate. I see this as a major cultural difference between policy makers and technologists. Technologists are not afraid to fail; failure is even a badge of honor, because it means that you tried and learned something. Technologists are not afraid to iterate; they know that it’s easier to get something right by version 3.0 if you can learn from the mistakes of versions 1.0 and 2.0. Policymakers don’t think like this. They want to get it right the first time. They know that if they get it wrong, their community is just as likely to abandon the idea entirely as to figure out how to improve it. This has to change. Whether it’s bringing the idea of agile development to legislation and regulation, or just bringing to government the idea that failure is a form of learning, policymakers need to think more like engineers. They need to see their work as solving problems, and not fighting for their preconceived solutions. How does designing better government relate to your work? I am a security technologist, and also a public interest technologist. I try to bridge the gap between technology and policy—in the world of cybersecurity and more broadly. I write about tech policy, and I teach cybersecurity to public policy students. In my way, I am helping to bring about an ecosystem where public interest technology is as viable a career path as public interest law is. What are the biggest challenges that our movement is facing today? Change is hard. Government has a set way of doing things, one that has evolved over generations. It works at speeds much slower than technology advances, and is suspicious of the “creative destruction” model that technology embraces. This isn’t necessarily a bad thing. Technology companies are quick to jettison—or at least ignore—many of the rules we have in place to protect minority voices and marginalized communities in favor of the majority and the powerful. Getting that right is a big challenge, and not one we can ignore in our rush to build the future. Bridging technology and public policy is hard. There aren’t enough people who want to do this work, and there aren’t enough jobs for the few who want to. The money isn’t there. The desire isn’t there. But this is how we are we are going to solve the major public-policy problems of the information age. In the second half of last century, the fundamental question of society was this: “How much of our lives should be governed by the state, and how much by the market?” Answering that question dominated both foreign and domestic policy, and is why institutions like the Harvard Kennedy School (where I teach) is filled with economists. In the first half of this century, the fundamental question of society is this: “How much of our lives should be governed by technology, and under what rules?” Answering this question will require public interest technologists. What are you excited for about Summit this year? I am excited to talk to a group of technologists working in the public interest. There aren’t a lot of us—despite how large this Summit is—and we come from a diversity of backgrounds. I want to meet the people who make up the Code for America community and learn how they are bridging the divide between technology and public policy. By the way, if you want to learn more about public interest technology, I maintain a resources page at . There you can read my writings on the subject, watch the videos from a one-day mini-track I hosted on the topic at the RSA Conference this year, read the reports that Freedman Consulting wrote for the Ford Foundation, look at all the university programs in this area, and see a list of all the NGOs that are doing public-interest tech. If you have any additions, please send them to me at . I’ll see you in Oakland!"},
{"title": "13 Security Myths You'll Hear—But Should You Believe?", "article": "Security Myth No. 1: “More Security is Always Better.” Bruce Schneier, security expert and author of several books, including his most recent, , explains why this security concept of “you can’t get enough” that’s often bandied about is off the mark to him. Schneier explains: “More security isn’t necessarily better. First security is always a trade-off, and sometimes additional security costs more than it’s worth. For example, it’s not worth spending $100,000 to protect a donut. Yes, the donut would be more secure, but it would make more sense to simply risk the donut.” He also notes that “additional security is subject to diminishing returns. That is, measures that reduce a particular crime—say, shoplifting—by 25% cost some amount of money; but additional measures to reduce it another 25% cost much more. There will always be a point where more security isn’t worth it. And as a corollary, absolute security is not achievable.” Sometimes security may even become a moral choice and being in compliance might be an immoral decision, as it could pertain to a totalitarian system, for example. “Security enforces compliance, and sometimes complying isn’t the right thing to do.”"},
{"title": "2006 Dr. Dobb's Journal Excellence in Programming Award", "article": "The Excellence in Programming Award is an annual award that acknowledges individuals who, in the spirit of innovation and cooperation, have made significant contributions to the advancement of software development. Past recipients include leaders and thinkers in the development community such as Linus Torvalds, James Gosling, Erich Gamma, Guido van Rossum, Jon Bentley, Anders Hejlsberg, P.J. Plauger, and Guy Steele Jr., among others. This year’s recipient—Bruce Schneier—is unique in that he has long been a member of the s family, so to speak. But while Bruce has been a contributing editor, columnist, and writer for , that’s not why he is receiving the award. Bruce is this year’s award recipient because of the many important contributions he has made in his chosen specialty of computer security. These include designing the Blowfish and Twofish encryption algorithms, both of which he wrote about in and the latter of which was a finalist for the Federal Advanced Encryption Standard. Bruce is also the author of eight books, including , which is a seminal work for software developers. Bruce is the founder of and chief technical officer for Counterpane Internet Security. He has served on the board of directors of the International Association for Cryptologic Research, and is an Advisory Board member for the Electronic Privacy Information Center. Bruce holds a Bachelor’s degree in physics from the University of Rochester, and a Master’s degree in computer science from American University. In short, by never wavering from focus on technical excellence and open communication, Bruce Schneier represents the finest the software development community has to offer."},
{"title": "Twofish Heads to Washington", "article": "A team led by author Bruce Schneier has invented a new block encryption algorithm and submitted it for consideration as the next new federal government standard for data scrambling. , the sequel to Schneier’s 5-year-old Blowfish block cypher, was submitted last week to the National Institute of Standards and Technology (NIST) for consideration as the Advanced Encryption Standard. Twofish is designed to be flexible with respect to the necessary performance tradeoffs between the creation of a “secret key” and execution of the actual encryption. As such, it is well suited to large microprocessors, smart cards, and dedicated hardware. “We designed Twofish with performance in mind,” said Schneier, president of the security consulting firm Counterpane Systems. “You can get faster encryption with more setup time, or you could do no setup and get slower encryption,” he said. “You can trade off efficiencies between hardware, software, RAM vs ROM, encryption speed … all of these are interoperable,” Schneier said. The free, publicly available algorithm is suitable for 128-, 192- or 256-bit key lengths. The Advanced Encryption Standard is designed to replace the current government specification, known as the Data Encryption Standard, which was first introduced in 1977. In submitting Twofish, Schneier’s group joins about a dozen others aiming for the NIST standard, including IBM, RSA Data Security and Cylink."},
{"title": "Everything about IT Security Will Change", "article": "Bruce Schneier, leading cryptologist described as a “security guru” and a “leading counterterrorism contrarian” by the media, shares his thoughts about the future of information security. “Crime, Crime, Crime!” Bruce Schneier is adamant when asked to talk about the worst security threats. It’s not coming from fanatics, but from people out to steal for money, he insists. “It doesn’t matter what form it takes,” he says. “It’s wrong that we defend ourselves against the tactics, because then these guys change tactics.” He describes a worst scenario where “the crime is so bad that people stop doing commerce on the net.” Information security is there to prevent this from happening. As a leading cryptologist, Schneier is the CTO of BT Counterpane, a security service firm and author of . He believes the security industry will undergo a transformation: “In a vibrant security market, security research and security companies no longer sell to consumers.” He believes that end users will soon expect that services they use over networks, such as online banking, will come with a guarantee of security from the service provider. “The hardest thing about working in IT security is convincing users to buy our technologies,” Schneier says. “An enormous amount of energy has been focused on this problem—risk analyses, ROI models, audits—yet critical technologies still remain uninstalled and important networks remain insecure.” Schneier is constantly asked how to solve this by frustrated security vendors but he says he has no good answer. “But I know the problem is temporary,” he shares. “In the long run, the information security industry as we know it will disappear.” Security as Utility Schneier thinks the entire IT security industry is an artifact of how the computer industry developed. “Computers are hard to use, and you need an IT department staffed with experts to make it work.” Whereas for other mature high-tech products, such as those for power and lighting, heating and air conditioning, automobiles and airplanes, the job of installation and maintenance is outsourced as a service. “No company has an automotive-technology department, filled with car geeks to install the latest engine mods and help users recover from the inevitable crashes,” he comments. According to Schneier, IT is heading in that direction of becoming a utility where users are buying more services than products. “By their nature, services are more about results than technologies,” he says. “Service customers—from home users to multinational corporations—care less about the technological specifics and just expect IT to work.” Counterpane, the internet security company that Schneier formed eight years ago on the premise that “large IT departments don’t really want to deal with network security”, was acquired by BT last year to have their network security services embedded in the service portfolio. “Many customers don’t want to deal with network management at all; they just want the network to work,” he says. “They want the Internet to be like a phone network, a power grid, or a water system—in short, they want it to be a utility.” Schneier goes on to explain that for these customers, security isn’t even something they purchase, but a small part of a larger IT services deal. That’s why IBM has bought ISS and EMC has purchased RSA -to create a more integrated solution for customers. “Someone is going to buy Symantec,” Schneier says firmly. “And someone is going to buy Network Associates.” Bruce Schneier Schneier uses email as an example as some corporations have outsourced their corporate email to companies like Google. “If you have a new email security solution, convincing Google to embed it in its email service is far more efficient than trying to sell it to users.” He believes when the IT industry matures, there’ll be no point in user conferences like InfoSec and RSA. “They won’t disappear; they’ll simply become industry conferences,” he says. “If you want to measure progress, look at the demographics of these conferences. A shift toward infrastructure-geared attendees is a measure of success.” Meanwhile, security products won’t disappear. “There will still be firewalls, antivirus software, and all sorts of new technologies and products,” notes Schneier. “But users won’t care about them.” Instead, the new technologies will be embedded within the services sold by large IT outsourcing companies or ISPs—”just like new automotive technologies are marketed to automobile manufacturers, rather than individual car owners.” Schneier believes this is progress. “As IT fades into the background and becomes just another utility, users will simply expect it to work,” he notes. “The details of how it works won’t matter.” Security will become a commodity, “just like an airbag in a car.” Schneier explains that in the US companies advertise their airbags as a differentiator. “Security will come out as a critical differentiator. You buy a product because it comes more secure,” he says. “It won’t be a separate thing, but it’s there to make a difference.” Schneier goes on to explain that when customers buy a service, they don’t need to tick a check box on whether they want security—security comes as part of what the service provider is offering. “Nobody wants to buy a house because of a door lock,” he says. “But you will never buy a house that didn’t come with a door lock.” Schneier also believes that security as utility solves the problemoffinger-pointingwhich is now happening around a lot of security issues. “If a company provides both, it doesn’t matter whose fault it is but they need to make it good.” Security Signals Schneier explains that for end users, because they are not experts in security, they will rely on the “signals”—such as consultants, certification and analyses—to help them make decisions. “It’s dangerous for customers to rely too much on this signal, but there are no better options,” he explains. “Your option is either learn it yourself or buy after the signals.” Schneier uses the analogy of an ill person seeing a doctor—”what are your other options? Learn medicine?” he explains. “You could, but it’s not realistic.” “I have no clue about pharmaceuticals, so I have to trust a doctor.” Schneier also believes vendors should be held responsible for security failure because “there is no other mature industry on the planet where this is not true.” He uses the electricity industry as an example to support his viewpoint and points out there are many liability models organisations can work on. Security as Part of Risk Management Schneier says the security people in the company won’t be obsolete: “they may have different bosses, but they are not going to go away.” While technical people will more likely be moved to some of the outsourcing companies, he also affirms that CIOs will not disappear from companies, because “you can never outsource the business risk management, you just outsource how it works.” Schneier also asserts that there is currently too much focus on technology. “Businesses are seeing security just as IT or information security,” he adds. “That’s changing—you see a lot of security go into general risk management.” Security, ultimately, is part of enterprise risk management and business continuity. He says though IT person is the best one to be in charge of IT security, someone above him has to approve his budget and that person will not be an IT person, “so that the IT person has to speak a language of the people above you.” However, the overall risk management stretches much more than merely IT. “You need to be secure against burglary, you need to consider risks of fire and hurricane,” Schneier says. “All those are basic business risks and information security should be seen as part of this larger landscape.” Bruce Schneier’s Top Ten Trends in Information Security It always has had value but we never realised before, or at least not in the same way we do now. For a lot of banks, their database has become their only sellable asset. The recent blackout in the United States saw lots of companies take long time to resume operations, not because of the power outage, but the data outage. Most legal statements are designed for owners of the property, but nowadays you don’t have the paper anymore, you have an outsourcing company. When your data resides on an outsourcer’s server, your own rules of security don’t apply anymore. Things get more complex, and less secure; the internet is the most complex machine humankind has ever built. There is an arms race. You wait a couple of years, computers get better, but security gets worse. When the security gets better, the system gets more complex, we lose the ground. Threats have evolved from hobby threats to more criminal threats: Identity theft, fraud due to impersonation, bot networks. There is a career path to become a hacker. The command and control of worms is so sophisticated that we don’t know how to take it down. The criminal worm is quiet, unlike hacker worms. Lots of embedded systems are now out in the market. Patches have to be released fast—that’s fundamentally impossible as on-the-spot patching can’t have both quality and speed. In 2004, Microsoft started once-a-month patching—they release patches on a schedule, which has proven to be effective. People are squandering too much effort on technologies such as SSL and PGP to protect the communications channel, while end points are now easier targets—hackers try to get decrypted information sitting on the harddisks. Secrecy is not security—something that is truly secure should be made public. For example, if the value of an encryption algorithm depends on its secrecy, all security will be lost once it’s no longer secret. Most security protects you from the bad guys; but now there are security roadblocks on your computer to prevent you from doing “bad” things. In 2005, Sony secretly installed a root key in music CDs to prevent users from copying the songs. The company viewed viewed users as potential attackers, and is an example of “I can either protect you or protect against you” thinking. The root kit had a side effect of opening the PC to malicious attack. Greed and fear are the two things based on which you can sell something; security has already been and is fundamentally a fear sale; regulation gives IT guys a stick to force their manager to give them budget; lots of money goes to regulations, but not security itself."},
{"title": "Going Meta: A Conversation and AMA with Bruce Schneier", "article": "In this episode, Perry Carpenter interviews cybersecurity guru Bruce Schneier. Perry and Bruce explore how cybersecurity is about so much more than technology—It’s about people, so we benefit by taking a multidisciplinary approach. In preparing for this interview, Perry solicited his LinkedIn network to see what questions people had for Bruce. This is a wide ranging conversation covering everything from Bruce’s thoughts on cybersecurity’s “first principles” to the impact that the pandemic had on society to need for regulation to help raise the overall standards for security and privacy. Transcript Hi, I’m Perry Carpenter, and you’re listening to 8th Layer Insights. This is a cybersecurity show about people and human nature and how we can’t afford to ignore human nature in the way that we designed security and in the way that we build our programs. Today’s episode is going to be a little bit different than previous episodes but, if I were to summarize the theme of today’s episode and use one word to describe it, that word would be synthesis. Now, if this was a nature documentary, I might open with some type of imagery depicting a chemical combination that has this visually stunning reaction, or I might show an animation of molecules combining to form a new molecular structure. But, this is a podcast and our virtual paintbrushes are sounds and ideas, and those introductory establishing shots can be really hard, especially when talking about an abstract topic like synthesis. So, let’s jump ahead and get straight to the point, because you’re probably asking yourself “what is all of this talk about synthesis?” and “what do we mean in this context?” Let me explain. Synthesis is really what 8th Layer Insights is all about. And by that, I mean the heart of this podcast, the intent behind it, is to bring ideas and concepts together from several different disciplines, covering a wide swath of topics, all coming together to shed light on the human condition as it relates to security and risk. In the couple of decades that I’ve been involved in cybersecurity, I’ve seen the industry mature quite a bit, but one of the stumbling blocks that we seem to face again and again and again, is that we tend to believe that somehow cybersecurity is different than everything else. It’s its own unique animal. It’s so unique, so new, that we have to figure things out ourselves. And that can have the effect of creating tunnel vision and echo chambers, where we don’t think to learn from the past or look at how other disciplines have grappled with similar problems, or even ask ourselves how our new fangled security controls will collide with human nature. Sometimes we focus on the individual trees so much, that we forget about the forest and the entire ecosystem that drives the forest and sustains the forest. And that reminds me of a story. You may have heard it before but, even if that’s true, go ahead and listen again with fresh ears. This is the story of the blind man and the elephant. The blind man and an elephant. The earliest versions of the parable of blind men and elephant is found in Buddhist, Hindu and Jain texts, as they discuss the limits of perception and the importance of complete context. The parable has several Indian variations, but basically goes like this. A group of blind men heard that a strange animal, called an elephant, had been brought to the town, but none of them were aware of its shape and form. Out of curiosity, they said, “we must inspect and know it by touch.” So, they started out and found it. The first person whose hand landed on the trunk said, “this being is like a thick snake.” For another one whose hand reached its ear, it seemed like a kind of fan. As for another person, whose hand was upon its leg, said “the elephant is a pillar, like a tree trunk.” The blind man, who placed his hand upon its side, said “the elephant is a wall.” Another, who felt its tail, described it as a rope. The last felt its tusk, stating “the elephant is that which is hard, smooth, and like a spear.” There you have it, sometimes we don’t perceive things correctly, because we are too close. We lack context and that lack of context means that we make mistakes. Back to you, Perry. So, we make mistakes when we lack context, and that’s where big picture thinking becomes so critical, and big picture thinking and synthesis and meta analysis is what today’s guest is known for. If you’re a cybersecurity professional, it’s very likely that you’ve heard the name Bruce Schneier before, and it’s hard to underestimate Bruce’s impact in the field of cybersecurity. This is so much so that most intros for him end up defaulting to words like “guru” and “luminary”. He made a big splash in the computer security world as a cryptographer when he published Applied Cryptography back in 1991, but most people don’t know that Bruce’s earliest training was actually as a physicist. He earned a Bachelors in physics in 1984 before moving on to study computer science for his Master’s degree, which he earned in 1998. Bruce created a few popular cryptographic cyphers, which you’ve probably heard of: Blowfish and Twofish and Threefish among others. In 1999, he invented a cryptographic algorithm called solitaire, which is designed to manually encrypt data using a deck of cards, and this algorithm was a key plot point in Neal Stephenson’s book Cryptonomicon. Bruce even wrote an afterword to that book describing the cypher. Here’s where the big picture thinking comes in. Despite all of Bruce’s success on the cryptography side of things, he realized something. He realized that cryptography and technology alone will never solve security issues. They face the same issues that the blind men faced when trying to describe the elephant. Over the past couple of decades and change, Bruce has been focusing on security at more of a macro level. He’s been taking the 30,000 ft and above level, that big picture view and he’ll be the first to tell you that we need to approach security in a multidisciplinary manner. Security is inherently about people. It means a lot of technology, but security has to take people into account, and it’s often economic psychology, sociology non-security topics that explain how security works and fails. Security theater is a phrase I invented post-911 to describe security measures that look good but don’t accomplish anything. This is regulation, it gets a bad name, but actually it keeps us alive. There’s a lot of technologies we have for authentication that aren’t being used because the market doesn’t reward it. All men are not angels, all people are not angels. Security is a tax on the honest, and something we have to pay for, even though we get nothing for it, but we get everything for it. I’m Bruce Schneier, I work at the intersection of security technology and people. We’ll be touching on several topics with Bruce today but, first, let’s cue the intro. Hi there, my name is Perry Carpenter. Join me for a deep dive into what cybersecurity professionals refer to as the 8th Layer of security: humans. This podcast is a multidisciplinary exploration into the complexities of human nature and how those complexities impact everything from why we think the things that we think, to why we do the things that we do. And how we can all make better decisions every day. Welcome to 8th Layer Insights. I’m your host, Perry Carpenter. We’ll be right back after this message. So, what’s a con game? It’s a fraud that works by getting the victim to misplace their confidence in the con artist. In the world of security, we call confidence tricks social engineering. And as our sponsors at KnowBe4 can tell you, human error is how most organizations get compromised. What are some of the ways organizations are victimized by social engineering? We’ll find out later in the show. Welcome back. I mentioned in the opening section that today’s episode is a little bit different than most of the other shows and that’s because today we have one guest, and that one guest is driving the agenda. Back before I officially launched the podcast, I solicited LinkedIn with a request for topics and potential guests, and Bruce Schneier’s name came up. Thankfully, he agreed to be interviewed, and because that suggestion came to me from LinkedIn, I reached back out and I asked people what kind of questions they would like to have Bruce answer. This is a little bit of a guided AMA session, an ask me anything session with Bruce. And I think this comes at a great time, because most of us have not been going to conferences this year. and so many of us that have become habitualized to hearing Bruce speak from the stage every year, have not had that. As a result, you might be feeling like his voice has been missing from some of the conversations, and I think that’s what makes today’s episode special. What I’ll be doing is I’ll be playing clips from the interview and I’ll be giving some context or some commentary before or after some of these questions, but these are largely Bruce’s words unfiltered and Bruce’s thoughts unfiltered, being brought directly to you. Thank you so much to those of you who submitted questions for Bruce. I hope that you enjoy this interview as much as I did. With that, let’s go to the interview. I’m Bruce Schneier. I am a security technologist. I do a lot of things. I teach at the Harvard Kennedy school. I teach security to public policy students. I have a company Inrupt, that is trying to commercialize Tim Berners-Lee’s Solid, distributed data ownership technology. I write, I speak, I have a website. I’m a security technologist. I work at the intersection of security technology and people. Fantastic. You know, Bruce, one of the things that I think people really appreciate about you is that you’re extremely multi-faceted in the way that you think about security and in the way that you opine on security. I want to go philosophical for just a couple of minutes. In philosophy, a first principle is a basic proposition or assumption that cannot be deduced from any other basic proposition or assumption. For you, what would be a couple of first principles for security? Security is inherently about people. It means a lot of technology, but security has to take people into account, and it’s often economic psychology, sociology, non-security topics that explain how security works and fails. Security is a way of incenting people to behave in a certain manner, and we need to understand. That might be my first principle, that security actually is rarely about security, it’s about other things. Okay, if you don’t mind, let’s drill into that for a minute. You’ve got a pretty famous quote. I think it was in the preface for Secrets and Lies, where you said “if you think technology can solve your security problems, then you don’t understand the problems and you don’t understand technology.” But then a bit later in a 2013 blog post, you also mention that at the time you felt like training users was a little bit of a waste of time and that the money could be better spent elsewhere. What is the through line between those two opinions? One being that technology can’t solve the problem, so we need to focus on humanity; and the other being that training isn’t necessarily the answer to everything either. I’m going to pull that apart and give you two separate answers you can use. That’s not even my quote. I believe that’s Roger Needham at Cambridge University who first said that. It is something that we in security say a lot, and I think I popularized it, but I won’t take credit for it. Okay, quick note, after that reminder from Bruce, I went and looked at the preface to Secrets and Lies again and here is how he phrases it. He says “a few years ago, I heard a quotation and I’m going to modify it here.” Then he goes on to say “if you think technology can solve your security problems, then you don’t understand the problems and you don’t understand the technology.” What I did after that is I went on a search for the original quote and, of course, I used some of those key words and I used Roger Needham’s name and I eventually found it. It looks like this is attributed to both Roger Needham and then also, in a lot of cases, to Peter G. Newman. The original quote is “if you think cryptography will solve your problem, you either don’t understand cryptography or you don’t understand your problem.” Okay, back to Bruce for the second part of his answer. Let me do the second now. Our society is so complex that we’ve built it in a way that you don’t have to be an expert in things to take advantage of them. I can get on an airplane – wow remember airplanes – without knowing anything about aircraft, safety, or maintenance or pilot training, or any of that. Right, there’s an entire infrastructure that I can safely ignore and know that I’m walking onto a plane and it’s not going to crash. The same thing with cars, pharmaceuticals, restaurants. Society has my back with a lot of expertise so I don’t need it, and computers, networks need to be the same. It can’t be that everyone has to be an information security expert to use a computer securely. Just, it can’t be that everyone needs to be a food hygiene sanitation expert in order to eat at a restaurant and buy at a grocery store. When I think about user training, that’s what I think about. Why are we training the user? Why don’t we train the experts and have them decide what’s right, like we do everywhere else. This is regulation, it gets a bad name, but actually it keeps us alive. A lot of user education covers for bad security design. You think about the sort of advice we try to give people. “Don’t plug strange USB sticks into computers.” It’s a USB stick. What are you supposed to do with it? “Don’t click on URLs you don’t know.” It’s a URL, you’re supposed to click on it. “Don’t open attachments.” It’s an attachment. In every one of those cases, there’s a pretty serious design problem, and why is it that a malicious URL can hack a computer? Or an attachment? Or a USB stick? Those are design issues. Those are failures in how the computers were built, and blaming the user for not doing the obvious thing, doesn’t make any sense. So, I don’t like user education, because it goes hand in hand with use blaming, and because it implies a level of expertise that we shouldn’t expect. Yes, that’s an insightful perspective because it recognizes that the user behavior problem is actually a technology failure. It’s a failure of design, but unless and until those fundamental design issues are addressed, the user ends up bearing the brunt of dealing with the issue. It does seem like every year some vendor says that they’ve solved this issue and user behavior won’t be something that we have to think about anymore. But, to date none of those vendor promises or technology problems stand up to the ultimate test of reality, where persistent attackers continue to prevail by manipulating the technologies in unexpected ways, or by breaking the tech outright, or by bypassing the technology through social engineering. Give some of your thoughts on how we get to secure behavior by design, or how we deal with these technology based holes that for decades haven’t yet been plugged? We in society deal with these things through government. We’ve lived through decades where food wasn’t safe, where cars and airplanes crashed all the time, and we changed that through government regulation. When you stare at a security hole or vulnerability, it hasn’t been fixed in 40 years, you have to realize that the system incensed it not to be fixed. We’ve designed a market where not fixing it is more profitable, so it’s never going to get fixed. If we want more security, we need to require it and we need to require it collectively as citizens, that means government, and then we need to enforce those regulations. I mean this shouldn’t be a surprise. It’s what we do everywhere else, but somehow in the computer world, we have this weird belief that the market will solve problems even if the market doesn’t reward those solutions. Yes, I think when it comes down to that, there’s still some complex issues that we’ve been grappling with for a long time. I mean, one of those main things that we grapple with is authentication. I mean, why haven’t we solved authentication yet? I’m not sure what solve authentication yet means? So, I’ll ask you what does solve authentication mean? My bank works pretty well, it feels solved. I go home for Thanksgiving, I know everybody. That all seems solved. What do you mean by solve authentication? Yes, to where people can’t easily socially engineer passwords or reroute two-factor authentication tokens or something like that, to where if there’s a data breach, or if I know something about somebody that I can’t trick that person into giving up keys to the kingdom. So, you’re saying why haven’t we made humans smarter? We’ve been trying to do that for centuries. That’s hard. My guess is just genetic engineering isn’t good enough yet? I mean I think that comes down to part of the problem though. If there’s a place where training can’t fix it because we’re dealing with very complex systems and the user can’t be expected to do that. Then, how do we get to a situation where the technology is secure by design in the very basic ways that we just interact with our systems? There’s a lot there. One, again, my bank works pretty well. So, I think we have it. Where you don’t have it, it’s because it is more profitable for the companies to have lousy security than to have good security. The reason your phone is so easy to hijack, sim swapping, all those tricks where you call up the phone company and pretend to be somebody else and then hijack their phone. The phone company likes it that way. They make more money, because the authentication is bad. Banks make less money, that’s why bank authentication is better. It really comes down to economics. Facebook makes a lot more money spying on people, having misinformation, than they do fixing any of those problems. We have a lot of authentication systems, and some work better, some work worse, and companies will use the one that is most profitable for them, not the one that is in the user’s best interest. You want to fix that, pass a law. Some of this is inherent. Remote authentication will always be harder than in person. If you and I meet in person, it will be harder for someone else to impersonate you, than it will be if we meet on the telephone, or we meet, I don’t know, in a text app. That’s always going to be true. There’s a lot of technologies we have for authentication that aren’t being used, because the market doesn’t reward it. Then, it comes back to basic economic incentives and the fact that there’s not necessarily the regulation in place for some of the advances that we need to make all of this more ubiquitous. From your perspective, we’ve been in various forms of lockdown around the world for the past year. what have we learned about ourselves and security during this time? Has anything stood out to you? I think we’ve learned that a lot of things are possible remotely that we never imagined or never allowed; remote learning, remote medicine. And that we’re not going to go back to before, we’re going to go back to some hybrid. We’ve learned that organizations can thrive, even without people sitting in an office together, for prescribed hours, and we’re not going back there. And we’ve learned that our network infrastructure is incredibly important to society and that’s not going back. What it means for security is that more things are going to happen remotely, are going to happen without the same social lubricant that gives us so much security in these settings, and that’s going to be more dangerous, from personal corporate and national security. We do need to start facing these challenges. We’re pretty good at muddling through and making things sort of work. We might have to up our game, because looking around the threat actors certainly have upped theirs. Can you go a bit deeper on that? How are you seeing threat actors up their game and take advantage of this more disconnected form of society that we’re in right now? Where does remote work and everything else fit in with that? Supply chain is the new attack factor. We are seeing the Russians, the Chinese – I’m sure the US are doing the same thing – criminals, all going after the supply chain in different ways. Whether it’s code libraries that end up into a piece of software; update mechanisms; distribution mechanisms. It’s all being attacked and sometimes the attacks are very fruitful. The Russians’ solar winds campaign was really impressive. The Chinese attack against Microsoft exchange, was very successful. We are seeing different actors go after the supply chain, so it’s less disconnected and more interconnected. Things are massively interconnected, massively global in a way that makes us more vulnerable. I don’t know whether the pandemic fits into this or if it was just by coincidence. The pandemic made us more vulnerable by changing everything so fast. Criminals especially took advantage of the chaos and confusion in those first few months after the shutdowns mid-March 2020. To take advantage of people’s not knowing what’s going on or how to do things, to sort of trick them into doing things they might not want to do. I think we’re better about that now, but we still are accessing our networks via VPNs, our data is all across the internet at various cloud providers. That brings with it new vulnerabilities, and we’re slowly dealing with those. Fantastic. Let’s go ahead and transition to some of the questions that came in over LinkedIn. I’m going to give you a bunch of rapid fire questions. If you could just give some of your first thoughts that come to mind. The first one is, if you could put one thing on a billboard for everyone to see related to security, what would that be? Oh wow, what an odd question. It’s so funny, my first thought is security isn’t free. Because it’s not, you have to pay for security, and we all expect it to be free. The more esoteric way of saying that is security is a tax on the honest. Security is something that the honest have to pay for in order for systems to work. It was, I think, Thomas Jefferson who said this, “if all men were angels, then no government would be necessary”, and if all men were angels, no security would be necessary. But all men are not angels, all people are not angels and security is a tax on the honest and something we have to pay for, even though we get nothing for it, but we get everything for it. Alright, it’s a long billboard and pretty philosophical. Yeah, long billboard, but it speaks of the fracturing of the social contract. So, I think that that’s super, super important to recognize. Alright, if you could change three things about our industry today, what would they be? I think what’s missing in our industry is government regulation. Right now the security industry is designed with profit in mind. That works well as far as it goes, but there are some enormous market failures that lead to bad outcomes because people are making individual decisions and no one’s making collective societal decisions. What has been missing for decades in cybersecurity, is government, and I think we need to fix that, and that is the first thing we need to fix. Let me think about number two. I think we need more understanding on human psychology, that security is inherently about people. Too much security has been tech focused and not people focused, and that really means putting social scientists in security development teams. That’s psychologists, sociologists, anthropologists, polisci people who know how people work, and I think that will help us build a lot better security systems. The third is, I think we need way more diversity in our industry. That we fall into a lot of bad rabbit holes because this has been the one voice in thinking and designing security. If you watch AI, I’ve seen a new area of security spring up, which showed the problems of a mono culture. But the diversity and voices that are now talking about AI security, really show the promise of ways of thinking and solutions, and I’d like to clone that for the rest of cybersecurity. Okay, so this is a divergence from the LinkedIn questions, but since you’ve mentioned regulation several times now, let me ask you this. You believe that regulation is a chunk of the answer, but we’ve seen regulators try to get involved with things like encryption, from let’s say an overly US centric perspective, where they want to essentially break security by adding things like back doors to preserve what they believe, or what the regulators or the intelligence community believe is going to be within the national interest or what is best for them. How do we deal with that when it comes to regulation? Well, this is a problem, right. Regulation should be for security and safety, not for insecurity. The problem with the encryption debate is it’s not regulation for defense, it’s regulation for attack, for offense. Yes, if the department of justice is in charge, if the NSA is in charge, you’re going to get lousy security because everyone likes to attack and no one likes to defend. I see the internet as part of our critical infrastructure, that protecting it is paramount. That is a challenge we have, that we really don’t have any agency that is wholly on the side of defense. I mean in the US, I suppose, we have CISA, but they are still really finding their way, and you do have the offensive nature of cyber much more in the ascended. I’m just finished on the Nicole Perlroth’s new book, a great book on the offensive cyberspace, with the awesome title of “This is how they tell me the world ends.” Her titles are better than mine, which I write great titles, but this is a fantastic title. And it’s something that internationally we need to figure out. You’re right that kind of US focused, offense focused, anti security regulation is not going to get us anywhere. Back to questions from LinkedIn. You’ve been outspoken especially post-911, about security theater. Do you have any examples of security theater that you’re seeing today? If you want to expand on that, can you give some ideas on where those might be helpful psychologically, harmful or benign? Security theater is a phrase I invented post-911 to describe security measures that look good but don’t accomplish anything. Random ID checks in buildings, National Guard troops in airports holding guns with no bullets, a lot of examples of that; making people feel better, even though they don’t actually do anything. Now, that’s not necessarily bad. We want people to have a feeling of security that is line with the reality. Post-911, everyone was scared. The risk didn’t increase that much actually, so a little theater went a long way. In the past year, we’ve really seen a lot of the same thing, I’ve heard it called hygiene theater. It’s the scrubbing down surfaces for Covid did absolutely nothing and it made people feel better. I know people who would wipe down their groceries when they brought them home or leave them outside for three days. You know, complete nonsense, made no sense, given the science even back then, and now we know it was a waste of time. But that was a piece of theater. Even today, some of the measures people take outdoors are security theater. We know how the virus spreads, it spreads through the air, it spreads indoors, it spreads with not good airflow. I’m not worried about people at the beach; I’m worried about people in the crowded bars in the evening. We saw a lot of that kind of theater in the last year with Covid, and it was interesting to watch what worked, what didn’t, what people did anyway. Some of it is social signaling. I think, right now, we wear masks even though we’re vaccinated to signal to others who don’t know if we’re vaccinated. I walk into a restaurant, of course I’m going to wear a mask, even if I’m vaccinated because we’re indoors and we don’t know who else is, and those restaurant workers are really at great risk. So, there’s a little theater there and there’s a place where theater is valuable. You talked a little bit about some of the hygiene theater that you saw and potentially some things that may even erode trust in authorities. How do we deal with these fractures in the view of authority in society when you’ve got hundreds of millions of people probably that may not trust the experts as much as they used to, when, in the long run, it’s probably better to trust the experts? How do we start to get some of that trust back? That’s an area that I really don’t understand. It’s so far from my field, it’s not security, it’s really psychology. But you’re right, we’re now living in a world where science is disputed. I think of it as the counter enlightenment, that there is a group of people, pretty skewed along political lines, that don’t trust science, that don’t trust math, that don’t trust experts, that have their own answers and damned anybody that contradicts them, even if they have facts. It’s something we as a society have to deal with, and I don’t know how. It’s exacerbated by the press, by social media, by a world where everyone can be a publisher. Right, there’s value in that, but there’s risk in that. Yeah. Do you have any thoughts then on the rampant spread of disinformation? Is that accelerating the way that I feel like it might be? Or is it more the same that it’s always been, but we are just more aware of it now? It’s hard to measure. We know that more disinformation is spreading, but we don’t know the effects of it. We have a lot of data, but no real good analysis on what does what. Take this very broadly, you go back a bunch of 100 years, being a publisher was hard, and being a recipient of publications was hard. You had a world of information that didn’t really flow at all. You move into the world of the printing press and increase literacy and you had broadcast, like publishing was hard, but being the recipient was easier. That moved into radio and then television and, again, one to many was the way it all worked. Now we’re going half backwards; it’s very easy to publish, and it’s very easy to receive information. Now we’re in this world where everybody is speaking. We had history nobody speaking, and nobody listening; to 100 years ago, a few people speaking, everyone else listening; today everyone speaking, everyone listening. This is new and I think we don’t fully understand the ramifications of the world we’re in. This is something that people way smarter than me are studying and it’s very outside my area, but I think I gave you my philosophy on it. Philosophy welcomed. This is a meta type of interview. I’d love to get your thoughts on how technology might be increasing polarization or some of the social issues that we are seeing today. It’s pretty clear that technology is increasing polarization just by allowing more discrimination. By that word, I mean the ability for people to segment themselves and to be segmented. That you can live a lot of your life now and not come into contact with an idea you disagree with, except in a divisive way. That is something that just wasn’t true before the modern internet technologies so that is affecting things. We’re not sure how but it obviously is. What about the intentional or unintentional algorithmic encouragement of that? What about it? Do you have any thoughts on whether that should be something that’s addressed because it does seem like social media is intentionally or unintentionally encouraging that polarization based on social media’s engagement models. This gets back to regulation. It seems odd that we would organize our societal political discourse around the short-term financial benefit of a handful of tech billionaires. That seems a really weird way to organize our political system and to organize politics. Yes, I would like to see government regulation here, because the full profit model of political speech isn’t serving our country very well. We’ll be right back after the break. And now we return to our sponsor’s question about forms of social engineering. KnowBe4 will tell you that where there’s human contact, there can be con games. It’s important to build the kind of security culture in which your employees are enabled to make smart security decisions. To do that, they need new school security awareness training. See how your security culture stacks up against Knowbe4’s free phishing test. Get it at knowbe4.com/phishingtest. That’s knowbe4.com/phishingtest. Welcome back to our discussion with Bruce Schneier. Okay, so back in your wheelhouse with cryptography for a minute, we had multiple questions on LinkedIn from people asking about quantum computing and when quantum computing will begin to threaten existing commercial encryption. Then, of course, when our currently trusted encryption models, because of that, won’t be able to be trusted anymore. Quantum computing is a new way of doing computing and it does threaten some cryptography, but not all of it. I wrote an essay on this, and I urge everyone to go find it. The title is “Cryptography after the aliens land.” Just type that into your favorite search engine and my name and it’ll pop up. If you want to check out that essay, go ahead and check our show notes. I’ve included a link to the essay there. I go through the various promises and perils of quantum computing and what it means. The long and short of it is that we’ll be okay, that it will break some encryption, but not all of it, that we are already working on post quantum algorithms and we don’t actually know if it’ll work at all. We know quantum computing is hard, but we don’t know if it is put a person on the moon hard, or put a person on the sun hard. And I mean that really truly. If it’s put a person on the moon hard, we should have breakthroughs over the coming years, in a decade or so we’ll have working quantum computers actually solving problems. If it’s put a person on the sun hard, it’s going to be centuries and we’re not going to solve it. It’s a very speculative technology right now. My money is on put a person on the moon hard, if we’re going to have a betting pool, but cryptography is going to be okay. We have a lot of things we can do that are quantum resistant, even theoretically, and we’ll be fine. Great, thanks. In this last section I want to be a little bit reflective and speculative. Over the past few decades as you look back, do you see any critical tipping point moments with respect to security that stand out to you? Nothing comes to mind. I’m not convinced had I thought about it a lot more, I wouldn’t come up with any, but I can’t think of any huge moments. Certainly the terrorist attacks 911 were a moment where a lot of things changed. I’m not sure it changed in cryptography or internet security in the same way. We might want to talk about some of the famous worms or malware. But, again, all of this feels like trends to me, and I don’t think of any flash bulb moments where everything shifted. Maybe that itself is interesting, that things haven’t shifted, that it’s been the same stuff we’ve been dealing with for decades. Yeah, so as you consider this and think about it, there could potentially be some small inflection points like this year where digital transformation had to speed up a bit and people embrace new technology, but it’s more of a continuation rather than a complete break or shift. Then, I guess, let’s pivot to the future. As we look to the future, what are you most excited about or worried about? I mean to me the future is the continuation of the present. I worry about the threat actors, I worry about nation states, I worry about criminals. I worry about all of the legal security threats that come from companies following the law and doing things that are bad for our security: the Googles, the Facebooks, surveillance capitalism. I worry about the Internet of Things, and the computerization and networkization of everything and how that will change the threat landscape. I think there is promise in governments getting involved. Not the US, we’re way too dysfunctional, but the EU is the regulatory superpower on the planet, and they pass comprehensive privacy laws and they’re looking at Internet of Things, looking at vulnerability disclosure, looking at AI security. They are large enough that they will move the planet in ways that are good so that’s really where I’m looking. Alright, so these last few questions that came in from LinkedIn are really just about you. You’ve been at this for a long time now. How do you maintain such a prolific level of output and where does the passion come from to sustain that? For me, writing is understanding and also writing is a way I can channel my energies. Often I write about a topic because I want to figure it out and the act of writing is how I figure it out. Putting my thoughts in a coherent essay form or book form, helps me explore the issue. If something happens that pisses me off, the way for me to calm down is to write something. Writing is not hard, writing is easy and it actually helps me know what’s going on and I want to affect the debate. It’s how I can talk about the issues in ways that people understand and maybe I can move a political needle. Do you ever deal with something like imposter syndrome as you’re a cyber security expert speaking into all these other areas that have to do with human dynamics and psychology, sociology and economics? Do you ever feel like you’re speaking into areas where you really don’t deserve a platform? Or do people accuse you of speaking into areas where you don’t have a platform? I try to stay in my lane. The questions you asked me about misinformation is a good example. I could pontificate about it, but I know there are people other than me who are really researching this and it’s enough outside my area that I don’t want to opine. In a lot of ways, I’m a synthesist; I’m a meta person and I work in the ideas of security that apply in all domains. I actually have opinions on, I don’t know, stadium security, or security at rock concerts, based on some of the systemic things I know about security, even though I’m not a domain expert, even though I know nothing about how stadiums work. I try to maintain an interest and a humility and know where I am speaking and what I can and can’t speak about, and then be honest about it. I can tell you what I think and then couch that with saying “and there are people who are better than me, so if they contradict me, believe them and not me.” I think that’s all part of maintaining what you know. In our world generalists have it hard, because specialists almost always trump them, even though I think generalists have important things to say. Fantastic. Last question is probably the easiest and simultaneously most important question. Is there any question that you wish that I had asked that I didn’t think to ask? Or is there a last word or thought that you want people to be thinking about and having discussions about? In the past couple of years, I’ve started thinking about the role of technology and public policy, and the importance of what we’re coming to call public interest technologists; people who bridge the gap between technology and public policy. I am one of those people. I know many of those people, but there aren’t nearly enough of them. We need a career path in public interest tech. We need the ability for lots of people to move into the space that requires expertise in both camps, because all of the important societal political problems of this century are fundamentally technological, like climate change, future of work, robotics, medical. We have a lot of bad policy coming from people who don’t understand the tech, and a lot of bad tech from people who don’t understand the policy. We need a cadre of people who understand both, and that’s what I’m trying to advocate for. It’s been on hold during the pandemic, because everything’s been on hold, but it’s something I feel passionate about and what to get back to. Well, that was Bruce’s last question and last response and that brings us to the end of today’s show. Bruce’s perspective is always fascinating. As he mentioned, it’s important to recognize that security is about so much more than just technology. It’s about people and taking a meta level approach and seeking a synthesis of ideas across multiple disciplines, is ultimately how we can evolve security and reduce risk. Bruce mentioned fields like psychology and sociology and economics and political science as being critical to improving our security posture. He also didn’t shy away from mentioning the need for regulation as a critical step. His analogy to how regulation elevated the safety and reliability in other industries like the automotive industry, pharmaceutical industry, food industry and finance, is compelling. The reason that regulation works in these circumstances is because most organizations within a specific vertical, don’t want to bear the brunt of being the first to do something and the only to do something. At that point, they have to pass on the cost and the friction to their customer, and they’ll be the only one that does it. In those circumstances the others that aren’t doing it may get a benefit, because people don’t know why they’re doing the new thing that they’re having to do, why they have these new requirements, and if they can go somewhere else, then they may. Regulation comes in and it deals with that by leveling the playing field, and it puts the economic piece in its place, and also ensures that all organizations within that industry are held to the same standards. That’s been proven to work. Well, I hope that you enjoyed this interview with Bruce. Next time we’ll be back in our regular format and I’ve got several great guests lined up for that show. Thanks so much for listening and thank you to my guest, Bruce Schneier. I’ve loaded up the show notes with links to all the relevant topics from today’s discussion, including Bruce’s books and much more. Be sure to check those out. If you’ve been enjoying 8th Layer Insights, please go ahead and take a couple of seconds to head over to Apple Podcast and rate and consider leaving a review. That does so much to help. You can also help by posting about it on social media, recommending it within your network, and, heck, maybe even referring an episode to a friend or a family member. If you haven’t, go ahead and subscribe or follow, wherever you like to get your podcasts. Lastly, if you want to connect with me, feel free to reach out on LinkedIn or Twitter. I also participate in a group on Clubhouse. We meet once a week on Friday. It’s called the Human Layer Club, and you can just search for it on Clubhouse and find it pretty easily. Well, until next time, thank you so much. I’m Perry Carpenter signing off."},
{"title": "The Coming AI Hackers", "article": "AI hackers are coming, and it’s not just our computer networks at risk – our laws and regulations are also vulnerable. Bruce Schneier, internationally renowned security technologist and fellow at Harvard’s Berkman Klein Center for Internet and Society, joins Azeem Azhar to explore how humans have always exploited loopholes in rule-based systems, and how that will change as AIs become more powerful. They also discuss:"},
{"title": "Everything We Know About Security Is Wrong", "article": "So says counterterrorism contrarian Bruce Schneier. And the Transportation Security Administration is listening. In late July, Transportation Security Administration chief Kip Hawley announced a change in his agency’s air travel screening policy: Effective August 4, cigarette lighters would no longer be banned from airplanes. Explaining the measure in an interview with the New York Times, Hawley acknowledged that confiscating lighters at security checkpoints—the TSA’s policy for the last two years in the wake of a failed shoe-bombing attempt—had been a waste of resources. Terrorists, he noted, might just as well ignite bombs on airplanes using small batteries (or, as he didn’t note, matches). “Taking lighters away is security theater,” Hawley told the Times. “It trivializes the security process.” Among those struck by Hawley’s about-face was Bruce Schneier, a Minneapolis man alternately called a “security guru” (The Economist), “the smartest guy in the room on security” (the ACLU), and “unquestionably the world’s foremost security technologist” (Connections). Schneier, who wears the graying beard and thinning ponytail of a computer geek chieftain, didn’t earn such accolades by mincing words. “There have been exactly two things since 9/11 that have made air travel safer,” Schneier said recently over spring rolls at a favorite Vietnamese restaurant on Nicollet Avenue. “Reinforcing the cockpit door and telling people to fight back in the event of an attack.” After a brief pause, half-devoured roll in hand, he reconsidered. “Well, maybe three,” he said. “I’m on the fence about sky marshals.” One thing Schneier isn’t on the fence about is the billions of dollars that the TSA has spent making air travelers pour out their water, take off their shoes, and until recently, throw out their cigarette lighters. All of this, Schneier argues, might make people feel safer, but it does little to actually improve security. Waiting for his bowl of pho to arrive, a triumphal smile crept across Schneier’s face when he brought up Hawley’s recent announcement. It wasn’t just that the TSA head had shifted policy. It was also that phrase: “security theater.” Schneier coined it back in 2003, to encapsulate what by his lights was a parade of new measures that conveyed safety but accomplished little. Such elegantly blunt criticisms have helped make Schneier a leading counterterrorism contrarian. A prolific writer—he has published several books, maintains regular columns for Wired.com and Forbes.com, and has a blog and electronic newsletter with a combined monthly readership of about 200,000—Schneier is also a seasoned public speaker, having addressed, among other august bodies, the House of Lords, the World Economic Forum, and the U.S. Congress. And that’s just in his spare time. Schneier’s paying job is chief technology officer for BT Counterpane, a network security company he founded in 1999 that last year was bought out for tens of millions of dollars. In addition, Schneier is quoted almost daily in one media outlet or another, on everything from data mining (usually a bad idea), to paperless voting (always a bad idea), to buying stuff with a credit card online (in the grand scheme of things, not such a bad idea). But he’s most passionate about the government’s response to terrorism since September 11, which he says has been both out of proportion to the threat and overly governed by our collective fears. His pho placed in front of him, Schneier picked up his spoon and jabbed the air with it. “We’re one terrorist attack away from a police state,” he said. On a recent morning at the Minneapolis-St. Paul International Airport, Schneier set out to foil airport security. Dressed in a black blazer and jeans, Schneier approached a stone-faced Northwest Airlines ticket agent and informed her that he’d lost his ID. “Do you have a credit card in your name?” she asked. “No,” Schneier answered. In accordance with airline policy, the agent printed Schneier’s boarding pass, scrawling “NO ID” on it. Schneier thanked her and headed to the security line, where he would receive extra scrutiny. In the end, though, Schneier was allowed to board his plane with little difficulty, even though the airline had no idea who he was. In so doing, Schneier demonstrated why the so-called “No Fly” list—the backbone of the airport security system—is, as he puts it, “a complete waste of time.” The No Fly list is a confidential database of people deemed by the federal government to be too dangerous to fly under any circumstances (albeit, as Schneier wryly points out, “too innocent to arrest”). A secondary classification, the lesser-known “Selectee” list, requires passengers to submit to a luggage search and wanding. But because, as Schneier demonstrated, anyone can check in without an ID and be treated as a selectee (not to mention board as a normal passenger by bribing a DMV worker for a fake license, as some of the 9/11 hijackers did), the No Fly list is easily circumvented. The government knows this, of course, and has pledged to overhaul the system by taking it out of the hands of the airlines. However, as Schneier points out, people will always lose their IDs, and there will always have to be a system in place to allow them to fly without one. Skeptical? Just imagine having your wallet stolen in Tulsa and being stuck there for weeks while waiting for a replacement driver’s license. Imagine that happening to hundreds of people a day, and the subsequent angry calls to congressmen and congresswomen demanding a change in the law. Which, says Schneier, is why any form of air travel security based on identifying passengers will never work. It will always be just a form of “security theater.” In a recent series of email exchanges with TSA chief Hawley that Schneier posted on his blog, he scolded Hawley for engaging in “cover your ass” security measures: A guy tries to blow up an airplane with his shoes, so now everyone has to take their shoes off; some people think of smuggling liquid explosives on a plane, so now everyone has to put liquids in three-ounce containers (unless the bottle is labeled “saline solution,” which counts as medication, and thus can be brought aboard in a vaguely defined “reasonable quantity”). As Cory Doctorow, the co-editor of the popular tech blog Boing Boing, puts it: “Bruce has a particular gift for puncturing ridiculous statements about security.” But though Schneier has been winning converts, his views are hardly gospel in government circles. Clark Kent Ervin, the former inspector general of the Department of Homeland Security, accuses Schneier of downplaying the terrorist threat. “It’s true that the chance of being killed by a terror attack is much smaller than being stricken by cancer,” says Ervin, who heads the homeland security program at the Aspen Institute, a Washington, D.C.-based think tank. “But it’s comparing apples and brass buttons.” Terror attacks, he says, “have a huge psychological as well as an economic impact. It’s silly talk to say that the chances of being killed in a terrorist attack are so small, and to infer from that that we needn’t worry about it.” Ultimately, Ervin says, Schneier’s legacy may be to lull people into a false sense of security. “His kind of thinking might be excusable in a pre-9/11 world,” Ervin says. “But in the post-9/11 world, it’s irresponsible and dangerous.” Bruce Schneier has had a fascination with security since childhood. As a boy in Brooklyn in the 1960s, he would crack secret codes written for him by his father. When he got older, he found himself studying the placement of security cameras to figure out the best strategy for shoplifting (a purely intellectual exercise—he says he never followed through on the idea). After graduating from SUNY Rochester with a degree in physics, Schneier spent the latter half of the 1980s at the Defense Department. He won’t elaborate on his time there, other than to say it involved “implementing security solutions at military installations.” A few years later, in 1993, Schneier penned his first best-selling book. The mathematics-heavy Applied Cryptography quickly became the seminal how-to guide for writing ciphers—complex algorithms that scramble data, protecting it when sent from one computer to another. In the years that followed, computer programmers—many looking to Schneier’s book for instruction—designed ever-more-impenetrable ciphers, with an eye toward keeping the data of multinational companies secure. This posed a problem for the U.S. government, which considered such so-called “strong crypto” a risk to national security. The Clinton administration, following in the footsteps of its predecessors, sought to put a stop to it, asserting that selling the encryption programs to foreign companies amounted to a breach of the International Traffic in Arms Regulations. A loose affiliation of mathematicians, civil libertarians, and antigovernment hard-liners fought back, giving rise to what came to be known as the “Crypto Wars.” In the ensuing public debate, Schneier found himself firmly in the fray, writing opinion papers and testifying before Senate and House committees. “He could respond to the government’s experts tit for tat,” says Jim Dempsey, policy director for the Center for Technology and Democracy, which advocated for strong crypto. “And nobody could say that he didn’t know what he was talking about, because he literally wrote the book on cryptography.” In 1999, after an appellate court ruled that restricting encryption was illegal, the Clinton administration surrendered. Encryption technology was allowed to flourish. But as Schneier’s co-combatants celebrated a hard-won victory, he found himself unable to join them. “We won the war,” he says, “but it was the wrong war.” Schneier had realized that the most important component of any security system is not its strengths but its weaknesses. Strong crypto is nearly impossible to penetrate. But the computer, the network, and even the user are far more fallible. Take, for example, the case of Dennis Alba and Mark Forrester. In 2001, the DEA investigated the middle-aged pair—who had become friends while in prison—on suspicion of setting up and running a large-scale, sophisticated Ecstasy ring in Escondido, California. The partners used code words to communicate and shielded their computer files with stong crypto. The DEA’s extensive investigation included obtaining a search warrant to break into their office and install a “keystroke logger” on a computer. That piece of software, which records what’s typed on the keyboard, enabled the government to get the key that unlocked their encryption. In 2005, both men were sentenced to 30 years in federal prison. (Forrester’s conviction was later overturned on a technicality.) The lesson was clear: All the crypto in the world is powerless to protect you if the front door is so easily pried open. Taking this to heart, Schneier, with a few million dollars of venture capital in tow, set up Counterpane Internet Security. The mission of the Silicon Valley-based firm was to monitor computer networks in much the same way ADT Home Security protects houses: by having human beings work in concert with technology. J.P. Vossen, a senior engineer at Counterpane, was inspired to join the company after hearing Schneier speak at a computer security conference. “He can explain some counterintuitive stuff very clearly,” Vossen says. “I like Bruce’s approach. That’s the largest reason I’m working here.” In the years that followed, Counterpane grew to a 115-employee firm worth tens of millions of dollars. This is where Schneier may have lived happily ever after, as a successful businessman and computer security geek extraordinaire. But then came the events of September 11, and what Schneier calls the “silly security season.” Schneier’s house, which has no more security than the locks on the doors, is a handsome stone structure on a leafy street across from Minnehaha Creek. Along with his wife, Karen Cooper, Schneier has lived here for the past 11 years. Most of the downstairs is one large open space, with no walls separating the living room, dining area, and sunroom, the last of which serves as Schneier’s office. Sitting near a window facing the creek on a recent afternoon, Schneier’s blue eyes opened wide and his voice rose as he explained his frustration with how the media covers would-be terrorists. “We’re just getting scared over idiots, like the London bombers,” he argued, referring to the clique of foreign-born medical personnel in Britain who tried to set off three car bombs. “Nothing would have exploded in those cars. You would have had a bunch of hot nails.” Then there was the group of men who planned to blow up Kennedy airport in New York. “You ever been to Kennedy airport?” Schneier asked. “It’s acres wide. You can’t blow up Kennedy airport. And they had a stupid plan that wouldn’t have worked. But we get all panicky. We end up saying, ‘Oh God! These people are going to blow up Kennedy airport!'” Such fears are examples of what Schneier calls “movie-plot threats”—grandiose scenarios that capture the imagination but are highly unlikely to succeed. “They’re good for scaring people, but it’s just silly to build national security policy around them,” Schneier says. On his blog last year, Schneier took issue with the idea that terrorists might target school buses. To protect against this dreamed-up threat, the Department of Homeland Security started training school bus drivers to be on the lookout for hijackers. In addition to being a waste of resources, Schneier pointed out, the measure may have actually put kids at greater risk, because bus drivers distracted by phantom terrorists could be more vulnerable to the much more realistic danger of oncoming traffic. For an example of how to spend money appropriately, Schneier says one need look no further than the I-35W bridge collapse. By investing federal Homeland Security money in communications equipment and a disaster preparedness plan—a response mechanism useful in any type of attack or catastrophe—local and state authorities were able to coordinate their efforts and, in all likelihood, save lives. “If they’d have put all that money into protecting the Foshay Tower, it would have been a complete waste,” Schneier says. In January, Schneier visited his newly born godson, Nicholas Quillen Perry, at Abbott Northwestern hospital in Minneapolis. As he looked at the snoozing infants in the maternity ward, he noticed that each one had an electronic bracelet around its ankle, which would trigger an alarm if the newborn were taken out of the ward. Sizing up this anti-infant-abduction measure, Schneier was initially struck by the stupidity of it. Hospital baby snatchings, after all, are extremely rare. Since 1983, there have been fewer than 250 reported cases in the United States, out of more than 80 million babies born in that time. In other words, the chance of a baby being abducted from a hospital is less than three in a million. But as Schneier watched the babies being removed from their cribs for one test or another, he began to wonder if this blatant display of security theater was such a bad thing after all. Parents of newborns are in a highly anxious state, prone to feeling less secure than they really are. Electronic bracelets, while not providing much actual security, can do wonders for the emotional well-being of the frazzled parents. Which has led to Schneier’s most recent revelation: Security theater can actually be a good thing when it brings our feelings of safety into line with the actual threat. Take the tamper-evident seals on over-the-counter medicine. In 1982, seven people in the Chicago area died after someone slipped cyanide into packages of Extra Strength Tylenol. Responding to widespread fears, manufacturers introduced tamper-evident seals. Although these were security theater—they don’t protect against syringes, for instance—the sense of safety they brought made the public’s comfort level come back in line with the actual threat, which was, statistically speaking, quite minimal. Sitting at a coffee shop around the corner from his house, Schneier considered the implications of his turnabout. Here he was, having spent years deriding security theater in all its manifestations, now saying that, in some cases, it’s actually a good thing. Did that mean he owed TSA chief Kip Hawley an apology? Schneier let out a chortle. His answer was true to form: so self-evident that it left the questioner feeling somewhat silly for even asking. The baby bracelets and tamper-evident medications, Schneier explained, are there to calm people. Banning lighters but not matches does nothing to relax fears. “It’s bad theater,” Schneier said. “Everyone sees what the TSA’s doing is a joke.”"},
{"title": "Cryptographer Slams NT Security", "article": "A top cryptographer said Microsoft’s version of a key protocol in Windows NT is so flawed that users should avoid using virtual private network software based on Microsoft’s Point to Point Tunneling Protocol. Bruce Schneier, a noted cryptographer, said the PPTP in Windows NT 4.0 is so broken it can’t be fixed with patches—a position that Microsoft disputes. “I believe it’s fundamentally broken,” said Schneier, who authored a widely used cryptography textbook. “What we’re seeing is the basic problem of proprietary security standards. These are really dumb mistakes, kindergarten crypto.” He advised users interested in setting up virtual private networks to buy software that supports the emerging IPSec standard, not PPTP.  Both of those protocols are designed for securely linking remote users to corporate networks over the Internet. But Microsoft said several of Schneier’s criticisms of Windows NT security have already been addressed in product updates or bug patches. “Several items in the press release are no longer issues,” said Microsoft’s Ed Muth, group product manager for security marketing. However, industry observers say only a handful of companies are implementing VPNs today, and many VPN firms are beginning to support IPSec, though some also back PPTP for interoperability. Microsoft said NT version 5.0, due next year, will support both protocols. “Large enterprise customers know PPTP is a relatively weak security protocol,” said Evan Kaplan, president of VPN firm . \"Our belief is that Microsoft will come around and fix it.\" Kaplan and others noted that some of Schneier’s critiques have been circulating for months on Internet newsgroups. “The security problems are very real, but almost all customers know about this security problem,” Kaplan added. Schneier contends weaknesses in Microsoft’s original NT 4.0 operating system could lead to stolen passwords, disclosure of private data, and server crashes when running some VPN software. Schneier stressed that the issues are in Microsoft’s version of PPTP, not in the protocol itself. But Microsoft’s Muth said the firm has fixed most of the problems already—one related to requiring strong passwords in December 1996, one in February to prevent attackers from crashing a PPTP server, and another last week on scrambling passwords sent over the Internet. Password-based security has inherent weaknesses, Muth said, which is why Microsoft is incorporating digital certificates using public key encryption into its operating systems. “PPTP is one small part of Windows NT, and Bruce made no general indictment of Windows NT,” Muth said. “We think the glass is half full, and we intend to fill it the rest of the way.” But Schneier, who has done consulting work for Microsoft and other major firms around the world, discounted Microsoft’s reaction. “Their normal tactic is either to ignore a security problem, claim it’s not a problem, or claim it’s being fixed,” Schneier said. “Then it blows over and they ignore it.” VPN vendors such as and , which is licensing its IPSec-compliant software to giant , have opted to make IPSec their strategic direction. “We’re in the IPSec business and figure that if you want to do it right, go to the trouble of using IPSec,” said Jim Hart, Red Creek’s director of engineering. “Even if it’s a perfect implementation of PPTP, if you’re a CIO or network administrator, you’re always going be criticized for doing PPTP.” John McCown, technical director for network security of the , which is testing IPSec-based VPNs, noted that IPSec has been widely discussed at the standards body . That kind of scrutiny tends to weed out errors, he said. “Having Bruce Schneier criticize something moves it out of the realm of, ‘This in general has problems’ to ‘Yes, the people who really know about this stuff have looked at it,’ and that’s serious,” McCown said."},
{"title": "Thought Leadership: Bruce Schneier on ‘A Hacker’s Mind’", "article": "Welcome to Cyber Security America, the podcast where we delve deep into the world of cybersecurity and provide insights on past trends, current challenges, and areas for improvement. Our goal is to help you stay informed and prepared for the next cyber threat. In this episode, we have a very special guest, Bruce Schneier, an internationally renowned security technologist, known as a “security guru” by The Economist. With over a dozen books and hundreds of articles and academic papers under his belt, Bruce is a true legend in the information security field. He’s also the author of the latest book, “A Hacker’s Mind,” where he takes hacking out of the world of computing and uses it to analyze the systems that underpin our society. During our conversation, Bruce provides us with valuable insights on the current state of cybersecurity. He discusses the impact of coordinated takedowns by federal forces on ransomware actors, and how less payment transactions on the blockchain related to ransomware actors is a promising sign. He also highlights an emerging threat, Black Lotus, and shares his thoughts on how artificial intelligence thinking like a hacker could be catastrophic. This episode is packed with expert tips and lessons learned. So tune in now to Cyber Security America and join the conversation."},
{"title": "Bruce Schneier on Regulating at the Pace of Tech", "article": "Harvard cyber security expert Bruce Schneier spoke with Transform for the publication’s inaugural issue on trust in tech. He said that, given how central technology is to our daily lives, we should be able to trust that tech systems are secure – in the same way we trust that food from the grocery store is safe to eat and planes are safe to fly in. If those things are safe, it’s only because governments regulate them. “We walk into a restaurant and don’t have to check the kitchen ourselves,” Schneier says. “Governments perform a valuable function in our stead: they are our experts.” But sometimes technology evolves too quickly for regulations to keep up. “The tech industry moves quickly. And we really don’t know how to regulate at the speed of tech. Tech moves much faster than government, and we don’t really have a good theory of agile regulation. The best we have, at least in the United States, are regulatory agencies, which are able to move faster than Congress can, but still slower than technology. So I think right now we in society, we’re working out how to regulate technology at technology’s pace.”"},
{"title": "Sociotechnical Exploitation with Bruce Schneier", "article": "The Sociotechnical Theory is an organizational theory that emphasizes the importance of both social and technical factors in designing and managing systems. Sociotechnical systems are deeply embedded within society and prone to “hacking,” a term meaning to subvert a systematic rules in unintended way.  In his most recent book, , Bruce Schneier takes hacking beyond computer systems and uses it to analyze the systems that underpin our society. He stops by and we define the true definition of hacking, who has the edge in the endless arms race, revealing who the world’s best hackers are, how AI will impact the future of hacking, and the truth about AI democratization. TIMESTAMPS 0:02:37 – Exploring the Hacker’s Mindset and How to Bend Society’s Rules 0:04:53 – The Importance of System Hacking in Today’s World 0:06:42 – The Inevitability of System Hacks and the Impact of AI 0:14:41 – Digital Simulation Technology on Policy and Legal Code 0:16:21 – Impact of Hacking on Existing Inequalities 0:18:21 – Hacking Resources and Loopholes"}
]